{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from typing import Optional, List, Tuple\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from vllm import EngineArgs, LLMEngine, SamplingParams, RequestOutput\n",
    "from tqdm.notebook import tqdm\n",
    "from vllm.lora.request import LoRARequest\n",
    "tqdm.pandas(desc='pandas bar')\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Specific LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已删除: lora_weight/expert/CTA_SimTab_train_init/checkpoint-50\n",
      "已删除: lora_weight/expert/CTA_SimTab_train_init/checkpoint-100\n",
      "已删除: lora_weight/expert/CTA_SimTab_train_init/checkpoint-150\n",
      "已删除: lora_weight/expert/CTA_WebTable_train_init/checkpoint-50\n",
      "已删除: lora_weight/expert/CTA_WebTable_train_init/checkpoint-100\n",
      "已删除: lora_weight/expert/CTA_WebTable_train_init/checkpoint-150\n",
      "已删除: lora_weight/expert/CTA_WebTable_train_init/checkpoint-200\n",
      "已删除: lora_weight/expert/CTA_WebTable_train_init/checkpoint-250\n",
      "已删除: lora_weight/expert/CTA_WebTable_train_init/checkpoint-300\n",
      "已删除: lora_weight/expert/CTA_WebTable_train_init/checkpoint-350\n",
      "已删除: lora_weight/expert/CTA_WebTable_train_init/checkpoint-400\n",
      "已删除: lora_weight/expert/CTA_WebTable_train_init/checkpoint-450\n",
      "已删除: lora_weight/expert/CTA_WebTable_train_init/checkpoint-500\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def delete_checkpoints(directory):\n",
    "    \"\"\"遍历指定目录，删除所有名字包含'checkpoint'的子文件夹。\"\"\"\n",
    "    for root, dirs, files in os.walk(directory, topdown=False):\n",
    "        # 遍历目录\n",
    "        for name in dirs:\n",
    "            if 'checkpoint' in name:\n",
    "                # 构建完整的文件夹路径\n",
    "                full_path = os.path.join(root, name)\n",
    "                # 删除文件夹\n",
    "                shutil.rmtree(full_path)\n",
    "                print(f\"已删除: {full_path}\")\n",
    "\n",
    "# 使用示例\n",
    "directory_path = 'lora_weight'\n",
    "delete_checkpoints(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98c0325295e481fa5dda9a99ae23400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "# 加载基础模型\n",
    "from peft import PeftModel\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"/data/home/wangys/LLAMA-backup/LLaMA-Factory-main/Mistral-7B-Instruct-v0.2\",device_map='cpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = \"lora_weight/expert/amazon_google-MoE-Add\"\n",
    "model = PeftModel.from_pretrained(base_model, model_id=peft_model_id,adapter_name='amazon-google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_adapter(\"lora_weight/expert/amazon_train-MoE-Add\", adapter_name=\"amazon\")\n",
    "model.load_adapter(\"lora_weight/expert/ant_buy-MoE-Add\", adapter_name=\"ant-buy\")\n",
    "model.load_adapter(\"lora_weight/expert/CMS_train-MoE-Add\", adapter_name=\"CMS\")\n",
    "model.load_adapter(\"lora_weight/expert/restaurant_train-MoE-Add\", adapter_name=\"restaurant\")\n",
    "model.load_adapter(\"lora_weight/expert/semi_text_c-MoE-Add\", adapter_name=\"semi-text-c\")\n",
    "model.load_adapter(\"lora_weight/expert/semi_text_w-MoE-Add\", adapter_name=\"semi-text-w\")\n",
    "model.load_adapter(\"lora_weight/expert/walmart_amazon-MoE-Add\", adapter_name=\"walmart-amazon\")\n",
    "model.load_adapter(\"lora_weight/expert/walmart_train-MoE-Add\", adapter_name=\"walmart\")\n",
    "model.load_adapter(\"lora_weight/expert/wdc_all-MoE-Add\", adapter_name=\"wdc-all\")\n",
    "model.load_adapter(\"lora_weight/expert/CTA_SimTab_train_init\", adapter_name=\"SimTab\")\n",
    "model.load_adapter(\"lora_weight/expert/CTA_WebTable_train_init\", adapter_name=\"WebTable\")\n",
    "model.load_adapter(\"lora_weight/expert/hospital_train-MoE-Add\", adapter_name=\"hospital\")\n",
    "model.load_adapter(\"lora_weight/expert/beer_train-MoE-Add\", adapter_name=\"beer\")\n",
    "model.load_adapter(\"lora_weight/expert/rayyan_train-MoE-Add\", adapter_name=\"rayyan\")\n",
    "model.load_adapter(\"lora_weight/expert/RE-MoE-Add\", adapter_name=\"RE\")\n",
    "model.load_adapter(\"lora_weight/expert/synthea_train-MoE-Add\", adapter_name=\"synthea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_weighted_adapter(\n",
    "    adapters=[\"WebTable\", \"semi-text-w\"],\n",
    "    weights=[1, 1],\n",
    "    adapter_name=\"Mistral|webtable-MoE-CT#Mistral|SimTab-MoE-CT\",\n",
    "    combination_type=\"cat\"\n",
    ")\n",
    "model_id_merge = 'Mistral|webtable-MoE-CT#Mistral|SimTab-MoE-CT'\n",
    "model.save_pretrained(model_id = model_id_merge,save_directory='lora_weight/merge/%s' % model_id_merge) ## Save the Merged File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=7 python semantic_uncertainty/generate_answers.py --model_name=/data/home/wangys/model/llama3-8b-instruct --dataset=trivia_qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize vLLM multi-lora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-02 00:33:29 config.py:813] Defaulting to use mp for distributed inference\n",
      "INFO 09-02 00:33:29 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='/data/home/wangys/LLAMA-backup/LLaMA-Factory-main/Mistral-7B-Instruct-v0.2', speculative_config=None, tokenizer='/data/home/wangys/LLAMA-backup/LLaMA-Factory-main/Mistral-7B-Instruct-v0.2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/data/home/wangys/LLAMA-backup/LLaMA-Factory-main/Mistral-7B-Instruct-v0.2, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "WARNING 09-02 00:33:29 multiproc_gpu_executor.py:59] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-02 00:33:29 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3314749)\u001b[0;0m INFO 09-02 00:33:29 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3314750)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3314751)\u001b[0;0m INFO 09-02 00:33:29 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 09-02 00:33:29 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 09-02 00:33:30 utils.py:975] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3314751)\u001b[0;0m INFO 09-02 00:33:30 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3314750)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3314749)\u001b[0;0m INFO 09-02 00:33:30 utils.py:975] Found nccl from library libnccl.so.2\n",
      "INFO 09-02 00:33:30 utils.py:975] Found nccl from library libnccl.so.2\n",
      "INFO 09-02 00:33:30 utils.py:975] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3314751)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3314750)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3314749)\u001b[0;0m INFO 09-02 00:33:30 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-02 00:33:30 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-02 00:33:30 pynccl.py:63] vLLM is using nccl==2.20.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W902 00:33:30.780543017 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:54223 (errno: 97 - Address family not supported by protocol).\n",
      "[W902 00:33:30.800237946 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:54223 (errno: 97 - Address family not supported by protocol).\n",
      "[W902 00:33:30.848398666 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:54223 (errno: 97 - Address family not supported by protocol).\n",
      "[W902 00:33:30.900539567 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:54223 (errno: 97 - Address family not supported by protocol).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-02 00:33:30 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /home/wangys/.cache/vllm/gpu_p2p_access_cache_for_4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3314750)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3314749)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3314751)\u001b[0;0m INFO 09-02 00:33:30 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /home/wangys/.cache/vllm/gpu_p2p_access_cache_for_4,5,6,7.json\n",
      "INFO 09-02 00:33:30 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /home/wangys/.cache/vllm/gpu_p2p_access_cache_for_4,5,6,7.json\n",
      "INFO 09-02 00:33:30 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /home/wangys/.cache/vllm/gpu_p2p_access_cache_for_4,5,6,7.json\n",
      "INFO 09-02 00:33:30 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7ff0ad796180>, local_subscribe_port=60381, remote_subscribe_port=None)\n",
      "INFO 09-02 00:33:30 model_runner.py:879] Starting to load model /data/home/wangys/LLAMA-backup/LLaMA-Factory-main/Mistral-7B-Instruct-v0.2...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3314749)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3314750)\u001b[0;0m INFO 09-02 00:33:30 model_runner.py:879] Starting to load model /data/home/wangys/LLAMA-backup/LLaMA-Factory-main/Mistral-7B-Instruct-v0.2...\n",
      "INFO 09-02 00:33:30 model_runner.py:879] Starting to load model /data/home/wangys/LLAMA-backup/LLaMA-Factory-main/Mistral-7B-Instruct-v0.2...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3314751)\u001b[0;0m INFO 09-02 00:33:30 model_runner.py:879] Starting to load model /data/home/wangys/LLAMA-backup/LLaMA-Factory-main/Mistral-7B-Instruct-v0.2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=3314750)\u001b[0;0m /home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:416: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3314750)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3314751)\u001b[0;0m /home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:416: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3314751)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3314749)\u001b[0;0m /home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:416: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3314749)\u001b[0;0m "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63398fde47894b97bc87dcf00eff6d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pt checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:416: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=3314750)\u001b[0;0m INFO 09-02 00:33:39 model_runner.py:890] Loading model weights took 3.3975 GB\n",
      "INFO 09-02 00:33:39 model_runner.py:890] Loading model weights took 3.3975 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3314751)\u001b[0;0m INFO 09-02 00:33:39 model_runner.py:890] Loading model weights took 3.3975 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3314749)\u001b[0;0m INFO 09-02 00:33:39 model_runner.py:890] Loading model weights took 3.3975 GB\n",
      "INFO 09-02 00:33:44 distributed_gpu_executor.py:56] # GPU blocks: 121115, # CPU blocks: 8192\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, List, Tuple\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from vllm import EngineArgs, LLMEngine, SamplingParams, RequestOutput\n",
    "from vllm.lora.request import LoRARequest\n",
    "def initialize_engine() -> LLMEngine:\n",
    "    \"\"\"Initialize the LLMEngine.\"\"\"\n",
    "    # max_loras: controls the number of LoRAs that can be used in the same\n",
    "    #   batch. Larger numbers will cause higher memory usage, as each LoRA\n",
    "    #   slot requires its own preallocated tensor.\n",
    "    # max_lora_rank: controls the maximum supported rank of all LoRAs. Larger\n",
    "    #   numbers will cause higher memory usage. If you know that all LoRAs will\n",
    "    #   use the same rank, it is recommended to set this as low as possible.\n",
    "    # max_cpu_loras: controls the size of the CPU LoRA cache.\n",
    "    engine_args = EngineArgs(model=\"/data/home/wangys/LLAMA-backup/LLaMA-Factory-main/Mistral-7B-Instruct-v0.2\",\n",
    "                             enable_lora=True,\n",
    "                             max_loras=32,\n",
    "                             max_lora_rank=64,\n",
    "                             max_cpu_loras=32,\n",
    "                             max_num_seqs=256,enforce_eager=True,tensor_parallel_size=4,\n",
    "                             disable_log_stats=True)\n",
    "    return LLMEngine.from_engine_args(engine_args)\n",
    "model = initialize_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m MoE_list_update_top_2 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/home/wangys/MoE-Example/Router/MoE_list_update_top_2.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "MoE_list_update_top_2 = pd.read_csv('/data/home/wangys/MoE-Example/Router/MoE_list_update_top_2.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos               ['Mistral|amazon_google-MoE-CT', 'Mistral|semi...\n",
       "neg                                                              []\n",
       "expert                                 Mistral|amazon_google-MoE-CT\n",
       "query             You are an expert in detecting if two text des...\n",
       "expert_predict    ['Mistral|amazon_google-MoE-CT', 'Mistral|SimT...\n",
       "domain                                 Mistral|amazon_google-MoE-CT\n",
       "cross-dataset     [Mistral|SimTab-MoE-CT, Mistral|semi_text_w-Mo...\n",
       "cross-task          [Mistral|SimTab-MoE-CT, Mistral|walmart-MoE-CT]\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MoE_list_update_top_2.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AST(row):\n",
    "    CD = row['cross-dataset']\n",
    "    row['cross-dataset'] = eval(CD)\n",
    "    CT = row['cross-task']\n",
    "    row['cross-task'] = eval(CT)\n",
    "    expert = row['expert_predict']\n",
    "    row['expert_predict'] = eval(expert)\n",
    "    return row\n",
    "MoE_list_update_top_2 = MoE_list_update_top_2.progress_apply(AST,axis=1)\n",
    "expert_list = []\n",
    "for index,row in MoE_list_update_top_2.iterrows():\n",
    "    # expert_list.append(set(row['cross-dataset'])) if set(row['cross-dataset']) not in expert_list else None\n",
    "    # expert_list.append(set(row['cross-task'])) if set(row['cross-task']) not in expert_list else None\n",
    "    expert_list.append(set(row['expert_predict'])) if set(row['expert_predict']) not in expert_list else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_path_dict = {}\n",
    "evaluate_task = 'expert_predict'\n",
    "for e in expert_list:\n",
    "    expert_0,expert_1 = list(e)\n",
    "    folder_path = 'lora_weight/merge/%s#%s' % (expert_0,expert_1)\n",
    "    folder_path_rev = 'lora_weight/merge/%s#%s' % (expert_1,expert_0)\n",
    "\n",
    "    if(os.path.exists(folder_path)):\n",
    "        lora_path_dict['%s#%s' % (expert_0,expert_1)] = folder_path\n",
    "        lora_path_dict['%s#%s' % (expert_1,expert_0)] = folder_path\n",
    "    elif(os.path.exists(folder_path_rev)):\n",
    "        lora_path_dict['%s#%s' % (expert_0,expert_1)] = folder_path_rev\n",
    "        lora_path_dict['%s#%s' % (expert_1,expert_0)] = folder_path_rev\n",
    "    else:\n",
    "        print(folder_path)\n",
    "def create_multi_lora_call(df,lora_id_list=['']):\n",
    "    multi_lora_call = []\n",
    "    for index,row in df.iterrows():\n",
    "        # lora_id = '#'.join(row['cross-task'])\n",
    "        lora_id = '#'.join(row[evaluate_task])\n",
    "        multi_lora_call.append([row['query'],lora_id,index])\n",
    "    return multi_lora_call\n",
    "def create_test_prompts(multi_lora_call: list,lora_path: dict)-> List[Tuple[str, SamplingParams]]:\n",
    "    output_list = []\n",
    "    lora_all = list(lora_path.keys())\n",
    "    for m in multi_lora_call:\n",
    "        m_output = (\"[INST] %s [/INST]\" % m[0],\n",
    "         SamplingParams(temperature=0.0,\n",
    "                        top_p=1,\n",
    "                        # prompt_logprobs=1,\n",
    "                        max_tokens=512),\n",
    "         LoRARequest(m[1], lora_all.index(m[1]) + 1, lora_path[m[1]]),\n",
    "         m[2])\n",
    "        output_list.append(m_output)\n",
    "    return output_list\n",
    "lora_id_list=list(lora_path_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3314210/2318660558.py:31: DeprecationWarning: The 'lora_local_path' attribute is deprecated and will be removed in a future version. Please use 'lora_path' instead.\n",
      "  LoRARequest(m[1], lora_all.index(m[1]) + 1, lora_path[m[1]]),\n",
      "100%|██████████| 2000/2000 [03:17<00:00, 10.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197.08821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_requests(engine: LLMEngine,\n",
    "                     test_prompts: List[Tuple[str, SamplingParams,\n",
    "                                              Optional[LoRARequest]]]):\n",
    "    \"\"\"Continuously process a list of prompts and handle the outputs.\"\"\"\n",
    "    request_id = 0\n",
    "    output_list = []\n",
    "    output_request = []\n",
    "    pbar = tqdm(total=len(test_prompts))  # 初始化tqdm进度条\n",
    "    while test_prompts or engine.has_unfinished_requests():\n",
    "        \n",
    "        if test_prompts:\n",
    "            prompt, sampling_params, lora_request, index = test_prompts.pop(0)\n",
    "            engine.add_request(str(index),\n",
    "                               prompt,\n",
    "                               sampling_params,\n",
    "                               lora_request=lora_request)\n",
    "            request_id += 1\n",
    "            pbar.update(1)  # 更新进度条\n",
    "        request_outputs: List[RequestOutput] = engine.step()\n",
    "        for request_output in request_outputs:\n",
    "            if request_output.finished:\n",
    "                output_list.append(request_output)\n",
    "    pbar.close()  # 关闭进度条\n",
    "    return output_list\n",
    "\n",
    "# 示例使用\n",
    "lora_id_list = list(lora_path_dict.keys())\n",
    "multi_lora_call = create_multi_lora_call(MoE_list_update_top_2.iloc[:2000], lora_id_list=lora_id_list)\n",
    "test_prompts_input = create_test_prompts(multi_lora_call=multi_lora_call, lora_path=lora_path_dict)\n",
    "start_time = datetime.now()\n",
    "result_all = process_requests(model, test_prompts=test_prompts_input)\n",
    "end_time = datetime.now()\n",
    "print((end_time - start_time).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(result_all[0].prompt)\n",
    "# result_all[33]\n",
    "output_ins = {}\n",
    "output_predict = {}\n",
    "for lora_id in lora_id_list:\n",
    "    # output_ins[lora_id] = [''] * int(len(result_all) / len(lora_path_dict))\n",
    "    # output_predict[lora_id] = [''] * int(len(result_all) / len(lora_path_dict))\n",
    "    output_ins[lora_id] = [''] * int(len(result_all) )\n",
    "    output_predict[lora_id] = [''] * int(len(result_all) )\n",
    "# output_lora_id = [''] * len(result_all)\n",
    "for request in result_all:\n",
    "    request_id = int(request.request_id)\n",
    "    request_ins = request.prompt.strip()\n",
    "    request_lora = request.lora_request.lora_name\n",
    "    request_output = request.outputs[0].text.strip()\n",
    "    output_ins[request_lora][request_id] = request_ins\n",
    "    output_predict[request_lora][request_id] = request_output\n",
    "    # output_lora_id[request_id] = request_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_output_MoE = {}\n",
    "for result in result_all:\n",
    "    prompt = result.prompt.replace('[INST] ','').replace(' [/INST]','')\n",
    "    output = result.outputs[0].text.strip()\n",
    "    dict_output_MoE[prompt] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('tests/MoE_result_2000.npy',dict_output_MoE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MoE_list_update_top_2_few = MoE_list_update_top_2.iloc[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3314210/153846079.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MoE_list_update_top_2_few['prediction'] = MoE_list_update_top_2_few['query'].map(dict_output_MoE)\n"
     ]
    }
   ],
   "source": [
    "MoE_list_update_top_2_few['prediction'] = MoE_list_update_top_2_few['query'].map(dict_output_MoE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'performance': {'accuracy': {'mean': 0.66, 'bootstrap': {'std_err': 0.024141193860101266, 'low': 0.62, 'high': 0.6975}}}, 'uncertainty': {'p_false': {'AUROC': {'mean': 0.7035984848484848, 'bootstrap': {'std_err': 0.02868401073143026, 'low': 0.653203966827957, 'high': 0.7460714715683334}}, 'area_under_thresholded_accuracy': {'mean': 0.7292793838384062, 'bootstrap': {'std_err': 0.02544652776737242, 'low': 0.6854210115670789, 'high': 0.7696289034003679}}, 'mean_uncertainty': {'mean': 1.369315084692098, 'bootstrap': {'std_err': 0.042032147321903794, 'low': 1.3005711938199878, 'high': 1.4418123183850153}}, 'accuracy_at_0.8_answer_fraction': {'mean': 0.734375, 'bootstrap': {'std_err': 0.025600775278244323, 'low': 0.6934984520123839, 'high': 0.7770833249833019}}, 'accuracy_at_0.9_answer_fraction': {'mean': 0.6805555555555556, 'bootstrap': {'std_err': 0.02436561822303607, 'low': 0.6371191135734072, 'high': 0.7166666666666667}}, 'accuracy_at_0.95_answer_fraction': {'mean': 0.6763157894736842, 'bootstrap': {'std_err': 0.02443786481709613, 'low': 0.6394736842105263, 'high': 0.718421052631579}}, 'accuracy_at_1.0_answer_fraction': {'mean': 0.66, 'bootstrap': {'std_err': 0.02298985378655391, 'low': 0.6225, 'high': 0.6975}}}, 'p_false_UNANSWERABLE': {'AUROC': {'mean': nan, 'bootstrap': {'std_err': nan, 'low': nan, 'high': nan}}, 'area_under_thresholded_accuracy': {'mean': 0.9473684210526319, 'bootstrap': {'std_err': 0.0, 'low': 0.9473684210526319, 'high': 0.9473684210526319}}, 'mean_uncertainty': {'mean': 1.369315084692098, 'bootstrap': {'std_err': 0.04231973671562589, 'low': 1.3071708831006759, 'high': 1.450182662198471}}, 'accuracy_at_0.8_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}, 'accuracy_at_0.9_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}, 'accuracy_at_0.95_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}, 'accuracy_at_1.0_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}}, 'p_false_fixed': {'AUROC': {'mean': 0.7035984848484848, 'bootstrap': {'std_err': 0.029221055564121294, 'low': 0.6516528019465964, 'high': 0.7483264771615535}}, 'area_under_thresholded_accuracy': {'mean': 0.7292793838384062, 'bootstrap': {'std_err': 0.024442589921248144, 'low': 0.6877122458301619, 'high': 0.7656358315815655}}, 'mean_uncertainty': {'mean': 0.1643655148000058, 'bootstrap': {'std_err': 0.014291699531727502, 'low': 0.14193933071971335, 'high': 0.19023172928351054}}, 'accuracy_at_0.8_answer_fraction': {'mean': 0.734375, 'bootstrap': {'std_err': 0.02662937236491397, 'low': 0.6930013488514927, 'high': 0.78125}}, 'accuracy_at_0.9_answer_fraction': {'mean': 0.6805555555555556, 'bootstrap': {'std_err': 0.026021578899907926, 'low': 0.631578947368421, 'high': 0.7194444444444444}}, 'accuracy_at_0.95_answer_fraction': {'mean': 0.6763157894736842, 'bootstrap': {'std_err': 0.024189868065330418, 'low': 0.6377952755905512, 'high': 0.7165354330708661}}, 'accuracy_at_1.0_answer_fraction': {'mean': 0.66, 'bootstrap': {'std_err': 0.024109943786524875, 'low': 0.6197431891104098, 'high': 0.6975}}}, 'p_false_fixed_UNANSWERABLE': {'AUROC': {'mean': nan, 'bootstrap': {'std_err': nan, 'low': nan, 'high': nan}}, 'area_under_thresholded_accuracy': {'mean': 0.9473684210526319, 'bootstrap': {'std_err': 0.0, 'low': 0.9473684210526319, 'high': 0.9473684210526319}}, 'mean_uncertainty': {'mean': 0.1643655148000058, 'bootstrap': {'std_err': 0.015471778412932095, 'low': 0.1410452565055509, 'high': 0.19063765082904466}}, 'accuracy_at_0.8_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}, 'accuracy_at_0.9_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}, 'accuracy_at_0.95_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}, 'accuracy_at_1.0_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}}, 'cluster_assignment_entropy': {'AUROC': {'mean': 0.7458082664884135, 'bootstrap': {'std_err': 0.02695053899385819, 'low': 0.696066896826084, 'high': 0.7849903830459627}}, 'area_under_thresholded_accuracy': {'mean': 0.7418208412716618, 'bootstrap': {'std_err': 0.02330995878459444, 'low': 0.6998433386690897, 'high': 0.7764516132607315}}, 'mean_uncertainty': {'mean': 0.64406744955496, 'bootstrap': {'std_err': 0.03349239526642996, 'low': 0.5921619059734258, 'high': 0.7047643441835325}}, 'accuracy_at_0.8_answer_fraction': {'mean': 0.7507788161993769, 'bootstrap': {'std_err': 0.026008505698620125, 'low': 0.7046530970893384, 'high': 0.7907925647520161}}, 'accuracy_at_0.9_answer_fraction': {'mean': 0.7154696132596685, 'bootstrap': {'std_err': 0.024463068328288616, 'low': 0.6721775870328311, 'high': 0.7527777777777778}}, 'accuracy_at_0.95_answer_fraction': {'mean': 0.6839378238341969, 'bootstrap': {'std_err': 0.025364921189658284, 'low': 0.6371294633724262, 'high': 0.7193063301221577}}, 'accuracy_at_1.0_answer_fraction': {'mean': 0.66, 'bootstrap': {'std_err': 0.02413476925696799, 'low': 0.6175, 'high': 0.6975}}}, 'cluster_assignment_entropy_UNANSWERABLE': {'AUROC': {'mean': nan, 'bootstrap': {'std_err': nan, 'low': nan, 'high': nan}}, 'area_under_thresholded_accuracy': {'mean': 0.9473684210526319, 'bootstrap': {'std_err': 0.0, 'low': 0.9473684210526319, 'high': 0.9473684210526319}}, 'mean_uncertainty': {'mean': 0.64406744955496, 'bootstrap': {'std_err': 0.03412283819837339, 'low': 0.5920744891489007, 'high': 0.7046586903017125}}, 'accuracy_at_0.8_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}, 'accuracy_at_0.9_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}, 'accuracy_at_0.95_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}, 'accuracy_at_1.0_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}}, 'regular_entropy': {'AUROC': {'mean': 0.7593025846702317, 'bootstrap': {'std_err': 0.026247063554649504, 'low': 0.7124753402150099, 'high': 0.7993677564270243}}, 'area_under_thresholded_accuracy': {'mean': 0.7574959306284539, 'bootstrap': {'std_err': 0.022247095521513518, 'low': 0.7181114931645975, 'high': 0.7908064226078415}}, 'mean_uncertainty': {'mean': 0.6669306986157606, 'bootstrap': {'std_err': 0.0315462424011832, 'low': 0.6144860698546385, 'high': 0.7185335210467023}}, 'accuracy_at_0.8_answer_fraction': {'mean': 0.7625, 'bootstrap': {'std_err': 0.027558846067912316, 'low': 0.715625, 'high': 0.80625}}, 'accuracy_at_0.9_answer_fraction': {'mean': 0.7111111111111111, 'bootstrap': {'std_err': 0.02476664936675551, 'low': 0.6638888888888889, 'high': 0.7451523545706371}}, 'accuracy_at_0.95_answer_fraction': {'mean': 0.6947368421052632, 'bootstrap': {'std_err': 0.024990425713854226, 'low': 0.6526315789473685, 'high': 0.7322834645669292}}, 'accuracy_at_1.0_answer_fraction': {'mean': 0.66, 'bootstrap': {'std_err': 0.023594578781955356, 'low': 0.62, 'high': 0.695}}}, 'regular_entropy_UNANSWERABLE': {'AUROC': {'mean': nan, 'bootstrap': {'std_err': nan, 'low': nan, 'high': nan}}, 'area_under_thresholded_accuracy': {'mean': 0.9473684210526319, 'bootstrap': {'std_err': 0.0, 'low': 0.9473684210526319, 'high': 0.9473684210526319}}, 'mean_uncertainty': {'mean': 0.6669306986157606, 'bootstrap': {'std_err': 0.031238346732170397, 'low': 0.6147131369942241, 'high': 0.7174470980494095}}, 'accuracy_at_0.8_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}, 'accuracy_at_0.9_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}, 'accuracy_at_0.95_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}, 'accuracy_at_1.0_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}}, 'semantic_entropy': {'AUROC': {'mean': 0.7442067736185384, 'bootstrap': {'std_err': 0.028247504655536767, 'low': 0.6915618337648043, 'high': 0.7882459476310489}}, 'area_under_thresholded_accuracy': {'mean': 0.7444770381737311, 'bootstrap': {'std_err': 0.02397198041436991, 'low': 0.702229990822112, 'high': 0.7805534193313592}}, 'mean_uncertainty': {'mean': 0.44979299889069735, 'bootstrap': {'std_err': 0.028296649135451272, 'low': 0.40567772053218304, 'high': 0.5005747356440328}}, 'accuracy_at_0.8_answer_fraction': {'mean': 0.75, 'bootstrap': {'std_err': 0.02688927514531295, 'low': 0.7071651090342679, 'high': 0.794392523364486}}, 'accuracy_at_0.9_answer_fraction': {'mean': 0.7166666666666667, 'bootstrap': {'std_err': 0.026028887657561714, 'low': 0.675, 'high': 0.7555555555555555}}, 'accuracy_at_0.95_answer_fraction': {'mean': 0.6894736842105263, 'bootstrap': {'std_err': 0.024552704636588544, 'low': 0.6492146596858639, 'high': 0.7313725308312533}}, 'accuracy_at_1.0_answer_fraction': {'mean': 0.66, 'bootstrap': {'std_err': 0.02398854505892436, 'low': 0.62, 'high': 0.6975}}}, 'semantic_entropy_UNANSWERABLE': {'AUROC': {'mean': nan, 'bootstrap': {'std_err': nan, 'low': nan, 'high': nan}}, 'area_under_thresholded_accuracy': {'mean': 0.9473684210526319, 'bootstrap': {'std_err': 0.0, 'low': 0.9473684210526319, 'high': 0.9473684210526319}}, 'mean_uncertainty': {'mean': 0.44979299889069735, 'bootstrap': {'std_err': 0.03007733158812221, 'low': 0.4028665346303029, 'high': 0.5017891613519951}}, 'accuracy_at_0.8_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}, 'accuracy_at_0.9_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}, 'accuracy_at_0.95_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}, 'accuracy_at_1.0_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}}, 'p_ik': {'AUROC': {'mean': 0.6278966131907308, 'bootstrap': {'std_err': 0.029611489562178066, 'low': 0.5749703439229479, 'high': 0.673213816603144}}, 'area_under_thresholded_accuracy': {'mean': 0.6986251550698442, 'bootstrap': {'std_err': 0.0258544578124125, 'low': 0.6549891784009043, 'high': 0.7377444578308109}}, 'mean_uncertainty': {'mean': 0.25540223396559353, 'bootstrap': {'std_err': 0.017929461172182942, 'low': 0.22589726309512342, 'high': 0.28597168929048955}}, 'accuracy_at_0.8_answer_fraction': {'mean': 0.70625, 'bootstrap': {'std_err': 0.02638966540639941, 'low': 0.6625, 'high': 0.746875}}, 'accuracy_at_0.9_answer_fraction': {'mean': 0.6777777777777778, 'bootstrap': {'std_err': 0.025501545493097837, 'low': 0.631914336072809, 'high': 0.7146814404432132}}, 'accuracy_at_0.95_answer_fraction': {'mean': 0.6710526315789473, 'bootstrap': {'std_err': 0.025096668349502298, 'low': 0.6289473684210526, 'high': 0.7114211256149086}}, 'accuracy_at_1.0_answer_fraction': {'mean': 0.66, 'bootstrap': {'std_err': 0.023036104398440633, 'low': 0.62, 'high': 0.6975}}}, 'p_ik_UNANSWERABLE': {'AUROC': {'mean': nan, 'bootstrap': {'std_err': nan, 'low': nan, 'high': nan}}, 'area_under_thresholded_accuracy': {'mean': 0.9473684210526319, 'bootstrap': {'std_err': 0.0, 'low': 0.9473684210526319, 'high': 0.9473684210526319}}, 'mean_uncertainty': {'mean': 0.25540223396559353, 'bootstrap': {'std_err': 0.01767447719530058, 'low': 0.2248553087358517, 'high': 0.2831552998184846}}, 'accuracy_at_0.8_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}, 'accuracy_at_0.9_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}, 'accuracy_at_0.95_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}, 'accuracy_at_1.0_answer_fraction': {'mean': 1.0, 'bootstrap': {'std_err': 0.0, 'low': nan, 'high': nan}}}}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
