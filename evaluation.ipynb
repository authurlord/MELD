{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for ER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    " CUDA_VISIBLE_DEVICES=6,7 python vllm_inference_mistral_api.py --directory lora_weight/Expert/ER/wdc_all-MoE-Add --gpu_num 2 --file dataset/ER/wdc_all_test_output.json --max_token 32 --json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9102455546147333 0.8973288814691152 0.9037410676754939\n"
     ]
    }
   ],
   "source": [
    "test_file_path = 'inference_Transfer_ER/wdc_all-MoE-Add--wdc_all_test_output.csv' ## replace with the inference file\n",
    "result_merge = pd.read_csv(test_file_path,index_col=0).fillna('')\n",
    "def Transfer(row):\n",
    "    if(row['output'].__contains__('mismatch')) or (row['output'].__contains__('dismatch')):\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    if(row['predict'].__contains__('mismatch')) or (row['predict'].__contains__('dismatch')):\n",
    "        predict = 0\n",
    "    else:\n",
    "        predict = 1\n",
    "    return label,predict\n",
    "result_output = result_merge.apply(Transfer,axis=1,result_type='expand')\n",
    "from sklearn.metrics import f1_score, precision_score,recall_score\n",
    "print(precision_score(y_true=result_output[0],y_pred=result_output[1]),recall_score(y_true=result_output[0],y_pred=result_output[1]),f1_score(y_true=result_output[0],y_pred=result_output[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the result of Error Detection changed, please change the `detector.npy` and test file, e.g. `dataset/DC/beer_test.json` correspondingly. Each detected error is related to one record in test file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Hospital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " CUDA_VISIBLE_DEVICES=6,7 python vllm_inference_mistral_api.py --directory lora_weight/Expert/DC/rayyan_train-MoE-Add --gpu_num 2 --file dataset/DC/rayyan-test-20.json --max_token 512 --json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    " CUDA_VISIBLE_DEVICES=6,7 python vllm_inference_mistral_api.py --directory lora_weight/Expert/DC/beer_train-MoE-Add --gpu_num 2 --file dataset/DC/beer_test.json --max_token 512 --json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    " CUDA_VISIBLE_DEVICES=6,7 python vllm_inference_mistral_api.py --directory lora_weight/Expert/DC/hospital_train-MoE-Add --gpu_num 2 --file dataset/DC/hospital-test.json --max_token 512 --json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e077f798f94fd195acc4aa07054152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.9664694280078896, 0.962671905697446, 0.9645669291338582)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file_path = 'inference/hospital-test.csv' ## Replace the train file\n",
    "hospital_result = pd.read_csv(test_file_path,index_col=0) ## \n",
    "count = 0\n",
    "hospital_clean = pd.read_csv('raw_dataset/DC/hospital/original/clean.csv').astype(str)\n",
    "hospital_dirty = pd.read_csv('raw_dataset/DC/hospital/original/dirty.csv').astype(str)\n",
    "hospital_dirty.columns = hospital_clean.columns\n",
    "hospital_correction = hospital_dirty.copy()\n",
    "hospital_detector = np.load('raw_dataset/DC/hospital/detector/detector.npy').reshape((-1,20))\n",
    "import ast\n",
    "for d in np.argwhere(hospital_detector==1):\n",
    "    i = d[0]\n",
    "    j = d[1]\n",
    "    try:\n",
    "        predict = list(eval(hospital_result.iloc[count,-1]).values())[0]\n",
    "        hospital_correction.iloc[i,j] = predict\n",
    "        count += 1\n",
    "    except:\n",
    "        predict = hospital_result.iloc[count,-1]\n",
    "        hospital_correction.iloc[i,j] = predict\n",
    "        count += 1\n",
    "# print(count,len(hospital_result))\n",
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "clean = hospital_clean.copy()\n",
    "dirty = hospital_dirty.copy()\n",
    "correction = hospital_correction.copy()\n",
    "for i in tqdm(range(len(clean))):\n",
    "# for i in tqdm(tax_error):\n",
    "    for j in range(clean.shape[1]):\n",
    "        dirty_cell = dirty.iloc[i,j]\n",
    "        clean_cell = clean.iloc[i,j]\n",
    "        correct_cell = correction.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell or correct_cell in clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Beers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9973190348525469, 0.9973190348525469, 0.9973190348525469)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file_path = 'inference/beer_test.csv' ## Replace the train file\n",
    "beer_result = pd.read_csv(test_file_path,index_col=0)\n",
    "\n",
    "count = 0\n",
    "beer_clean = pd.read_csv('raw_dataset/DC/beers/original/clean.csv').fillna('')\n",
    "beer_dirty = pd.read_csv('raw_dataset/DC/beers/original/dirty.csv').fillna('')\n",
    "detector_beer = np.load('raw_dataset/DC/beers/detector/detector.npy')\n",
    "beer_dirty.columns = beer_clean.columns\n",
    "def try_convert_to_int(row):\n",
    "    for x,y in row.items():\n",
    "        if(x in ['ounces','ibu']):\n",
    "            try:\n",
    "                row[x] = int(y)\n",
    "            except:\n",
    "                row[x] = y\n",
    "    return row\n",
    "beer_clean = beer_clean.apply(try_convert_to_int,axis=1).astype(str)\n",
    "beer_dirty = beer_dirty.apply(try_convert_to_int,axis=1).astype(str)\n",
    "beer_correction = beer_dirty.copy()\n",
    "for d in np.argwhere(detector_beer==1):\n",
    "    i = d[0] \n",
    "    j = d[1] + 2\n",
    "    \n",
    "    try:\n",
    "        predict = list(eval(beer_result.iloc[count,-1]).values())[0]\n",
    "        beer_correction.iloc[i,j] = predict\n",
    "        count += 1\n",
    "    except:\n",
    "        print(count)\n",
    "        count += 1\n",
    "# print(count,len(np.argwhere(detector_beer==1)),len(beer_result))\n",
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "for i in range(len(beer_clean)):\n",
    "    for j in range(11):\n",
    "        dirty_cell = beer_dirty.iloc[i,j]\n",
    "        clean_cell = beer_clean.iloc[i,j]\n",
    "        correct_cell = beer_correction.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Rayyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27622/2627275201.py:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  temp = row[index]\n",
      "/tmp/ipykernel_27622/2627275201.py:13: FutureWarning: Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`\n",
      "  row[index] = str(int(temp))\n",
      "/tmp/ipykernel_27622/2627275201.py:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  temp = row[index]\n",
      "/tmp/ipykernel_27622/2627275201.py:13: FutureWarning: Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`\n",
      "  row[index] = str(int(temp))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa09945dfd84547b94108ae267f1a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d588286bb14b7990a9160a66cd06d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.783001808318264, 0.9135021097046413, 0.8432327166504382)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Try to test recall on Rayyan Dataset\n",
    "import json\n",
    "test_file_path = 'inference_Transfer_ER/rayyan_train-MoE-Add--rayyan-test-20.csv'\n",
    "result = pd.read_csv(test_file_path,index_col=0)\n",
    "\n",
    "# result = pd.read_csv('inference_Transfer_ER/select--rayyan-test.csv',index_col=0)\n",
    "rayyan_detector = np.load('raw_dataset/DC/rayyan/detector/detector.npy')\n",
    "## If you run a different Error Detection result, change the \n",
    "\n",
    "rayyan_clean = pd.read_csv('raw_dataset/DC/rayyan/original/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('raw_dataset/DC/rayyan/original/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)\n",
    "count = 0\n",
    "valid_count = 0\n",
    "rayyan_correction = rayyan_dirty.copy()\n",
    "import ast\n",
    "for d in tqdm(np.argwhere(rayyan_detector==1)):\n",
    "    i = d[0]\n",
    "    j = d[1] + 1 ## Ignore Index\n",
    "    try:\n",
    "        predict = list(eval(result.iloc[count,-1]).values())[0]\n",
    "        rayyan_correction.iloc[i,j] = predict\n",
    "        valid_count += 1\n",
    "    except:\n",
    "        # print(result.iloc[count,-1])\n",
    "        predict = result.iloc[count,-1]\n",
    "        rayyan_correction.iloc[i,j] = predict\n",
    "    count += 1\n",
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "clean = rayyan_clean.copy()\n",
    "dirty = rayyan_dirty.copy()\n",
    "correction = rayyan_correction.copy()\n",
    "for i in tqdm(range(len(clean))):\n",
    "# for i in tqdm(tax_error):\n",
    "    for j in range(clean.shape[1]):\n",
    "        dirty_cell = dirty.iloc[i,j]\n",
    "        clean_cell = clean.iloc[i,j]\n",
    "        correct_cell = correction.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell or correct_cell in clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Type Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimTab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=4,5,6,7 python vllm_inference_mistral_api.py --directory lora_weight/Expert/CTA/CTA_SimTab_train_init --gpu_num 4 --file dataset/CTA/SimTab_test_few.json --max_token 128 --json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=4,5,6,7 python vllm_inference_mistral_api.py --directory lora_weight/Expert/CTA/CTA_WebTable_train_init --gpu_num 4 --file dataset/CTA/WebTable_Test_few.json --max_token 128 --json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8935611038107752, 0.761725380779237)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file_path = 'inference/SimTab_test_few.csv'\n",
    "all_relation = np.load('raw_dataset/CTA/SimTab/sim_all_relation.npy',allow_pickle=True)\n",
    "relation_dict = {}\n",
    "for i in range(len(all_relation)):\n",
    "    relation_dict[all_relation[i]] = i\n",
    "SimTab_test = pd.read_csv(test_file_path,index_col=0)\n",
    "def Ast(row):\n",
    "    truth = list(eval(row['output']).values())[0]\n",
    "    try:\n",
    "        pred = list(eval(row['predict']).values())[0]\n",
    "    except:\n",
    "        pred = ''\n",
    "    index = row['index']\n",
    "    return truth, pred, index\n",
    "SimTab_test_Transform = SimTab_test.apply(Ast,axis=1,result_type='expand')\n",
    "SimTab_test_Transform.columns = ['truth','pred','index']\n",
    "count = 0\n",
    "truth_list = []\n",
    "pred_list = []\n",
    "for i in range(len(SimTab_test_Transform['index'].unique())):\n",
    "    select_df = SimTab_test_Transform[SimTab_test_Transform['index'] == i]\n",
    "    truth = select_df.iloc[0,0]\n",
    "    select_df_filter = select_df[select_df['pred'].isin(all_relation)]\n",
    "    try:\n",
    "        pred = select_df_filter['pred'].value_counts().idxmax()\n",
    "    except:\n",
    "        pred = select_df['pred'].value_counts().idxmax()\n",
    "    truth_list.append(truth)\n",
    "    pred_list.append(pred)\n",
    "    if truth==pred:\n",
    "        count += 1\n",
    "    # else:\n",
    "    #     print(truth,select_df['pred'].value_counts().idxmax())\n",
    "# count / len(SimTab_test_Transform['index'].unique()) ## \n",
    "SimTab_F1 = pd.DataFrame()\n",
    "SimTab_F1['pred_output'] = pred_list\n",
    "SimTab_F1['truth_output'] = truth_list\n",
    "from sklearn.metrics import f1_score\n",
    "pred = SimTab_F1['pred_output'].map(relation_dict).fillna(0).to_list()\n",
    "truth = SimTab_F1['truth_output'].map(relation_dict).to_list()\n",
    "f1_score(y_pred=pred,y_true=truth,average='micro'),f1_score(y_pred=pred,y_true=truth,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WebTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9643463497453311, 0.7877228958461471)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file_path = 'inference/WebTable_Test_few.csv'\n",
    "all_relation = np.load('raw_dataset/CTA/WebTable/webtable_all_relation.npy',allow_pickle=True)\n",
    "relation_dict = {}\n",
    "for i in range(len(all_relation)):\n",
    "    relation_dict[all_relation[i]] = i\n",
    "SimTab_test = pd.read_csv(test_file_path,index_col=0)\n",
    "def Ast(row):\n",
    "    truth = list(eval(row['output']).values())[0]\n",
    "    try:\n",
    "        pred = list(eval(row['predict']).values())[0]\n",
    "    except:\n",
    "        pred = ''\n",
    "    index = row['index']\n",
    "    return truth, pred, index\n",
    "SimTab_test_Transform = SimTab_test.apply(Ast,axis=1,result_type='expand')\n",
    "SimTab_test_Transform.columns = ['truth','pred','index']\n",
    "count = 0\n",
    "truth_list = []\n",
    "pred_list = []\n",
    "for i in range(len(SimTab_test_Transform['index'].unique())):\n",
    "    select_df = SimTab_test_Transform[SimTab_test_Transform['index'] == i]\n",
    "    truth = select_df.iloc[0,0]\n",
    "    select_df_filter = select_df[select_df['pred'].isin(all_relation)]\n",
    "    try:\n",
    "        pred = select_df_filter['pred'].value_counts().idxmax()\n",
    "    except:\n",
    "        pred = select_df['pred'].value_counts().idxmax()\n",
    "    truth_list.append(truth)\n",
    "    pred_list.append(pred)\n",
    "    if truth==pred:\n",
    "        count += 1\n",
    "    # else:\n",
    "    #     print(truth,select_df['pred'].value_counts().idxmax())\n",
    "# count / len(SimTab_test_Transform['index'].unique()) ## \n",
    "SimTab_F1 = pd.DataFrame()\n",
    "SimTab_F1['pred_output'] = pred_list\n",
    "SimTab_F1['truth_output'] = truth_list\n",
    "from sklearn.metrics import f1_score\n",
    "pred = SimTab_F1['pred_output'].map(relation_dict).fillna(0).to_list()\n",
    "truth = SimTab_F1['truth_output'].map(relation_dict).to_list()\n",
    "f1_score(y_pred=pred,y_true=truth,average='micro'),f1_score(y_pred=pred,y_true=truth,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Relation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RE have multiple correct choices, so we only evalutate precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=2,3 python vllm_inference_mistral_api.py --directory lora_weight/Expert/RE/RE-MoE-Add --gpu_num 2 --file dataset/RE/RE-test.json --max_token 128 --json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.900096525096525"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file_path = 'inference/RE-test.csv'\n",
    "# all_relation = np.load('raw_dataset/RE/',allow_pickle=True)\n",
    "RE_test = pd.read_csv('raw_dataset/RE/RE_test.csv',index_col=0) ## Multiple-Choice Ground Truth\n",
    "SimTab_test = pd.read_csv(test_file_path,index_col=0)\n",
    "SimTab_test['index'] = SimTab_test.index\n",
    "def Ast(row):\n",
    "    truth = list(eval(row['output']).values())[0]\n",
    "    try:\n",
    "        pred = list(eval(row['predict']).values())[0]\n",
    "    except:\n",
    "        pred = ''\n",
    "    index = row['index']\n",
    "    return truth, pred, index\n",
    "SimTab_test_Transform = SimTab_test.apply(Ast,axis=1,result_type='expand')\n",
    "SimTab_test_Transform.columns = ['truth','pred','index']\n",
    "count = 0\n",
    "truth_list = []\n",
    "pred_list = []\n",
    "for i in range(len(SimTab_test_Transform['index'].unique())):\n",
    "    select_df = SimTab_test_Transform[SimTab_test_Transform['index'] == i]\n",
    "    # truth = select_df.iloc[0,0]\n",
    "    truth = eval(RE_test.iloc[i,1])\n",
    "    select_df_filter = select_df[select_df['pred'].isin(all_relation)]\n",
    "    try:\n",
    "        pred = select_df_filter['pred'].value_counts().idxmax()\n",
    "    except:\n",
    "        pred = select_df['pred'].value_counts().idxmax()\n",
    "    truth_list.append(truth)\n",
    "    pred_list.append(pred)\n",
    "    if truth.__contains__(pred):\n",
    "        count += 1\n",
    "    # else:\n",
    "    #     print(truth,select_df['pred'].value_counts().idxmax())\n",
    "count / len(SimTab_test_Transform['index'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for Schema Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=6,7 python vllm_inference_mistral_api.py --directory lora_weight/Expert/SM/CMS_train-MoE-Add --gpu_num 2 --file dataset/SM/CMS-test.json --max_token 32 --json --guided_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=4,5 python vllm_inference_mistral_api.py --directory lora_weight/Expert/SM/synthea_train-MoE-Add --gpu_num 2 --file dataset/SM/synthea-test.json --max_token 32 --json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4838709677419355 0.7142857142857143 0.5769230769230769\n"
     ]
    }
   ],
   "source": [
    "# test_file_path = 'inference/CMS-test.csv' ## replace with the inference file\n",
    "test_file_path = 'inference/synthea-test.csv' ## replace with the inference file\n",
    "\n",
    "result_merge = pd.read_csv(test_file_path,index_col=0).fillna('')\n",
    "def Transfer(row):\n",
    "    if(row['output'].__contains__('mismatch')) or (row['output'].__contains__('dismatch')):\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    if(row['predict'].__contains__('mismatch')) or (row['predict'].__contains__('dismatch')):\n",
    "        predict = 0\n",
    "    else:\n",
    "        predict = 1\n",
    "    return label,predict\n",
    "result_output = result_merge.apply(Transfer,axis=1,result_type='expand')\n",
    "from sklearn.metrics import f1_score, precision_score,recall_score\n",
    "print(precision_score(y_true=result_output[0],y_pred=result_output[1]),recall_score(y_true=result_output[0],y_pred=result_output[1]),f1_score(y_true=result_output[0],y_pred=result_output[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=6,7 python vllm_inference_mistral_api.py --directory lora_weight/Expert/DI/restaurant_train-MoE-Add --gpu_num 2 --file dataset/DI/restaurant_test_output_wide.json --max_token 128 --json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=6,7 python vllm_inference_mistral_api.py --directory lora_weight/Expert/DI/walmart_train-MoE-Add --gpu_num 2 --file dataset/DI/walmart_test_output_wide.json --max_token 128 --json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restaurant\n",
    "- we strip the string and blank at the beginning/end, and replace certain synonym, e.g. `new york city` and `new york`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def Restaurant_string_clean(text):\n",
    "    text =  text.replace('new york city','new york')\n",
    "    pattern = r\"^[^a-zA-Z]*(.*?)\\s*'$\"\n",
    "    result = re.match(pattern, text)\n",
    "    if result:\n",
    "        cleaned_text = result.group(1)\n",
    "        return cleaned_text  \n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_file_path = 'inference/restaurant_test_output_wide.csv'\n",
    "train_file_path = 'inference/walmart_test_output_wide.csv'\n",
    "result = pd.read_csv(train_file_path,index_col=0)\n",
    "# walmart_DI_result[walmart_DI_result['output']!=walmart_DI_result['predict']]\n",
    "import ast\n",
    "def AST(row):\n",
    "    output = list(ast.literal_eval(row['output']).values())[0]\n",
    "    output = Restaurant_string_clean(output)\n",
    "    try:\n",
    "        predict = list(ast.literal_eval(row['predict'].strip()).values())[0]\n",
    "        predict = Restaurant_string_clean(predict)\n",
    "    except:\n",
    "        print(row['predict'])\n",
    "        predict  = ''\n",
    "    return output,predict\n",
    "walmart_DI_ast = result.apply(AST,axis=1,result_type='expand')\n",
    "len(walmart_DI_ast[walmart_DI_ast[0]==walmart_DI_ast[1]]) / len(walmart_DI_ast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for Atttibute Value Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8049775601795186"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ave_train = pd.read_csv('inference/oa_mine-test.csv',index_col=0)\n",
    "import ast\n",
    "def AST(row):\n",
    "    output = row['output'].strip()\n",
    "    predict = row['predict'].strip()\n",
    "    output_item = list(ast.literal_eval(output).values())[0]\n",
    "    predict_item = list(ast.literal_eval(predict).values())[0]\n",
    "    row['output_item'] = output_item.lower()\n",
    "    row['predict_item'] = predict_item.lower()\n",
    "    return row\n",
    "ave_train = ave_train.apply(AST,axis=1)\n",
    "len(ave_train[ave_train['output_item']==ave_train['predict_item']]) / len(ave_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
