{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "tqdm.pandas(desc='pandas bar')\n",
    "# from pandarallel import pandarallel\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "import logging\n",
    "# pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "from FlagEmbedding import FlagModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here we transfer the data for Baseline in MTL solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge and Shuffle\n",
    "amazon_google = pd.read_json('data/MoE/ER/amazon-google-train.json')\n",
    "ant_buy = pd.read_json('data/MoE/ER/ant_buy_train_output.json')\n",
    "semi_text_c = pd.read_json('data/MoE/ER/semi-text-c-train-MoE.json')\n",
    "semi_text_w = pd.read_json('data/MoE/ER/semi-text-w-train-MoE.json')\n",
    "walmart_amazon = pd.read_json('data/MoE/ER/walmart_amazon_train_output.json')\n",
    "wdc_all = pd.read_json('data/MoE/ER/wdc_all_train_output.json')\n",
    "hospital_train = pd.read_json('data/MoE/hospital-train-MoE.json')\n",
    "beer_train = pd.read_json('data/MoE/beer-train-MoE.json')\n",
    "rayyan_train = pd.read_json('data/MoE/rayyan-train-MoE.json')\n",
    "RE = pd.read_json('data/RE/RE-train_t=4.json')\n",
    "CMS_train = pd.read_json('data/MoE/SM/CMS_train_output.json')\n",
    "mimic_train = pd.read_json('data/MoE/SM/mimic_train_output.json')\n",
    "synthea_train = pd.read_json('data/MoE/SM/synthea_train_output.json')\n",
    "walmart_train = pd.read_json('data/MoE/DI/walmart_train_output_wide.json')\n",
    "amazon_train = pd.read_json('data/MoE/DI/amazon_train_output_wide.json')\n",
    "restaurant_train = pd.read_json('data/MoE/DI/restaurant_train_output_wide.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_google_train = pd.read_csv('LLM_ER/process_data/amazon-google/amazon_google_train_filter.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataframe_with_conditions(df,pos_num=100,neg_num=100):\n",
    "    \"\"\"\n",
    "    Sample 200 rows from a DataFrame with specific conditions:\n",
    "    - 100 rows with label=1, where (ltable, rtable) are unique.\n",
    "    - 100 rows with label=0, where (ltable, rtable) are unique.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame with 'ltable', 'rtable', and 'label' columns.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A sampled DataFrame satisfying the conditions with the same structure as the input DataFrame.\n",
    "    \"\"\"\n",
    "    # 分组数据框\n",
    "    grouped_df = df.groupby('label')\n",
    "    \n",
    "    # 从label=1的组中选择100行，确保(ltable, rtable)互不相同\n",
    "    sample_label_1 = grouped_df.get_group(1).groupby(['ltable_id', 'rtable_id']).apply(lambda x: x.sample(1))\n",
    "    sample_label_1 = sample_label_1.head(pos_num)\n",
    "    \n",
    "    # 从label=0的组中选择100行，确保(ltable, rtable)互不相同\n",
    "    sample_label_0 = grouped_df.get_group(0).groupby(['ltable_id', 'rtable_id']).apply(lambda x: x.sample(1))\n",
    "    sample_label_0 = sample_label_0.head(neg_num)\n",
    "    \n",
    "    # 合并两组样本\n",
    "    sampled_df = pd.concat([sample_label_1, sample_label_0])\n",
    "    \n",
    "    # 重置索引，确保索引是唯一的\n",
    "    sampled_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_google_fewshot = sample_dataframe_with_conditions(amazon_google_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableA = pd.read_csv('LLM_ER/process_data/amazon-google/amazon-google/tableA.csv').fillna('')\n",
    "tableB = pd.read_csv('LLM_ER/process_data/amazon-google/amazon-google/tableB.csv').fillna('')\n",
    "tableA_dict = tableA.set_index('id').to_dict(orient='records')\n",
    "tableB_dict = tableB.set_index('id').to_dict(orient='records')\n",
    "train = pd.read_csv('LLM_ER/process_data/amazon-google/amazon-google/train.csv')\n",
    "valid = pd.read_csv('LLM_ER/process_data/amazon-google/amazon-google/valid.csv')\n",
    "test = pd.read_csv('LLM_ER/process_data/amazon-google/amazon-google/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 200/200 [00:00<00:00, 14229.07it/s]\n",
      "pandas bar: 100%|██████████| 2293/2293 [00:00<00:00, 20877.47it/s]\n"
     ]
    }
   ],
   "source": [
    "import json,ast\n",
    "def Prompt_Generalize_Baseline(row): ## 在train筛选ltable_id and rtbale_id中所有类似的pos/neg放进去，但是不能完全相同，如果没有pos，那么从ground truth选4个，总数维持8个\n",
    "    temp_dict = {}\n",
    "    temp_dict['Output'] = ''\n",
    "    RAG_text = ''\n",
    "    ltable_id = row['ltable_id']\n",
    "    rtable_id = row['rtable_id']\n",
    "    text = 'You are an expert in detecting if two text descriptions of entities refer to the same or different entities. You are well-known to examine the fine details and can find subtle matches or mismatches to help you arrive at the final judgement. \\n\\nJudge whether Entity 1 and Entity 2 are match or dismatch, and choose within the given Options.\\n\\nReturn in JSON format.\\n\\nOptions: [match,dismatch]\\n\\nOutput format example:%s\\n\\nEntity 1:%s\\n\\nEntity 2:%s' % (json.dumps(temp_dict),json.dumps(tableA_dict[ltable_id]),json.dumps(tableB_dict[rtable_id]))\n",
    "    output_dict = {}\n",
    "    output_dict['Output'] = 'dismatch' if row['label']==0 else 'match'\n",
    "    return text + RAG_text, '' , str(output_dict)\n",
    "amazon_google_fewshot_output = amazon_google_fewshot.progress_apply(Prompt_Generalize_Baseline,axis=1,result_type='expand')\n",
    "amazon_google_fewshot_output_test = test.progress_apply(Prompt_Generalize_Baseline,axis=1,result_type='expand')           \n",
    "# amazon_google_output = amazon_google_test.progress_apply(Prompt_Generalize,axis=1,result_type='expand')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 6612/6612 [00:03<00:00, 2032.78it/s]\n",
      "pandas bar: 100%|██████████| 2293/2293 [00:01<00:00, 2045.09it/s]\n"
     ]
    }
   ],
   "source": [
    "import json,ast\n",
    "df = amazon_google_train\n",
    "def Prompt_Generalize_Baseline_NoMeta(row): ## 在train筛选ltable_id and rtbale_id中所有类似的pos/neg放进去，但是不能完全相同，如果没有pos，那么从ground truth选4个，总数维持8个\n",
    "    temp_dict = {}\n",
    "    temp_dict['Output'] = ''\n",
    "    RAG_text = ''\n",
    "    ltable_id = row['ltable_id']\n",
    "    rtable_id = row['rtable_id']\n",
    "\n",
    "    text = 'You are an expert in detecting if two text descriptions of entities refer to the same or different entities. You are well-known to examine the fine details and can find subtle matches or mismatches to help you arrive at the final judgement. \\n\\nJudge whether Entity 1 and Entity 2 are match or dismatch, and choose within the given Options.\\n\\nReturn in JSON format.\\n\\nOptions: [match,dismatch]\\n\\nOutput format example:%s\\n\\nEntity 1:%s\\n\\nEntity 2:%s' % (json.dumps(temp_dict),json.dumps(tableA_dict[ltable_id]),json.dumps(tableB_dict[rtable_id]))\n",
    "    output_dict = {}\n",
    "    output_dict['Output'] = 'dismatch' if row['label']==0 else 'match'\n",
    "    sample_df = df.sample(n=5)\n",
    "    RAG_text += '\\n\\nTake these examples as reference:\\n\\n'\n",
    "    for index,row_ in sample_df.iterrows():\n",
    "        ltable_id = row_['ltable_id']\n",
    "        rtable_id = row_['rtable_id']\n",
    "        RAG_dict = {}\n",
    "        RAG_dict['Entity_1'] = str(tableA_dict[ltable_id])\n",
    "        RAG_dict['Entity_2'] = str(tableB_dict[rtable_id])\n",
    "        RAG_dict['Output'] = 'dismatch' if row_['label']==0 else 'match'\n",
    "        RAG_text += '%s\\n\\n' % str(RAG_dict)\n",
    "    return text + RAG_text, '' , str(output_dict)\n",
    "amazon_google_fewshot_output = amazon_google_train.progress_apply(Prompt_Generalize_Baseline_NoMeta,axis=1,result_type='expand')\n",
    "amazon_google_fewshot_output_test = test.progress_apply(Prompt_Generalize_Baseline_NoMeta,axis=1,result_type='expand')           \n",
    "# amazon_google_output = amazon_google_test.progress_apply(Prompt_Generalize,axis=1,result_type='expand')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_google_fewshot.to_csv('LLM_ER/process_data/baseline/amazon_google_fewshot.csv')\n",
    "semi_text_w_fewshot.to_csv('LLM_ER/process_data/baseline/semi_text_w_fewshot.csv')\n",
    "semi_text_c_train_fewshot.to_csv('LLM_ER/process_data/baseline/semi_text_c_train_fewshot.csv')\n",
    "walmart_amazon_train_fewshot.to_csv('LLM_ER/process_data/baseline/walmart_amazon_train_fewshot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_google_fewshot_output.columns = ['instruction','input','output']\n",
    "amazon_google_fewshot_output_test.columns = ['instruction','input','output']\n",
    "json.dump(amazon_google_fewshot_output.to_dict(orient='records'), open('data/MoE_no_meta/ER/amazon_google_train.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(amazon_google_fewshot_output_test.to_dict(orient='records'), open('data/MoE_no_meta/ER/amazon_google_test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-Text-W-Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_text_w_test = pd.read_csv('LLM_ER/process_data/semi-text-w/semi-text-w/test.csv',header=None)\n",
    "semi_text_w_test.columns = ['ltable_id','rtable_id','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_text_w_train = pd.read_csv('LLM_ER/process_data/semi-text-w/train_add.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ltable = pd.read_json('LLM_ER/process_data/semi-text-w/semi-text-w/left.json').fillna('')\n",
    "dict_ltable['index'] = dict_ltable.index\n",
    "dict_ltable = dict_ltable.set_index('index').to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e579624d37eb47beb9b1292f27bd3138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "read text entity...:   0%|          | 0/9234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dict_rtable = pd.read_table('LLM_ER/process_data/semi-text-w/semi-text-w/right.txt',encoding='utf-8')\n",
    "entities = []\n",
    "file_path = \"LLM_ER/process_data/semi-text-w/semi-text-w/right.txt\"\n",
    "with open(file_path, \"r\",encoding='utf-8') as rd:\n",
    "    lines = rd.readlines()\n",
    "    for line in tqdm(lines, desc=\"read text entity...\"):\n",
    "        text = line.strip()\n",
    "        entities.append(text)\n",
    "dict_rtable = entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_text_w_fewshot = sample_dataframe_with_conditions(semi_text_w_train,pos_num=80,neg_num=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 240/240 [00:00<00:00, 5474.04it/s]\n",
      "pandas bar:   0%|          | 0/1846 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 1846/1846 [00:00<00:00, 14811.19it/s]\n"
     ]
    }
   ],
   "source": [
    "import json,ast\n",
    "def Prompt_Generalize_Baseline(row): ## 在train筛选ltable_id and rtbale_id中所有类似的pos/neg放进去，但是不能完全相同，如果没有pos，那么从ground truth选4个，总数维持8个\n",
    "    temp_dict = {}\n",
    "    temp_dict['Output'] = ''\n",
    "    RAG_text = ''\n",
    "    ltable_id = row['ltable_id']\n",
    "    rtable_id = row['rtable_id']\n",
    "    text = 'You are an expert in detecting if two text descriptions of entities refer to the same or different entities. You are well-known to examine the fine details and can find subtle matches or mismatches to help you arrive at the final judgement. \\n\\nJudge whether Entity 1 and Entity 2 are match or dismatch, and choose within the given Options.\\n\\nReturn in JSON format.\\n\\nOptions: [match,dismatch]\\n\\nOutput format example:%s\\n\\nEntity 1:%s\\n\\nEntity 2:%s' % (json.dumps(temp_dict),json.dumps(dict_ltable[ltable_id]),json.dumps(dict_rtable[rtable_id]))\n",
    "    output_dict = {}\n",
    "    output_dict['Output'] = 'dismatch' if row['label']==0 else 'match'\n",
    "    return text + RAG_text, '' , str(output_dict)\n",
    "semi_text_w_fewshot_output = semi_text_w_fewshot.progress_apply(Prompt_Generalize_Baseline,axis=1,result_type='expand')\n",
    "semi_text_w_test_output = semi_text_w_test.progress_apply(Prompt_Generalize_Baseline,axis=1,result_type='expand')           \n",
    "# amazon_google_output = amazon_google_test.progress_apply(Prompt_Generalize,axis=1,result_type='expand')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 9187/9187 [00:05<00:00, 1717.17it/s]\n",
      "pandas bar: 100%|██████████| 1846/1846 [00:01<00:00, 1705.40it/s]\n"
     ]
    }
   ],
   "source": [
    "import json,ast\n",
    "df = semi_text_w_train\n",
    "def Prompt_Generalize_Baseline_NoMeta(row): ## 在train筛选ltable_id and rtbale_id中所有类似的pos/neg放进去，但是不能完全相同，如果没有pos，那么从ground truth选4个，总数维持8个\n",
    "    temp_dict = {}\n",
    "    temp_dict['Output'] = ''\n",
    "    RAG_text = ''\n",
    "    ltable_id = row['ltable_id']\n",
    "    rtable_id = row['rtable_id']\n",
    "    text = 'You are an expert in detecting if two text descriptions of entities refer to the same or different entities. You are well-known to examine the fine details and can find subtle matches or mismatches to help you arrive at the final judgement. \\n\\nJudge whether Entity 1 and Entity 2 are match or dismatch, and choose within the given Options.\\n\\nReturn in JSON format.\\n\\nOptions: [match,dismatch]\\n\\nOutput format example:%s\\n\\nEntity 1:%s\\n\\nEntity 2:%s' % (json.dumps(temp_dict),json.dumps(dict_ltable[ltable_id]),json.dumps(dict_rtable[rtable_id]))\n",
    "    output_dict = {}\n",
    "    output_dict['Output'] = 'dismatch' if row['label']==0 else 'match'\n",
    "    sample_df = df.sample(n=5)\n",
    "    RAG_text += '\\n\\nTake these examples as reference:\\n\\n'\n",
    "    for index,row_ in sample_df.iterrows():\n",
    "        ltable_id = row_['ltable_id']\n",
    "        rtable_id = row_['rtable_id']\n",
    "        RAG_dict = {}\n",
    "        RAG_dict['Entity_1'] = str(dict_ltable[ltable_id])\n",
    "        RAG_dict['Entity_2'] = str(dict_rtable[rtable_id])\n",
    "        RAG_dict['Output'] = 'dismatch' if row_['label']==0 else 'match'\n",
    "        RAG_text += '%s\\n\\n' % str(RAG_dict)\n",
    "    return text + RAG_text, '' , str(output_dict)\n",
    "    # return text + RAG_text, '' , str(output_dict)\n",
    "semi_text_w_fewshot_output = semi_text_w_train.progress_apply(Prompt_Generalize_Baseline_NoMeta,axis=1,result_type='expand')\n",
    "semi_text_w_test_output = semi_text_w_test.progress_apply(Prompt_Generalize_Baseline_NoMeta,axis=1,result_type='expand')           \n",
    "# amazon_google_output = amazon_google_test.progress_apply(Prompt_Generalize,axis=1,result_type='expand')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = semi_text_w_fewshot_output\n",
    "test_df = semi_text_w_test_output\n",
    "train_df.columns = ['instruction','input','output']\n",
    "test_df.columns = ['instruction','input','output']\n",
    "json.dump(train_df.to_dict(orient='records'), open('data/MoE_no_meta/ER/semi_text_w_train.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(test_df.to_dict(orient='records'), open('data/MoE_no_meta/ER/semi_text_w_test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf287b4657ab492eaf76091b94f0df8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "read text entity...:   0%|          | 0/20897 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Semi-Text-C\n",
    "semi_text_c_train = pd.read_csv('LLM_ER/process_data/semi-text-c/train_add.csv',index_col=0)\n",
    "semi_text_c_test = pd.read_csv('LLM_ER/process_data/semi-text-c/semi-text-c/test.csv',header=None)\n",
    "dict_ltable = pd.read_json('LLM_ER/process_data/semi-text-c/semi-text-c/left.json').fillna('')\n",
    "dict_ltable['index'] = dict_ltable.index\n",
    "dict_ltable = dict_ltable.set_index('index').to_dict(orient='records')\n",
    "entities = []\n",
    "file_path = \"LLM_ER/process_data/semi-text-c/semi-text-c/right.txt\"\n",
    "with open(file_path, \"r\",encoding='utf-8') as rd:\n",
    "    lines = rd.readlines()\n",
    "    for line in tqdm(lines, desc=\"read text entity...\"):\n",
    "        text = line.strip()\n",
    "        entities.append(text)\n",
    "dict_rtable = entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_text_c_train.columns = ['ltable_id','rtable_id','label']\n",
    "semi_text_c_test.columns = ['ltable_id','rtable_id','label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_text_c_train_fewshot = sample_dataframe_with_conditions(semi_text_c_train,pos_num=80,neg_num=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 240/240 [00:00<00:00, 5671.40it/s]\n",
      "pandas bar: 100%|██████████| 4179/4179 [00:00<00:00, 18401.90it/s]\n"
     ]
    }
   ],
   "source": [
    "import json,ast\n",
    "def Prompt_Generalize_Baseline(row): ## 在train筛选ltable_id and rtbale_id中所有类似的pos/neg放进去，但是不能完全相同，如果没有pos，那么从ground truth选4个，总数维持8个\n",
    "    temp_dict = {}\n",
    "    temp_dict['Output'] = ''\n",
    "    RAG_text = ''\n",
    "    ltable_id = row['ltable_id']\n",
    "    rtable_id = row['rtable_id']\n",
    "    text = 'You are an expert in detecting if two text descriptions of entities refer to the same or different entities. You are well-known to examine the fine details and can find subtle matches or mismatches to help you arrive at the final judgement. \\n\\nJudge whether Entity 1 and Entity 2 are match or dismatch, and choose within the given Options.\\n\\nReturn in JSON format.\\n\\nOptions: [match,dismatch]\\n\\nOutput format example:%s\\n\\nEntity 1:%s\\n\\nEntity 2:%s' % (json.dumps(temp_dict),json.dumps(dict_ltable[ltable_id]),json.dumps(dict_rtable[rtable_id]))\n",
    "    output_dict = {}\n",
    "    output_dict['Output'] = 'dismatch' if row['label']==0 else 'match'\n",
    "    return text + RAG_text, '' , str(output_dict)\n",
    "semi_text_c_fewshot_output = semi_text_c_train_fewshot.progress_apply(Prompt_Generalize_Baseline,axis=1,result_type='expand')\n",
    "semi_text_c_test_output = semi_text_c_test.progress_apply(Prompt_Generalize_Baseline,axis=1,result_type='expand')           \n",
    "# amazon_google_output = amazon_google_test.progress_apply(Prompt_Generalize,axis=1,result_type='expand')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 6174/6174 [00:03<00:00, 1990.94it/s]\n",
      "pandas bar: 100%|██████████| 4179/4179 [00:02<00:00, 2031.87it/s]\n"
     ]
    }
   ],
   "source": [
    "import json,ast\n",
    "df = semi_text_c_train\n",
    "def Prompt_Generalize_Baseline_NoMeta(row): ## 在train筛选ltable_id and rtbale_id中所有类似的pos/neg放进去，但是不能完全相同，如果没有pos，那么从ground truth选4个，总数维持8个\n",
    "    temp_dict = {}\n",
    "    temp_dict['Output'] = ''\n",
    "    RAG_text = ''\n",
    "    ltable_id = row['ltable_id']\n",
    "    rtable_id = row['rtable_id']\n",
    "    text = 'You are an expert in detecting if two text descriptions of entities refer to the same or different entities. You are well-known to examine the fine details and can find subtle matches or mismatches to help you arrive at the final judgement. \\n\\nJudge whether Entity 1 and Entity 2 are match or dismatch, and choose within the given Options.\\n\\nReturn in JSON format.\\n\\nOptions: [match,dismatch]\\n\\nOutput format example:%s\\n\\nEntity 1:%s\\n\\nEntity 2:%s' % (json.dumps(temp_dict),json.dumps(dict_ltable[ltable_id]),json.dumps(dict_rtable[rtable_id]))\n",
    "    output_dict = {}\n",
    "    output_dict['Output'] = 'dismatch' if row['label']==0 else 'match'\n",
    "    sample_df = df.sample(n=5)\n",
    "    RAG_text += '\\n\\nTake these examples as reference:\\n\\n'\n",
    "    for index,row_ in sample_df.iterrows():\n",
    "        ltable_id = row_['ltable_id']\n",
    "        rtable_id = row_['rtable_id']\n",
    "        RAG_dict = {}\n",
    "        RAG_dict['Entity_1'] = str(dict_ltable[ltable_id])\n",
    "        RAG_dict['Entity_2'] = str(dict_rtable[rtable_id])\n",
    "        RAG_dict['Output'] = 'dismatch' if row_['label']==0 else 'match'\n",
    "        RAG_text += '%s\\n\\n' % str(RAG_dict)\n",
    "    return text + RAG_text, '' , str(output_dict)\n",
    "semi_text_c_fewshot_output = semi_text_c_train.progress_apply(Prompt_Generalize_Baseline_NoMeta,axis=1,result_type='expand')\n",
    "semi_text_c_test_output = semi_text_c_test.progress_apply(Prompt_Generalize_Baseline_NoMeta,axis=1,result_type='expand')           \n",
    "# amazon_google_output = amazon_google_test.progress_apply(Prompt_Generalize,axis=1,result_type='expand')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = semi_text_c_fewshot_output\n",
    "test_df = semi_text_c_test_output\n",
    "train_df.columns = ['instruction','input','output']\n",
    "test_df.columns = ['instruction','input','output']\n",
    "json.dump(train_df.to_dict(orient='records'), open('data/MoE_no_meta/ER/semi_text_c_train.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(test_df.to_dict(orient='records'), open('data/MoE_no_meta/ER/semi_text_c_test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walmart-Amazon-Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_amazon_train = pd.read_csv('LLM_ER/process_data/walmart-amazon/train.csv',index_col=0)\n",
    "walmart_amazon_test = pd.read_csv('LLM_ER/process_data/walmart-amazon/test_align.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableA = pd.read_csv('LLM_ER/process_data/walmart-amazon/exp_data/tableA.csv').fillna('')\n",
    "tableB = pd.read_csv('LLM_ER/process_data/walmart-amazon/exp_data/tableB.csv').fillna('')\n",
    "dict_ltable = tableA.set_index('id').to_dict(orient='records')\n",
    "dict_rtable = tableB.set_index('id').to_dict(orient='records')\n",
    "# train = pd.read_csv('LLM_ER/process_data/amazon-google/amazon-google/train.csv')\n",
    "# valid = pd.read_csv('LLM_ER/process_data/amazon-google/amazon-google/valid.csv')\n",
    "test = pd.read_csv('LLM_ER/process_data/walmart-amazon/exp_data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_amazon_train_fewshot = sample_dataframe_with_conditions(walmart_amazon_train,pos_num=100,neg_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 200/200 [00:00<00:00, 5482.28it/s]\n",
      "pandas bar: 100%|██████████| 2049/2049 [00:00<00:00, 15684.78it/s]\n"
     ]
    }
   ],
   "source": [
    "import json,ast\n",
    "def Prompt_Generalize_Baseline(row): ## 在train筛选ltable_id and rtbale_id中所有类似的pos/neg放进去，但是不能完全相同，如果没有pos，那么从ground truth选4个，总数维持8个\n",
    "    temp_dict = {}\n",
    "    temp_dict['Output'] = ''\n",
    "    RAG_text = ''\n",
    "    ltable_id = row['ltable_id']\n",
    "    rtable_id = row['rtable_id']\n",
    "    text = 'You are an expert in detecting if two text descriptions of entities refer to the same or different entities. You are well-known to examine the fine details and can find subtle matches or mismatches to help you arrive at the final judgement. \\n\\nJudge whether Entity 1 and Entity 2 are match or dismatch, and choose within the given Options.\\n\\nReturn in JSON format.\\n\\nOptions: [match,dismatch]\\n\\nOutput format example:%s\\n\\nEntity 1:%s\\n\\nEntity 2:%s' % (json.dumps(temp_dict),json.dumps(dict_ltable[ltable_id]),json.dumps(dict_rtable[rtable_id]))\n",
    "    output_dict = {}\n",
    "    output_dict['Output'] = 'dismatch' if row['label']==0 else 'match'\n",
    "    return text + RAG_text, '' , str(output_dict)\n",
    "walmart_amazon_train_fewshot_output = walmart_amazon_train_fewshot.progress_apply(Prompt_Generalize_Baseline,axis=1,result_type='expand')\n",
    "walmart_amazon_test = walmart_amazon_test.progress_apply(Prompt_Generalize_Baseline,axis=1,result_type='expand')           \n",
    "# amazon_google_output = amazon_google_test.progress_apply(Prompt_Generalize,axis=1,result_type='expand')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 7103/7103 [00:03<00:00, 1921.59it/s]\n",
      "pandas bar: 100%|██████████| 2049/2049 [00:01<00:00, 1925.55it/s]\n"
     ]
    }
   ],
   "source": [
    "import json,ast\n",
    "df = walmart_amazon_train\n",
    "def Prompt_Generalize_Baseline_NoMeta(row): ## 在train筛选ltable_id and rtbale_id中所有类似的pos/neg放进去，但是不能完全相同，如果没有pos，那么从ground truth选4个，总数维持8个\n",
    "    temp_dict = {}\n",
    "    temp_dict['Output'] = ''\n",
    "    RAG_text = ''\n",
    "    ltable_id = row['ltable_id']\n",
    "    rtable_id = row['rtable_id']\n",
    "    text = 'You are an expert in detecting if two text descriptions of entities refer to the same or different entities. You are well-known to examine the fine details and can find subtle matches or mismatches to help you arrive at the final judgement. \\n\\nJudge whether Entity 1 and Entity 2 are match or dismatch, and choose within the given Options.\\n\\nReturn in JSON format.\\n\\nOptions: [match,dismatch]\\n\\nOutput format example:%s\\n\\nEntity 1:%s\\n\\nEntity 2:%s' % (json.dumps(temp_dict),json.dumps(dict_ltable[ltable_id]),json.dumps(dict_rtable[rtable_id]))\n",
    "    output_dict = {}\n",
    "    output_dict['Output'] = 'dismatch' if row['label']==0 else 'match'\n",
    "    sample_df = df.sample(n=5)\n",
    "    RAG_text += '\\n\\nTake these examples as reference:\\n\\n'\n",
    "    for index,row_ in sample_df.iterrows():\n",
    "        ltable_id = row_['ltable_id']\n",
    "        rtable_id = row_['rtable_id']\n",
    "        RAG_dict = {}\n",
    "        RAG_dict['Entity_1'] = str(dict_ltable[ltable_id])\n",
    "        RAG_dict['Entity_2'] = str(dict_rtable[rtable_id])\n",
    "        RAG_dict['Output'] = 'dismatch' if row_['label']==0 else 'match'\n",
    "        RAG_text += '%s\\n\\n' % str(RAG_dict)\n",
    "    return text + RAG_text, '' , str(output_dict)\n",
    "walmart_amazon_train_fewshot_output = walmart_amazon_train.progress_apply(Prompt_Generalize_Baseline_NoMeta,axis=1,result_type='expand')\n",
    "\n",
    "# walmart_amazon_test.columns = ['ltable_id','rtable_id','label']\n",
    "walmart_amazon_test = test.progress_apply(Prompt_Generalize_Baseline_NoMeta,axis=1,result_type='expand')           \n",
    "# amazon_google_output = amazon_google_test.progress_apply(Prompt_Generalize,axis=1,result_type='expand')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = walmart_amazon_train_fewshot_output\n",
    "test_df = walmart_amazon_test\n",
    "train_df.columns = ['instruction','input','output']\n",
    "test_df.columns = ['instruction','input','output']\n",
    "json.dump(train_df.to_dict(orient='records'), open('data//MoE_no_meta/ER/walmart_amazon_train.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(test_df.to_dict(orient='records'), open('data//MoE_no_meta/ER/walmart_amazon_test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wdc-all baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdc_all_train_sample = pd.read_csv('LLM_ER/process_data/wdc-all/train_unique.csv',index_col=0)\n",
    "wdc_all_test = pd.read_csv('LLM_ER/process_data/wdc-all/wdc_all/dpo_test.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdc_train = pd.read_csv('LLM_ER/process_data/wdc-all/all/train.txt.small',sep='\\t',header=None)\n",
    "wdc_valid = pd.read_csv('LLM_ER/process_data/wdc-all/all/valid.txt.small',sep='\\t',header=None)\n",
    "wdc_test = pd.read_csv('LLM_ER/process_data/wdc-all/all/test.txt',sep='\\t',header=None)\n",
    "wdc_train.columns = ['l_entity','r_entity','label']\n",
    "wdc_valid.columns = ['l_entity','r_entity','label']\n",
    "wdc_test.columns = ['l_entity','r_entity','label']\n",
    "def ExtractEntity(row):\n",
    "    l_entity = row[0].replace('COL title VAL ','').strip()\n",
    "    r_entity = row[1].replace('COL title VAL ','').strip()\n",
    "    return l_entity,r_entity,row[2]\n",
    "wdc_train_out = wdc_train.apply(ExtractEntity,axis=1,result_type='expand')\n",
    "wdc_valid_out = wdc_valid.apply(ExtractEntity,axis=1,result_type='expand')\n",
    "wdc_test_out = wdc_test.apply(ExtractEntity,axis=1,result_type='expand')\n",
    "wdc_all_origin = pd.concat([wdc_train_out,wdc_valid_out,wdc_test_out]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdc_all_origin['index'] = wdc_all_origin.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ltable = dict(zip(wdc_all_origin.index,wdc_all_origin[0]))\n",
    "dict_rtable = dict(zip(wdc_all_origin.index,wdc_all_origin[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdc_all_train_fewshot = sample_dataframe_with_conditions(wdc_all_train_sample,pos_num=100,neg_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 200/200 [00:00<00:00, 5898.83it/s]\n",
      "pandas bar: 100%|██████████| 4398/4398 [00:00<00:00, 20851.47it/s]\n"
     ]
    }
   ],
   "source": [
    "import json,ast\n",
    "def Prompt_Generalize_Baseline(row): ## 在train筛选ltable_id and rtbale_id中所有类似的pos/neg放进去，但是不能完全相同，如果没有pos，那么从ground truth选4个，总数维持8个\n",
    "    temp_dict = {}\n",
    "    temp_dict['Output'] = ''\n",
    "    RAG_text = ''\n",
    "    ltable_id = row['ltable_id']\n",
    "    rtable_id = row['rtable_id']\n",
    "    text = 'You are an expert in detecting if two text descriptions of entities refer to the same or different entities. You are well-known to examine the fine details and can find subtle matches or mismatches to help you arrive at the final judgement. \\n\\nJudge whether Entity 1 and Entity 2 are match or dismatch, and choose within the given Options.\\n\\nReturn in JSON format.\\n\\nOptions: [match,dismatch]\\n\\nOutput format example:%s\\n\\nEntity 1:%s\\n\\nEntity 2:%s' % (json.dumps(temp_dict),json.dumps(dict_ltable[ltable_id]),json.dumps(dict_rtable[rtable_id]))\n",
    "    output_dict = {}\n",
    "    output_dict['Output'] = 'dismatch' if row['label']==0 else 'match'\n",
    "    return text + RAG_text, '' , str(output_dict)\n",
    "wdc_all_train_fewshot_output = wdc_all_train_fewshot.progress_apply(Prompt_Generalize_Baseline,axis=1,result_type='expand')\n",
    "wdc_all_test_output = wdc_all_test.progress_apply(Prompt_Generalize_Baseline,axis=1,result_type='expand')           \n",
    "# amazon_google_output = amazon_google_test.progress_apply(Prompt_Generalize,axis=1,result_type='expand')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 10403/10403 [00:05<00:00, 1817.38it/s]\n",
      "pandas bar: 100%|██████████| 4398/4398 [00:02<00:00, 1867.13it/s]\n"
     ]
    }
   ],
   "source": [
    "import json,ast\n",
    "df = wdc_all_train_sample\n",
    "def Prompt_Generalize_Baseline_NoMeta(row): ## 在train筛选ltable_id and rtbale_id中所有类似的pos/neg放进去，但是不能完全相同，如果没有pos，那么从ground truth选4个，总数维持8个\n",
    "    temp_dict = {}\n",
    "    temp_dict['Output'] = ''\n",
    "    RAG_text = ''\n",
    "    ltable_id = row['ltable_id']\n",
    "    rtable_id = row['rtable_id']\n",
    "    text = 'You are an expert in detecting if two text descriptions of entities refer to the same or different entities. You are well-known to examine the fine details and can find subtle matches or mismatches to help you arrive at the final judgement. \\n\\nJudge whether Entity 1 and Entity 2 are match or dismatch, and choose within the given Options.\\n\\nReturn in JSON format.\\n\\nOptions: [match,dismatch]\\n\\nOutput format example:%s\\n\\nEntity 1:%s\\n\\nEntity 2:%s' % (json.dumps(temp_dict),json.dumps(dict_ltable[ltable_id]),json.dumps(dict_rtable[rtable_id]))\n",
    "    output_dict = {}\n",
    "    output_dict['Output'] = 'dismatch' if row['label']==0 else 'match'\n",
    "    sample_df = df.sample(n=5)\n",
    "    RAG_text += '\\n\\nTake these examples as reference:\\n\\n'\n",
    "    for index,row_ in sample_df.iterrows():\n",
    "        ltable_id = row_['ltable_id']\n",
    "        rtable_id = row_['rtable_id']\n",
    "        RAG_dict = {}\n",
    "        RAG_dict['Entity_1'] = str(dict_ltable[ltable_id])\n",
    "        RAG_dict['Entity_2'] = str(dict_rtable[rtable_id])\n",
    "        RAG_dict['Output'] = 'dismatch' if row_['label']==0 else 'match'\n",
    "        RAG_text += '%s\\n\\n' % str(RAG_dict)\n",
    "    return text + RAG_text, '' , str(output_dict)\n",
    "wdc_all_train_fewshot_output = wdc_all_train_sample.progress_apply(Prompt_Generalize_Baseline_NoMeta,axis=1,result_type='expand')\n",
    "wdc_all_test_output = wdc_all_test.progress_apply(Prompt_Generalize_Baseline_NoMeta,axis=1,result_type='expand')           \n",
    "# amazon_google_output = amazon_google_test.progress_apply(Prompt_Generalize,axis=1,result_type='expand')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = wdc_all_train_fewshot_output\n",
    "test_df = wdc_all_test_output\n",
    "train_df.columns = ['instruction','input','output']\n",
    "test_df.columns = ['instruction','input','output']\n",
    "json.dump(train_df.to_dict(orient='records'), open('data/MoE_no_meta/ER/wdc_all_train.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(test_df.to_dict(orient='records'), open('data/MoE_no_meta/ER/wdc_all_test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ant-Buy Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "ant_buy_train = pd.read_csv('LLM_ER/process_data/ant-buy/train.csv',index_col=0)\n",
    "ant_buy_test = pd.read_csv('LLM_ER/process_data/ant-buy/exp_data/dpo_test.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableA = pd.read_csv('LLM_ER/process_data/ant-buy/exp_data/tableA.csv').fillna('')\n",
    "tableB = pd.read_csv('LLM_ER/process_data/ant-buy/exp_data/tableB.csv').fillna('')\n",
    "dict_ltable = tableA.set_index('id').to_dict(orient='records')\n",
    "dict_rtable = tableB.set_index('id').to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "ant_buy_train_fewshot = sample_dataframe_with_conditions(ant_buy_train,pos_num=100,neg_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,ast\n",
    "def Prompt_Generalize_Baseline(row): ## 在train筛选ltable_id and rtbale_id中所有类似的pos/neg放进去，但是不能完全相同，如果没有pos，那么从ground truth选4个，总数维持8个\n",
    "    temp_dict = {}\n",
    "    temp_dict['Output'] = ''\n",
    "    RAG_text = ''\n",
    "    ltable_id = row['ltable_id']\n",
    "    rtable_id = row['rtable_id']\n",
    "    text = 'You are an expert in detecting if two text descriptions of entities refer to the same or different entities. You are well-known to examine the fine details and can find subtle matches or mismatches to help you arrive at the final judgement. \\n\\nJudge whether Entity 1 and Entity 2 are match or dismatch, and choose within the given Options.\\n\\nReturn in JSON format.\\n\\nOptions: [match,dismatch]\\n\\nOutput format example:%s\\n\\nEntity 1:%s\\n\\nEntity 2:%s' % (json.dumps(temp_dict),json.dumps(dict_ltable[ltable_id]),json.dumps(dict_rtable[rtable_id]))\n",
    "    output_dict = {}\n",
    "    output_dict['Output'] = 'dismatch' if row['label']==0 else 'match'\n",
    "    sample_df = df.sample(n=5)\n",
    "    RAG_text += '\\n\\nTake these examples as reference:\\n\\n'\n",
    "    for index,row_ in sample_df.iterrows():\n",
    "        ltable_id = row_['ltable_id']\n",
    "        rtable_id = row_['rtable_id']\n",
    "        RAG_dict = {}\n",
    "        RAG_dict['Entity_1'] = str(dict_ltable[ltable_id])\n",
    "        RAG_dict['Entity_2'] = str(dict_rtable[rtable_id])\n",
    "        RAG_dict['Output'] = 'dismatch' if row_['label']==0 else 'match'\n",
    "        RAG_text += '%s\\n\\n' % str(RAG_dict)\n",
    "    return text + RAG_text, '' , str(output_dict)\n",
    "ant_buy_train_fewshot_output = ant_buy_train_fewshot.progress_apply(Prompt_Generalize_Baseline,axis=1,result_type='expand')\n",
    "ant_buy_test_output = ant_buy_test.progress_apply(Prompt_Generalize_Baseline,axis=1,result_type='expand')           \n",
    "# amazon_google_output = amazon_google_test.progress_apply(Prompt_Generalize,axis=1,result_type='expand')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 7405/7405 [00:03<00:00, 1869.97it/s]\n",
      "pandas bar: 100%|██████████| 1916/1916 [00:01<00:00, 1890.88it/s]\n"
     ]
    }
   ],
   "source": [
    "import json,ast\n",
    "df = ant_buy_train\n",
    "def Prompt_Generalize_Baseline_NoMeta(row): ## 在train筛选ltable_id and rtbale_id中所有类似的pos/neg放进去，但是不能完全相同，如果没有pos，那么从ground truth选4个，总数维持8个\n",
    "    temp_dict = {}\n",
    "    temp_dict['Output'] = ''\n",
    "    RAG_text = ''\n",
    "    ltable_id = row['ltable_id']\n",
    "    rtable_id = row['rtable_id']\n",
    "    text = 'You are an expert in detecting if two text descriptions of entities refer to the same or different entities. You are well-known to examine the fine details and can find subtle matches or mismatches to help you arrive at the final judgement. \\n\\nJudge whether Entity 1 and Entity 2 are match or dismatch, and choose within the given Options.\\n\\nReturn in JSON format.\\n\\nOptions: [match,dismatch]\\n\\nOutput format example:%s\\n\\nEntity 1:%s\\n\\nEntity 2:%s' % (json.dumps(temp_dict),json.dumps(dict_ltable[ltable_id]),json.dumps(dict_rtable[rtable_id]))\n",
    "    output_dict = {}\n",
    "    output_dict['Output'] = 'dismatch' if row['label']==0 else 'match'\n",
    "    sample_df = df.sample(n=5)\n",
    "    RAG_text += '\\n\\nTake these examples as reference:\\n\\n'\n",
    "    for index,row_ in sample_df.iterrows():\n",
    "        ltable_id = row_['ltable_id']\n",
    "        rtable_id = row_['rtable_id']\n",
    "        RAG_dict = {}\n",
    "        RAG_dict['Entity_1'] = str(dict_ltable[ltable_id])\n",
    "        RAG_dict['Entity_2'] = str(dict_rtable[rtable_id])\n",
    "        RAG_dict['Output'] = 'dismatch' if row_['label']==0 else 'match'\n",
    "        RAG_text += '%s\\n\\n' % str(RAG_dict)\n",
    "    return text + RAG_text, '' , str(output_dict)\n",
    "ant_buy_train_fewshot_output = ant_buy_train.progress_apply(Prompt_Generalize_Baseline_NoMeta,axis=1,result_type='expand')\n",
    "ant_buy_test_output = ant_buy_test.progress_apply(Prompt_Generalize_Baseline_NoMeta,axis=1,result_type='expand')           \n",
    "# amazon_google_output = amazon_google_test.progress_apply(Prompt_Generalize,axis=1,result_type='expand')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = ant_buy_train_fewshot_output\n",
    "test_df = ant_buy_test_output\n",
    "train_df.columns = ['instruction','input','output']\n",
    "test_df.columns = ['instruction','input','output']\n",
    "json.dump(train_df.to_dict(orient='records'), open('data/MoE_no_meta/ER/ant_buy_train.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(test_df.to_dict(orient='records'), open('data/MoE_no_meta/ER/ant_buy_test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_csv_files_and_parents(folder_path):\n",
    "    \"\"\"\n",
    "    List all CSV files in the specified folder and their parent folder names.\n",
    "\n",
    "    Parameters:\n",
    "    folder_path (str): The path to the directory.\n",
    "\n",
    "    Returns:\n",
    "    list: A list where each element is a list containing the parent folder name and CSV file path.\n",
    "    \"\"\"\n",
    "    csv_files_info = []\n",
    "\n",
    "    # 遍历指定路径内的所有文件和文件夹\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\") and file.__contains__('train'):\n",
    "                # 获取CSV文件的路径\n",
    "                csv_file_path = os.path.join(root, file)\n",
    "                # 获取CSV文件所在一级父文件夹名字\n",
    "                parent_folder_name = os.path.basename(root)\n",
    "                # 将父文件夹名字和CSV文件路径添加到列表中\n",
    "                csv_files_info.append('/'.join(csv_file_path.split('/')[-3:]))\n",
    "\n",
    "    return csv_files_info\n",
    "MoE_noMeta = list_csv_files_and_parents('data/MoE_no_meta/ER/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SHA1_cal(path):\n",
    "    hexdigest = encrypt(path, 'sha1')\n",
    "    return hexdigest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_google_train\n",
      "semi_text_w_train\n",
      "semi_text_c_train\n",
      "walmart_amazon_train\n",
      "wdc_all_train\n",
      "ant_buy_train\n"
     ]
    }
   ],
   "source": [
    "dataset_info = {}\n",
    "def SHA1_cal(path):\n",
    "    hexdigest = encrypt(path, 'sha1')\n",
    "    return hexdigest\n",
    "# SHA1_cal('data/MoE/ER/ant_buy_train_output.json')\n",
    "for dataset in MoE_noMeta:\n",
    "# for i in [6,7,8]:\n",
    "    name = dataset.split('/')[-1].split('.')[0]\n",
    "    print(name)\n",
    "    # path_save = 'data/MoE/add/%s.json' % name\n",
    "    name += '-MoE-noMeta'\n",
    "    dataset_info[name] = {}\n",
    "    dataset_info[name][\"file_name\"] = dataset\n",
    "    dataset_info[name][\"file_sha1\"] = SHA1_cal('data/' + dataset)\n",
    "    dataset_info[name][\"columns\"] = {\n",
    "        \"prompt\": \"instruction\",\n",
    "        \"query\": \"input\",\n",
    "        \"response\": \"output\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"amazon_google_train-MoE-noMeta\": {\"file_name\": \"MoE_no_meta/ER/amazon_google_train.json\", \"file_sha1\": \"c94be84eaf41c96cffab1a1e399fc088b78d6c57\", \"columns\": {\"prompt\": \"instruction\", \"query\": \"input\", \"response\": \"output\"}}, \"semi_text_w_train-MoE-noMeta\": {\"file_name\": \"MoE_no_meta/ER/semi_text_w_train.json\", \"file_sha1\": \"6a954eec583781894e80e37056388e395f3140ec\", \"columns\": {\"prompt\": \"instruction\", \"query\": \"input\", \"response\": \"output\"}}, \"semi_text_c_train-MoE-noMeta\": {\"file_name\": \"MoE_no_meta/ER/semi_text_c_train.json\", \"file_sha1\": \"2d9641a34c8b02eab03a83568a5cc50224030a94\", \"columns\": {\"prompt\": \"instruction\", \"query\": \"input\", \"response\": \"output\"}}, \"walmart_amazon_train-MoE-noMeta\": {\"file_name\": \"MoE_no_meta/ER/walmart_amazon_train.json\", \"file_sha1\": \"c1ee9a10cc9a3b53b879d5bab35220a6431674f8\", \"columns\": {\"prompt\": \"instruction\", \"query\": \"input\", \"response\": \"output\"}}, \"wdc_all_train-MoE-noMeta\": {\"file_name\": \"MoE_no_meta/ER/wdc_all_train.json\", \"file_sha1\": \"ea6575ae823b5b799c789eca4dfcb7191ebc56ba\", \"columns\": {\"prompt\": \"instruction\", \"query\": \"input\", \"response\": \"output\"}}, \"ant_buy_train-MoE-noMeta\": {\"file_name\": \"MoE_no_meta/ER/ant_buy_train.json\", \"file_sha1\": \"87b9793072e5a6b1dbcd49603913beab8fc2f0cc\", \"columns\": {\"prompt\": \"instruction\", \"query\": \"input\", \"response\": \"output\"}}'"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(dataset_info).replace(\"'\",'\"')[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(merge_df_output)):\n",
    "# for i in [6,7,8]:\n",
    "    df = merge_df_output[i]\n",
    "    name = find_var_name(merge_df_input[i])\n",
    "    print(name)\n",
    "    path_save = 'data/MoE/add/%s.json' % name\n",
    "    json.dump(df.to_dict(orient='records'), open(path_save, 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "    name += '-MoE-Add'\n",
    "    dataset_info[name] = {}\n",
    "    dataset_info[name][\"file_name\"] = path_save.replace('data/','')\n",
    "    dataset_info[name][\"file_sha1\"] = SHA1_cal(path_save)\n",
    "    dataset_info[name][\"columns\"] = {\n",
    "        \"prompt\": \"instruction\",\n",
    "        \"query\": \"input\",\n",
    "        \"response\": \"output\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WANDB_MODE=disabled accelerate launch src/train_bash.py     --stage sft     --model_name_or_path Mistral-7B-Instruct-v0.2    --do_train     --finetuning_type lora     --dataset semi_text_w_train-MoE-noMeta     --output_dir lora_weight/MoE-no-Meta/semi_text_w_train-MoE-noMeta --overwrite_output_dir     --lr_scheduler_type cosine     --num_train_epochs 5.0     --gradient_accumulation_steps 8     --per_device_eval_batch_size 8     --fp16     --template mistral     --lora_r 16 --logging_steps 5 --plot_loss  --lora_target all --save_steps 50 --use_unsloth --cutoff_len 2048 --quantization_bit 8',\n",
       " 'WANDB_MODE=disabled accelerate launch src/train_bash.py     --stage sft     --model_name_or_path Mistral-7B-Instruct-v0.2    --do_train     --finetuning_type lora     --dataset semi_text_c_train-MoE-noMeta     --output_dir lora_weight/MoE-no-Meta/semi_text_c_train-MoE-noMeta --overwrite_output_dir     --lr_scheduler_type cosine     --num_train_epochs 5.0     --gradient_accumulation_steps 8     --per_device_eval_batch_size 8     --fp16     --template mistral     --lora_r 16 --logging_steps 5 --plot_loss  --lora_target all --save_steps 50 --use_unsloth --cutoff_len 2048 --quantization_bit 8',\n",
       " 'WANDB_MODE=disabled accelerate launch src/train_bash.py     --stage sft     --model_name_or_path Mistral-7B-Instruct-v0.2    --do_train     --finetuning_type lora     --dataset walmart_amazon_train-MoE-noMeta     --output_dir lora_weight/MoE-no-Meta/walmart_amazon_train-MoE-noMeta --overwrite_output_dir     --lr_scheduler_type cosine     --num_train_epochs 5.0     --gradient_accumulation_steps 8     --per_device_eval_batch_size 8     --fp16     --template mistral     --lora_r 16 --logging_steps 5 --plot_loss  --lora_target all --save_steps 50 --use_unsloth --cutoff_len 2048 --quantization_bit 8',\n",
       " 'WANDB_MODE=disabled accelerate launch src/train_bash.py     --stage sft     --model_name_or_path Mistral-7B-Instruct-v0.2    --do_train     --finetuning_type lora     --dataset wdc_all_train-MoE-noMeta     --output_dir lora_weight/MoE-no-Meta/wdc_all_train-MoE-noMeta --overwrite_output_dir     --lr_scheduler_type cosine     --num_train_epochs 5.0     --gradient_accumulation_steps 8     --per_device_eval_batch_size 8     --fp16     --template mistral     --lora_r 16 --logging_steps 5 --plot_loss  --lora_target all --save_steps 50 --use_unsloth --cutoff_len 2048 --quantization_bit 8',\n",
       " 'WANDB_MODE=disabled accelerate launch src/train_bash.py     --stage sft     --model_name_or_path Mistral-7B-Instruct-v0.2    --do_train     --finetuning_type lora     --dataset ant_buy_train-MoE-noMeta     --output_dir lora_weight/MoE-no-Meta/ant_buy_train-MoE-noMeta --overwrite_output_dir     --lr_scheduler_type cosine     --num_train_epochs 5.0     --gradient_accumulation_steps 8     --per_device_eval_batch_size 8     --fp16     --template mistral     --lora_r 16 --logging_steps 5 --plot_loss  --lora_target all --save_steps 50 --use_unsloth --cutoff_len 2048 --quantization_bit 8']"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"WANDB_MODE=disabled accelerate launch src/train_bash.py     --stage sft     --model_name_or_path Mistral-7B-Instruct-v0.2    --do_train     --finetuning_type lora     --dataset %s     --output_dir lora_weight/MoE-no-Meta/%s --overwrite_output_dir     --lr_scheduler_type cosine     --num_train_epochs 5.0     --gradient_accumulation_steps 8     --per_device_eval_batch_size 8     --fp16     --template mistral     --lora_r 16 --logging_steps 5 --plot_loss  --lora_target all --save_steps 50 --use_unsloth --cutoff_len 2048 --quantization_bit 8\" % (t,t) for t in dataset_info.keys()][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Baseline Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_train_baseline = pd.read_json('MoE-Example/DC/hospital-train-ablation.json').iloc[:200]\n",
    "beer_train_baseline = pd.read_json('MoE-Example/DC/beer-train-ablation.json').iloc[:200]\n",
    "rayyan_baseline = pd.read_json('MoE-Example/DC/rayyan-train-ablation.json').iloc[:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(hospital_train_baseline.to_dict(orient='records'), open('data/MoE_Baseline/DC/hospital_train_baseline.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(beer_train_baseline.to_dict(orient='records'), open('data/MoE_Baseline/DC/beer_train_baseline.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(rayyan_baseline.to_dict(orient='records'), open('data/MoE_Baseline/DC/rayyan_train_baseline.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Matching Baseline Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMS_test = pd.read_json('data/MoE/SM/CMS_test_output.json')\n",
    "mimic_test = pd.read_json('data/MoE/SM/mimic_test_output.json')\n",
    "synthea_test = pd.read_json('data/MoE/SM/synthea_test_output.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 660/660 [00:00<00:00, 33126.00it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CMS_train_baseline = pd.concat([CMS_train[CMS_train['output']==\"{'Output': 'match'}\"],CMS_train[CMS_train['output']==\"{'Output': 'dismatch'}\"].sample(n=500)])\n",
    "CMS_train_baseline['instruction'] = CMS_train_baseline.progress_apply(Remove_RAG,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 599/599 [00:00<00:00, 23150.74it/s]\n"
     ]
    }
   ],
   "source": [
    "mimic_train_baseline = pd.concat([mimic_train[mimic_train['output']==\"{'Output': 'match'}\"],mimic_train[mimic_train['output']==\"{'Output': 'dismatch'}\"].sample(n=500)])\n",
    "mimic_train_baseline['instruction'] = mimic_train_baseline.progress_apply(Remove_RAG,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 584/584 [00:00<00:00, 26047.70it/s]\n"
     ]
    }
   ],
   "source": [
    "synthea_train_baseline = pd.concat([synthea_train[synthea_train['output']==\"{'Output': 'match'}\"],synthea_train[synthea_train['output']==\"{'Output': 'dismatch'}\"].sample(n=500)])\n",
    "synthea_train_baseline['instruction'] = synthea_train_baseline.progress_apply(Remove_RAG,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 5127/5127 [00:00<00:00, 43349.47it/s]\n",
      "pandas bar: 100%|██████████| 12816/12816 [00:00<00:00, 95613.17it/s]\n",
      "pandas bar: 100%|██████████| 5928/5928 [00:00<00:00, 117964.99it/s]\n"
     ]
    }
   ],
   "source": [
    "CMS_test['instruction'] = CMS_test.progress_apply(Remove_RAG,axis=1)\n",
    "mimic_test['instruction'] = mimic_test.progress_apply(Remove_RAG,axis=1)\n",
    "synthea_test['instruction'] = synthea_test.progress_apply(Remove_RAG,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(CMS_train_baseline.to_dict(orient='records'), open('data/MoE_Baseline/SM/CMS_train_baseline.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(synthea_train_baseline.to_dict(orient='records'), open('data/MoE_Baseline/SM/synthea_train_baseline.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(mimic_train_baseline.to_dict(orient='records'), open('data/MoE_Baseline/SM/mimic_train_baseline.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(CMS_test.to_dict(orient='records'), open('data/MoE_Baseline/SM/CMS_test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(mimic_test.to_dict(orient='records'), open('data/MoE_Baseline/SM/mimic_test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(synthea_test.to_dict(orient='records'), open('data/MoE_Baseline/SM/synthea_test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_train_fewshot = walmart_train.iloc[:241]\n",
    "amazon_train_fewshot = amazon_train.iloc[:2001]\n",
    "restaurant_train_fewshot = restaurant_train.iloc[:86]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 241/241 [00:00<00:00, 21396.34it/s]\n",
      "/tmp/ipykernel_1942508/3893300183.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  walmart_train_fewshot['instruction'] = walmart_train_fewshot.progress_apply(Data_Imputation_Remove_RAG_Option,axis=1)\n",
      "pandas bar: 100%|██████████| 2001/2001 [00:00<00:00, 31933.89it/s]\n",
      "/tmp/ipykernel_1942508/3893300183.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  amazon_train_fewshot['instruction'] = amazon_train_fewshot.progress_apply(Data_Imputation_Remove_RAG_Option,axis=1)\n",
      "pandas bar: 100%|██████████| 86/86 [00:00<00:00, 36840.99it/s]\n",
      "/tmp/ipykernel_1942508/3893300183.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  restaurant_train_fewshot['instruction'] = restaurant_train_fewshot.progress_apply(Data_Imputation_Remove_RAG_Option,axis=1)\n"
     ]
    }
   ],
   "source": [
    "def Data_Imputation_Remove_RAG_Option(row):\n",
    "    instruction = row['instruction']\n",
    "    instruction_1st = instruction.split('\\n\\nOptions')[0]\n",
    "    instruction_2nd = '\\n\\n'+instruction.split(']\\n\\n')[1].split('\\n\\nTake these examples as reference')[0]\n",
    "    return instruction_1st + instruction_2nd\n",
    "# print(Data_Imputation_Remove_RAG_Option(walmart_train.iloc[0]))\n",
    "walmart_train_fewshot['instruction'] = walmart_train_fewshot.progress_apply(Data_Imputation_Remove_RAG_Option,axis=1)\n",
    "amazon_train_fewshot['instruction'] = amazon_train_fewshot.progress_apply(Data_Imputation_Remove_RAG_Option,axis=1)\n",
    "restaurant_train_fewshot['instruction'] = restaurant_train_fewshot.progress_apply(Data_Imputation_Remove_RAG_Option,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_test = pd.read_json('data/MoE/DI/walmart_test_output_wide.json')\n",
    "amazon_test = pd.read_json('data/MoE/DI/amazon_test_output_wide.json')\n",
    "restaurant_test = pd.read_json('data/MoE/DI/restaurant_test_output_wide.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 104/104 [00:00<00:00, 17257.78it/s]\n",
      "pandas bar: 100%|██████████| 816/816 [00:00<00:00, 25324.85it/s]\n",
      "pandas bar: 100%|██████████| 29/29 [00:00<00:00, 11917.97it/s]\n"
     ]
    }
   ],
   "source": [
    "walmart_test['instruction'] = walmart_test.progress_apply(Data_Imputation_Remove_RAG_Option,axis=1)\n",
    "amazon_test['instruction'] = amazon_test.progress_apply(Data_Imputation_Remove_RAG_Option,axis=1)\n",
    "restaurant_test['instruction'] = restaurant_test.progress_apply(Data_Imputation_Remove_RAG_Option,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(walmart_train_fewshot.to_dict(orient='records'), open('data/MoE_Baseline/DI/walmart_train_fewshot.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(amazon_train_fewshot.to_dict(orient='records'), open('data/MoE_Baseline/DI/amazon_train_fewshot.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(restaurant_train_fewshot.to_dict(orient='records'), open('data/MoE_Baseline/DI/restaurant_train_fewshot.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(walmart_test.to_dict(orient='records'), open('data/MoE_Baseline/DI/walmart_test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(amazon_test.to_dict(orient='records'), open('data/MoE_Baseline/DI/amazon_test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(restaurant_test.to_dict(orient='records'), open('data/MoE_Baseline/DI/restaurant_test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start AVE Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa_mine = pd.read_json('data/oa_mine/oa_mine_train_small.json')\n",
    "oa_mine_test = pd.read_json('data/oa_mine/oa_mine_test_small.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa_mine_fewshot = oa_mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AVE_Remove_RAG_Option(row):\n",
    "    instruction = row['instruction']\n",
    "    # instruction_1st = instruction.split('\\n\\nOptions')[0]\n",
    "    instruction_2nd = instruction.split('\\n\\nTake these')[0]\n",
    "    return instruction_2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 1443/1443 [00:00<00:00, 35523.04it/s]\n"
     ]
    }
   ],
   "source": [
    "oa_mine_fewshot['instruction'] = oa_mine_fewshot.progress_apply(AVE_Remove_RAG_Option,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 2451/2451 [00:00<00:00, 36020.59it/s]\n"
     ]
    }
   ],
   "source": [
    "oa_mine_test['instruction'] = oa_mine_test.progress_apply(AVE_Remove_RAG_Option,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dump(oa_mine_fewshot.to_dict(orient='records'), open('data/MoE_Baseline/AVE/oa_mine_fewshot_train.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(oa_mine_test.to_dict(orient='records'), open('data/MoE_Baseline/AVE/oa_mine_test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start CTA Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "SimTab_train = pd.read_json('data/CTA/SimTab_Train_few.json')\n",
    "SimTab_relation = np.load('data/CTA/sim_all_relation.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "SimTab_test = pd.read_json('data/CTA/SimTab_test_few.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 6369/6369 [00:02<00:00, 2427.08it/s]\n"
     ]
    }
   ],
   "source": [
    "def SimTab_remove_RAG(row):\n",
    "    instruction = row['instruction']\n",
    "    ins_1 = instruction.split('Options:')[0] + 'Options:%s' % str(SimTab_relation)\n",
    "    ins_2 = '\\n\\n'+instruction.split(']\\n\\n')[1].split('\\n\\nReference tables:')[0] \n",
    "    return ins_1 + ins_2\n",
    "SimTab_train['instruction'] = SimTab_train.progress_apply(SimTab_remove_RAG,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 7610/7610 [00:03<00:00, 2416.73it/s]\n"
     ]
    }
   ],
   "source": [
    "SimTab_test['instruction'] = SimTab_test.progress_apply(SimTab_remove_RAG,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtable_train = pd.read_json('data/CTA/WebTable_Train_few.json')\n",
    "webtable_relation = np.load('data/CTA/webtable_all_relation.npy',allow_pickle=True)\n",
    "webtable_test = pd.read_json('data/CTA/WebTable_Test_few.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in relation extraction from wikipedia web table to knowledge graph. Given table title and column pair for Table 1, please choose the most proper type from the provided options. Return in json format.\n",
      "\n",
      "Column: col_1\n",
      "\n",
      "Options:[\"city\", \"birthplace\", \"location\"]\n",
      "\n",
      "Output Format Example:\n",
      "\n",
      "{\"type\": \"\"}\n",
      "\n",
      "Table 1:\n",
      "\n",
      "{'col_1': 'Reno, NV'}\n",
      "{'col_1': 'Logan, UT'}\n",
      "\n",
      "Reference tables:\n",
      "\n",
      "{'Table': '{\\'description\\': \\'\"Schools Out\" Camp\\', \\'col_1\\': \\'Activity & Rec Ctr\\', \\'status\\': \\'Unavailable\\', \\'gender\\': \\'Co-ed\\'}', 'Column': 'col_1', 'type': 'location'}\n",
      "\n",
      "{'Table': \"{'col_1': 'Philadelphia', 'state': 'PA'}\\n{'col_1': 'Fayetteville', 'state': 'AR'}\\n{'col_1': 'New York', 'state': 'NY'}\\n{'col_1': 'Rochester', 'state': 'NY'}\", 'Column': 'col_1', 'type': 'city'}\n",
      "\n",
      "{'Table': \"{'name': 'Jacob Markstrom', 'weight': '196', 'age': '25', 'col_1': 'Gävle, SWE'}\\n{'name': 'Ryan Miller', 'weight': '168', 'age': '35', 'col_1': 'East Lansing, MI, USA'}\", 'Column': 'col_1', 'type': 'birthplace'}\n",
      "\n",
      "{'Table': \"{'col_1': 'Pittsburgh', 'state': 'PA'}\\n{'col_1': 'Flagstaff', 'state': 'AZ'}\\n{'col_1': 'Portland', 'state': 'OR'}\\n{'col_1': 'Omaha', 'state': 'NE'}\", 'Column': 'col_1', 'type': 'city'}\n",
      "\n",
      "{'Table': '{\\'name\\': \\'Chicago Lower Extremity Surgical Foundation Symposium\\', \\'col_1\\': \\'Chicago, IL\\'}\\n{\\'name\\': \"North American Spine Society\\'s (NASS) Annual Meeting\", \\'col_1\\': \\'New Orleans, LA\\'}\\n{\\'name\\': \\'2013 Desert Foot Conference\\', \\'col_1\\': \\'Phoenix, AZ\\'}\\n{\\'name\\': \\'American Society for Surgery of the Hand 68th Annual Meeting (ASSH)\\', \\'col_1\\': \\'San Francisco, CA\\'}', 'Column': 'col_1', 'type': 'location'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(webtable_test.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 17709/17709 [00:02<00:00, 7548.56it/s]\n"
     ]
    }
   ],
   "source": [
    "def webtable_remove_RAG(row):\n",
    "    instruction = row['instruction']\n",
    "    ins_1 = instruction.split('Options:')[0] + 'Options:%s' % str(webtable_relation)\n",
    "    ins_2 = '\\n\\n'+instruction.split(']\\n\\n')[1].split('\\n\\nReference tables:')[0] \n",
    "    return ins_1 + ins_2\n",
    "webtable_test['instruction'] = webtable_test.progress_apply(webtable_remove_RAG,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 17772/17772 [00:02<00:00, 7545.94it/s]\n",
      "pandas bar: 100%|██████████| 17709/17709 [00:02<00:00, 7859.19it/s]\n"
     ]
    }
   ],
   "source": [
    "webtable_train['instruction'] = webtable_train.progress_apply(webtable_remove_RAG,axis=1)\n",
    "webtable_test['instruction'] = webtable_test.progress_apply(webtable_remove_RAG,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 17772/17772 [00:02<00:00, 7550.10it/s]\n"
     ]
    }
   ],
   "source": [
    "webtable_train['instruction'] = webtable_train.progress_apply(webtable_remove_RAG,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(SimTab_train.to_dict(orient='records'), open('data/MoE_Baseline/CTA/SimTab_train.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(SimTab_test.to_dict(orient='records'), open('data/MoE_Baseline/CTA/SimTab_test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(webtable_train.to_dict(orient='records'), open('data/MoE_Baseline/CTA/webtable_train.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "# json.dump(webtable_test.to_dict(orient='records'), open('data/MoE_Baseline/CTA/webtable_test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_train = pd.read_json('data/RE/RE-train-no-ret.json')\n",
    "re_test = pd.read_json('data/RE/RE-test-no-ret.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "md5: df42054726d765b7dbb449a566f74774\n",
      "sha1: 471684456fc9e02903d71151ace37d45dc35be93\n",
      "sha256: 4dc3daa77ff784e29c963c33a18a2d49e3d49584042eb255cc075a02af873b31\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def encrypt(fpath: str, algorithm: str) -> str:\n",
    "    with open(fpath, 'rb') as f:\n",
    "        return hashlib.new(algorithm, f.read()).hexdigest()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for algorithm in ('md5', 'sha1', 'sha256'):\n",
    "        hexdigest = encrypt('data/MoE_Baseline/ED/error_detection_train.json', algorithm)\n",
    "        print(f'{algorithm}: {hexdigest}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad51518a7d7a4038b6f2e0c183bd148c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Lengths: 714\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "prompt_opt = pd.read_json('data/hospital/hospital-test.json') ## replace with your training file\n",
    "# prompt_opt = SimTab_train\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base') ## replace with any LM\n",
    "\n",
    "\n",
    "token_lengths = [len(tokenizer.encode(text, add_special_tokens=True)) for text in tqdm(prompt_opt['instruction'].to_list())]\n",
    "\n",
    "print(\"Token Lengths:\", max(token_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325112"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(token_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def concatenate_json_files_recursive(folder_path):\n",
    "    \"\"\"\n",
    "    Concatenate all JSON files in the given folder and its subfolders that have 'train' in their filenames into a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    folder_path (str): The path to the root folder.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A concatenated DataFrame.\n",
    "    \"\"\"\n",
    "    # 初始化空DataFrame，用于存储拼接的数据\n",
    "    concatenated_df = pd.DataFrame()\n",
    "\n",
    "    # 使用os.walk遍历所有子文件夹\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        # 获取当前文件夹下所有文件名含有'train'的json文件\n",
    "        json_files = [file for file in files if 'train' in file and file.endswith('.json')]\n",
    "        \n",
    "        # 遍历当前文件夹下的json文件\n",
    "        for json_file in json_files:\n",
    "            file_path = os.path.join(root, json_file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                # 读取json文件并转换为DataFrame\n",
    "                json_data = pd.read_json(file)\n",
    "                # 拼接到总的DataFrame\n",
    "                concatenated_df = pd.concat([concatenated_df, json_data])\n",
    "\n",
    "    # 重置索引以确保索引是唯一的\n",
    "    concatenated_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return concatenated_df\n",
    "concate_train = concatenate_json_files_recursive('data/MoE_Baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(concate_train.to_dict(orient='records'), open('data/MoE_Baseline/concate_train.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_json('data/MoE_Baseline/CTA/SimTab_train.json')\n",
    "df_2 = pd.read_json('data/MoE_Baseline/CTA/webtable_train.json')\n",
    "df_3 = pd.read_json('data/MoE_Baseline/RE/RE-train.json')\n",
    "Table_Baseline = pd.concat([df_1,df_2,df_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(Table_Baseline.to_dict(orient='records'), open('data/MoE_Baseline/concate_train_table.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26281"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Table_Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merging for CTA-SimTab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "SimTab_train_init = pd.read_json('data/CTA/SimTab_Train_few.json')\n",
    "WebTable_train_init = pd.read_json('data/CTA/WebTable_Train_few.json')\n",
    "Amazon_google_init = pd.read_json('data/MoE/ER/amazon-google-train.json')\n",
    "Semi_text_w_init = pd.read_json('data/MoE/ER/semi-text-w-train-MoE.json')\n",
    "Walmart_init = pd.read_json('data/MoE/DI/walmart_train_output_wide.json')\n",
    "restaurant_init = pd.read_json('data/MoE/DI/restaurant_train_output_wide.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def merge_dataframes(dataframes):\n",
    "    merged_dataframes = []\n",
    "\n",
    "    for i, df in enumerate(dataframes):\n",
    "        # 创建一个空的DataFrame用于累积合并的结果\n",
    "        cumulative_merge = pd.DataFrame()\n",
    "\n",
    "        for j, other_df in enumerate(dataframes):\n",
    "            if i != j:\n",
    "                # 根据output列中的元素种类执行不同的合并策略\n",
    "                if other_df['output'].nunique() == 2:\n",
    "                    # 执行原始策略\n",
    "                    dismatch_rows = other_df[other_df['output'].str.contains('dismatch', na=False)]\n",
    "                    non_dismatch_rows = other_df[~other_df['output'].str.contains('dismatch', na=False)]\n",
    "\n",
    "                    selected_dismatch = dismatch_rows.sample(n=min(100, len(dismatch_rows)), replace=True)\n",
    "                    selected_non_dismatch = non_dismatch_rows.sample(n=min(300, len(non_dismatch_rows)), replace=True)\n",
    "\n",
    "                    merge_result = pd.concat([selected_dismatch, selected_non_dismatch])\n",
    "                else:\n",
    "                    # 随机选择150行数据\n",
    "                    selected_rows = other_df.sample(n=min(300, len(other_df)), replace=True)\n",
    "                    merge_result = selected_rows\n",
    "\n",
    "                # 累积合并结果\n",
    "                cumulative_merge = pd.concat([cumulative_merge, merge_result])\n",
    "\n",
    "        # 将累积的合并结果与原始DataFrame合并\n",
    "        # final_merged_df = pd.concat([df, cumulative_merge])\n",
    "        # 不将累积的合并结果与原始DataFrame合并\n",
    "        final_merged_df = pd.concat([cumulative_merge])\n",
    "        # 将最终合并的DataFrame添加到结果列表中\n",
    "        merged_dataframes.append(final_merged_df)\n",
    "\n",
    "    return merged_dataframes\n",
    "\n",
    "merge_df_output = merge_dataframes([SimTab_train_init.iloc[:,:3],WebTable_train_init.iloc[:,:3],Amazon_google_init,Semi_text_w_init,Walmart_init,restaurant_init])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SHA1_cal(path):\n",
    "    hexdigest = encrypt(path, 'sha1')\n",
    "    return hexdigest\n",
    "def find_var_name(obj):\n",
    "    for var_name in globals():\n",
    "        if globals()[var_name] is obj:\n",
    "            return var_name\n",
    "    return None\n",
    "# SHA1_cal('data/MoE/ER/ant_buy_train_output.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SimTab_train_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df_input = [SimTab_train_init,WebTable_train_init,Amazon_google_init,Semi_text_w_init,Walmart_init,restaurant_init]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimTab_train_init\n",
      "WebTable_train_init\n",
      "Amazon_google_init\n",
      "Semi_text_w_init\n",
      "Walmart_init\n",
      "restaurant_init\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "dataset_info = {}\n",
    "for i in range(len(merge_df_output)):\n",
    "# for i in [6,7,8]:\n",
    "    df = merge_df_output[i]\n",
    "    name = find_var_name(merge_df_input[i])\n",
    "    print(name)\n",
    "    path_save = 'data/MoE/add_CT/%s.json' % name\n",
    "    json.dump(df.to_dict(orient='records'), open(path_save, 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "    name += '-MoE-Add'\n",
    "    dataset_info[name] = {}\n",
    "    dataset_info[name][\"file_name\"] = path_save.replace('data/','')\n",
    "    dataset_info[name][\"file_sha1\"] = SHA1_cal(path_save)\n",
    "    dataset_info[name][\"columns\"] = {\n",
    "        \"prompt\": \"instruction\",\n",
    "        \"query\": \"input\",\n",
    "        \"response\": \"output\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"SimTab_train_init-MoE-Add\": {\"file_name\": \"MoE/add_CT/SimTab_train_init.json\", \"file_sha1\": \"5a2de2592a9a7eb4fe0c7cdf7dfb62b061a519f3\", \"columns\": {\"prompt\": \"instruction\", \"query\": \"input\", \"response\": \"output\"}}, \"WebTable_train_init-MoE-Add\": {\"file_name\": \"MoE/add_CT/WebTable_train_init.json\", \"file_sha1\": \"47c71d2c00c88692661403a97aa6f7d99d5cab91\", \"columns\": {\"prompt\": \"instruction\", \"query\": \"input\", \"response\": \"output\"}}, \"Amazon_google_init-MoE-Add\": {\"file_name\": \"MoE/add_CT/Amazon_google_init.json\", \"file_sha1\": \"2f6cb27493379fffb05c533af54d6f0faa24129e\", \"columns\": {\"prompt\": \"instruction\", \"query\": \"input\", \"response\": \"output\"}}, \"Semi_text_w_init-MoE-Add\": {\"file_name\": \"MoE/add_CT/Semi_text_w_init.json\", \"file_sha1\": \"d1e4cfcc4a3603a4968eb5ac22ae7cd77f327131\", \"columns\": {\"prompt\": \"instruction\", \"query\": \"input\", \"response\": \"output\"}}, \"Walmart_init-MoE-Add\": {\"file_name\": \"MoE/add_CT/Walmart_init.json\", \"file_sha1\": \"47ea93bf5d4f04f9d0d6c18290225991913e3577\", \"columns\": {\"prompt\": \"instruction\", \"query\": \"input\", \"response\": \"output\"}}, \"restaurant_init-MoE-Add\": {\"file_name\": \"MoE/add_CT/restaurant_init.json\", \"file_sha1\": \"6b4c0cedd8aee76fd8a2a1b9bb1178ae0fd9cb0f\", \"columns\": {\"prompt\": \"instruction\", \"query\": \"input\", \"response\": \"output\"}}'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(dataset_info).replace(\"'\",'\"')[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SimTab_train_init-MoE-Add',\n",
       " 'WebTable_train_init-MoE-Add',\n",
       " 'Amazon_google_init-MoE-Add',\n",
       " 'Semi_text_w_init-MoE-Add',\n",
       " 'Walmart_init-MoE-Add',\n",
       " 'restaurant_init-MoE-Add']"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_list = list(dataset_info.keys())\n",
    "task_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Cleaning Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_index = np.load('MoE-Example/DC/GEIL_Data/hospital/detector/detector.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtable_train_all = pd.read_json('data/MoE_Baseline/CTA/SimTab_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4093"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def filter_df_by_occurrence(df, column_name, m):\n",
    "#     \"\"\"\n",
    "#     Filter a DataFrame based on the occurrence of elements in a specific column.\n",
    "\n",
    "#     Parameters:\n",
    "#     df (pd.DataFrame): The input DataFrame.\n",
    "#     column_name (str): The column name to consider for filtering.\n",
    "#     m (int): The threshold for occurrence.\n",
    "\n",
    "#     Returns:\n",
    "#     pd.DataFrame: A filtered DataFrame with the same structure.\n",
    "#     \"\"\"\n",
    "#     # 计算元素出现的次数\n",
    "#     occurrence_counts = df[column_name].value_counts()\n",
    "\n",
    "#     # 筛选出现次数小于等于m的元素\n",
    "#     elements_to_keep = occurrence_counts[occurrence_counts <= m].index.tolist()\n",
    "\n",
    "#     # 对出现次数大于m的元素，随机选择m个元素保留\n",
    "#     elements_to_sample = occurrence_counts[occurrence_counts > m].index.tolist()\n",
    "#     sampled_elements = np.random.choice(elements_to_sample, size=min(m, len(elements_to_sample)), replace=False)\n",
    "\n",
    "#     # Convert sampled elements to a list of strings\n",
    "#     sampled_elements = [str(element) for element in sampled_elements]\n",
    "\n",
    "#     # 合并要保留的元素\n",
    "#     elements_to_keep += sampled_elements\n",
    "\n",
    "#     # 根据筛选条件过滤DataFrame\n",
    "#     filtered_df = df[df[column_name].astype(str).isin(elements_to_keep)].copy()\n",
    "\n",
    "#     return filtered_df\n",
    "def shrink_df(df, m):\n",
    "    \"\"\"\n",
    "    Shrink a DataFrame by randomly selecting m rows with unique values in the 'index' column.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    m (int): The number of rows to select.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A shrunken DataFrame with the same structure.\n",
    "    \"\"\"\n",
    "    # 获取'index'列的元素\n",
    "    index_values = df['index'].values\n",
    "\n",
    "    # 如果m大于当前DataFrame的行数，则返回整个DataFrame\n",
    "    if m >= len(df):\n",
    "        return df\n",
    "\n",
    "    # 随机选择m个不重复的'index'值\n",
    "    selected_indices = np.random.choice(index_values, size=m, replace=False)\n",
    "\n",
    "    # 根据选取的'index'值筛选行\n",
    "    shrunken_df = df[df['index'].isin(selected_indices)].copy()\n",
    "\n",
    "    return shrunken_df\n",
    "m = 20\n",
    "df_sample = pd.DataFrame()\n",
    "unique_value = list(webtable_train_all['output'].unique())\n",
    "for u in unique_value:\n",
    "    select = webtable_train_all[webtable_train_all['output']==u]\n",
    "    if(len(select)<m):\n",
    "        df_sample = pd.concat([df_sample,select])\n",
    "    else:\n",
    "        df_sample = pd.concat([df_sample,shrink_df(select,m)])\n",
    "len(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(df_sample.to_dict(orient='records'), open('data/MoE_Baseline/CTA/SimTab_train_few.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here we combine the multiple MoE_no_Meta_Path test file for the convenience of batch inference with vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "def concatenate_test_json_files(folder_path):\n",
    "    # 构建一个路径模式来匹配所有的名字中包含'test'的json文件\n",
    "    pattern = os.path.join(folder_path, '*test*.json')\n",
    "    # 使用glob模块找到所有匹配的文件\n",
    "    files = glob(pattern)\n",
    "    \n",
    "    # 初始化一个空的DataFrame用于之后的数据拼接\n",
    "    concatenated_df = pd.DataFrame()\n",
    "    \n",
    "    # 遍历所有匹配的文件\n",
    "    for file_path in files:\n",
    "        # 读取json文件\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            df = pd.DataFrame(data)\n",
    "        \n",
    "        # 获取文件名作为domain值，去掉路径和.json后缀\n",
    "        domain = os.path.basename(file_path).replace('.json', '')\n",
    "        # 添加domain列\n",
    "        df['domain'] = domain\n",
    "        \n",
    "        # 将当前文件的DataFrame拼接到总的DataFrame中\n",
    "        concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
    "        \n",
    "    return concatenated_df\n",
    "\n",
    "# 使用示例\n",
    "folder_path = 'data/MoE_no_meta/ER'\n",
    "result_df = concatenate_test_json_files(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(result_df.to_dict(orient='records'), open('data/MoE_no_meta/ER/all_test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_detection = pd.read_csv('MoE-Example/GEIL_Data/hospital/detector/train.csv',index_col=0)\n",
    "beer_detection = pd.read_csv('MoE-Example/GEIL_Data/beers/detector/train.csv',index_col=0)\n",
    "\n",
    "rayyan_detection = pd.read_csv('MoE-Example/GEIL_Data/rayyan/detector/train.csv',index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_detection_test = pd.read_csv('MoE-Example/GEIL_Data/hospital/detector/test.csv',index_col=0)\n",
    "beer_detection_test = pd.read_csv('MoE-Example/GEIL_Data/beers/detector/test.csv',index_col=0)\n",
    "\n",
    "rayyan_detection_test = pd.read_csv('MoE-Example/GEIL_Data/rayyan/detector/test.csv',index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text_to_dict(text):\n",
    "    # 分割字符串以\"COL\"为标志，忽略第一个空分割结果\n",
    "    parts = text.split(\"COL\")[1:]\n",
    "    result_dict = {}\n",
    "    \n",
    "    for part in parts:\n",
    "        # 每个部分以\"VAL\"分割为键和值\n",
    "        key, value = part.split(\"VAL\")\n",
    "        # 去除键和值两边的空格\n",
    "        key = key.strip()\n",
    "        value = value.strip()\n",
    "        # 修正一些明显的拼写错误\n",
    "        corrected_key = key.replace(\"cexter\", \"center\").replace(\"hosxitals\", \"hospitals\").replace(\"pxeumoxia\", \"pneumonia\").replace(\"patiexts\", \"patients\").replace(\"axd\", \"and\").replace(\"givex\", \"given\").replace(\"ixfluexza\", \"influenza\").replace(\"vaccixatiox\", \"vaccination\")\n",
    "        corrected_value = value.replace(\"cexter\", \"center\").replace(\"hosxitals\", \"hospitals\").replace(\"pxeumoxia\", \"pneumonia\").replace(\"patiexts\", \"patients\").replace(\"axd\", \"and\").replace(\"givex\", \"given\").replace(\"ixfluexza\", \"influenza\").replace(\"vaccixatiox\", \"vaccination\")\n",
    "        # 如果值为\"empty\"，则将其转换为一个空字符串\n",
    "        if corrected_value.lower() == \"empty\":\n",
    "            corrected_value = \"\"\n",
    "        result_dict[corrected_key] = corrected_value\n",
    "    \n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar:  84%|████████▎ | 837/1000 [00:00<00:00, 8363.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 1000/1000 [00:00<00:00, 6326.82it/s]\n",
      "pandas bar: 100%|██████████| 1000/1000 [00:00<00:00, 18275.52it/s]\n",
      "pandas bar: 100%|██████████| 1000/1000 [00:00<00:00, 23761.61it/s]\n"
     ]
    }
   ],
   "source": [
    "def Prompt_Generalize_Baseline_ED(row): ## 在train筛选ltable_id and rtbale_id中所有类似的pos/neg放进去，但是不能完全相同，如果没有pos，那么从ground truth选4个，总数维持8个\n",
    "    left_text = row[0]\n",
    "    right_text = row[1]\n",
    "    temp_dict = {}\n",
    "    temp_dict['Output'] = ''\n",
    "    tableA_dict = parse_text_to_dict(left_text)\n",
    "    tableB_dict = parse_text_to_dict(right_text)\n",
    "    label = row[2]\n",
    "    # temp_dict = {}\n",
    "    # temp_dict['Output'] = ''\n",
    "    RAG_text = ''\n",
    "\n",
    "    text = 'You are an expert in detecting if the format of the provided attribute A in tuple T is dirty or not, and choose within the given Options.\\n\\nReturn in JSON format.\\n\\nOptions: [clean,dirty]\\n\\nOutput format example:%s\\n\\ntuple T:%s\\n\\nattribute A:%s' % (json.dumps(temp_dict),json.dumps(tableA_dict),json.dumps(tableB_dict))\n",
    "    output_dict = {}\n",
    "    output_dict['Output'] = 'clean' if label==0 else 'dirty'\n",
    "    \n",
    "    return text + RAG_text, '' , str(output_dict)\n",
    "# print(Prompt_Generalize_Baseline_ED(hospital_detection.iloc[42])[0])\n",
    "hospital_detection_output = hospital_detection.sample(n=2000).progress_apply(Prompt_Generalize_Baseline_ED,axis=1,result_type='expand')\n",
    "beer_detection_output = beer_detection.sample(n=1000).progress_apply(Prompt_Generalize_Baseline_ED,axis=1,result_type='expand')\n",
    "rayyan_detection_output = rayyan_detection.sample(n=1000).progress_apply(Prompt_Generalize_Baseline_ED,axis=1,result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 20000/20000 [00:01<00:00, 10826.03it/s]\n",
      "pandas bar: 100%|██████████| 21690/21690 [00:01<00:00, 13859.72it/s]\n",
      "pandas bar: 100%|██████████| 10000/10000 [00:00<00:00, 13756.11it/s]\n"
     ]
    }
   ],
   "source": [
    "def Prompt_Generalize_Baseline_ED(row): ## 在train筛选ltable_id and rtbale_id中所有类似的pos/neg放进去，但是不能完全相同，如果没有pos，那么从ground truth选4个，总数维持8个\n",
    "    left_text = row[0]\n",
    "    right_text = row[1]\n",
    "    temp_dict = {}\n",
    "    temp_dict['Output'] = ''\n",
    "    try:\n",
    "        tableA_dict = parse_text_to_dict(left_text)\n",
    "    except:\n",
    "        tableA_dict = {}\n",
    "    try:\n",
    "        tableB_dict = parse_text_to_dict(right_text)\n",
    "    except:\n",
    "        tableB_dict = {}\n",
    "    label = row[2]\n",
    "    # temp_dict = {}\n",
    "    # temp_dict['Output'] = ''\n",
    "    RAG_text = ''\n",
    "\n",
    "    text = 'You are an expert in detecting if the format of the provided attribute A in tuple T is dirty or not, and choose within the given Options.\\n\\nReturn in JSON format.\\n\\nOptions: [clean,dirty]\\n\\nOutput format example:%s\\n\\ntuple T:%s\\n\\nattribute A:%s' % (json.dumps(temp_dict),json.dumps(tableA_dict),json.dumps(tableB_dict))\n",
    "    output_dict = {}\n",
    "    output_dict['Output'] = 'clean' if label==0 else 'dirty'\n",
    "    \n",
    "    return text + RAG_text, '' , str(output_dict)\n",
    "hospital_detection_output_test = hospital_detection_test.progress_apply(Prompt_Generalize_Baseline_ED,axis=1,result_type='expand')\n",
    "beer_detection_output_test = beer_detection_test.progress_apply(Prompt_Generalize_Baseline_ED,axis=1,result_type='expand')\n",
    "rayyan_detection_output_test = rayyan_detection_test.progress_apply(Prompt_Generalize_Baseline_ED,axis=1,result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    COL article_title VAL Clinical and pharmacolog...\n",
       "1                   COL journal_title VAL ANN. ONCOL. \n",
       "2                                                    0\n",
       "Name: 4052, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_detection_test.iloc[4052]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_train = pd.concat([hospital_detection_output,beer_detection_output,rayyan_detection_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_train.columns = ['instruction','input','output']\n",
    "json.dump(ed_train.to_dict(orient='records'), open('data/MoE_Baseline/ED/error_detection_train.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rayyan_detection_output_test\n",
    "df.columns = ['instruction','input','output']\n",
    "json.dump(df.to_dict(orient='records'), open('data/MoE_Baseline/ED/rayyan_detection_test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encrypt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2804645/4229323758.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSHA1_cal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/data/home/wangys/LLaMA-Factory-main/data/MoE_Baseline/ED/error_detection_train.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2804645/3378912671.py\u001b[0m in \u001b[0;36mSHA1_cal\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mSHA1_cal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mhexdigest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencrypt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sha1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhexdigest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encrypt' is not defined"
     ]
    }
   ],
   "source": [
    "SHA1_cal('data/MoE_Baseline/ED/error_detection_train.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
