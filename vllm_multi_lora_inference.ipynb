{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2+cu121'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Tuple\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from vllm import EngineArgs, LLMEngine, SamplingParams, RequestOutput\n",
    "from vllm.lora.request import LoRARequest\n",
    "def initialize_engine() -> LLMEngine:\n",
    "    \"\"\"Initialize the LLMEngine.\"\"\"\n",
    "    # max_loras: controls the number of LoRAs that can be used in the same\n",
    "    #   batch. Larger numbers will cause higher memory usage, as each LoRA\n",
    "    #   slot requires its own preallocated tensor.\n",
    "    # max_lora_rank: controls the maximum supported rank of all LoRAs. Larger\n",
    "    #   numbers will cause higher memory usage. If you know that all LoRAs will\n",
    "    #   use the same rank, it is recommended to set this as low as possible.\n",
    "    # max_cpu_loras: controls the size of the CPU LoRA cache.\n",
    "    engine_args = EngineArgs(model=\"Mistral-7B-Instruct-v0.2\",\n",
    "                             enable_lora=True,\n",
    "                             max_loras=32,\n",
    "                             max_lora_rank=64,\n",
    "                             max_cpu_loras=32,\n",
    "                             max_num_seqs=256,enforce_eager=True,tensor_parallel_size=8,max_model_len=8192)\n",
    "    return LLMEngine.from_engine_args(engine_args)\n",
    "model = initialize_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>expert</th>\n",
       "      <th>query</th>\n",
       "      <th>expert_predict</th>\n",
       "      <th>domain</th>\n",
       "      <th>cross-dataset</th>\n",
       "      <th>cross-task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|semi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>You are an expert in detecting if two text des...</td>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|semi...</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>['Mistral|semi_text_w-MoE-CT', 'Mistral|SimTab...</td>\n",
       "      <td>['Mistral|SimTab-MoE-CT', 'Mistral|walmart-MoE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|semi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>You are an expert in detecting if two text des...</td>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|SimT...</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>['Mistral|SimTab-MoE-CT', 'Mistral|semi_text_w...</td>\n",
       "      <td>['Mistral|SimTab-MoE-CT', 'Mistral|walmart-MoE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|semi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>You are an expert in detecting if two text des...</td>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|walm...</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>['Mistral|walmart-MoE-CT', 'Mistral|semi_text_...</td>\n",
       "      <td>['Mistral|walmart-MoE-CT', 'Mistral|SimTab-MoE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|semi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>You are an expert in detecting if two text des...</td>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|semi...</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>['Mistral|semi_text_w-MoE-CT', 'Mistral|SimTab...</td>\n",
       "      <td>['Mistral|SimTab-MoE-CT', 'Mistral|walmart-MoE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|semi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>You are an expert in detecting if two text des...</td>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|semi...</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>['Mistral|semi_text_w-MoE-CT', 'Mistral|SimTab...</td>\n",
       "      <td>['Mistral|SimTab-MoE-CT', 'Mistral|walmart-MoE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29582</th>\n",
       "      <td>['Mistral|semi_text_w-MoE-CT', 'Mistral|webtab...</td>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|rest...</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>You are an expert in relation extraction from ...</td>\n",
       "      <td>['Mistral|SimTab-MoE-CT', 'Mistral|webtable-Mo...</td>\n",
       "      <td>Mistral|SimTab-MoE-CT</td>\n",
       "      <td>['Mistral|webtable-MoE-CT', 'Mistral|restauran...</td>\n",
       "      <td>['Mistral|restaurant-MoE-CT', 'Mistral|semi_te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29583</th>\n",
       "      <td>['Mistral|semi_text_w-MoE-CT', 'Mistral|webtab...</td>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|SimT...</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>You are an expert in relation extraction from ...</td>\n",
       "      <td>['Mistral|SimTab-MoE-CT', 'Mistral|webtable-Mo...</td>\n",
       "      <td>Mistral|SimTab-MoE-CT</td>\n",
       "      <td>['Mistral|webtable-MoE-CT', 'Mistral|restauran...</td>\n",
       "      <td>['Mistral|restaurant-MoE-CT', 'Mistral|semi_te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29584</th>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|webt...</td>\n",
       "      <td>['Mistral|semi_text_w-MoE-CT', 'Mistral|restau...</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>You are an expert in relation extraction from ...</td>\n",
       "      <td>['Mistral|SimTab-MoE-CT', 'Mistral|webtable-Mo...</td>\n",
       "      <td>Mistral|SimTab-MoE-CT</td>\n",
       "      <td>['Mistral|webtable-MoE-CT', 'Mistral|restauran...</td>\n",
       "      <td>['Mistral|restaurant-MoE-CT', 'Mistral|semi_te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29585</th>\n",
       "      <td>['Mistral|webtable-MoE-CT']</td>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|semi...</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>You are an expert in relation extraction from ...</td>\n",
       "      <td>['Mistral|SimTab-MoE-CT', 'Mistral|webtable-Mo...</td>\n",
       "      <td>Mistral|SimTab-MoE-CT</td>\n",
       "      <td>['Mistral|webtable-MoE-CT', 'Mistral|restauran...</td>\n",
       "      <td>['Mistral|restaurant-MoE-CT', 'Mistral|semi_te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29586</th>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|semi...</td>\n",
       "      <td>['Mistral|restaurant-MoE-CT', 'Mistral|walmart...</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>You are an expert in relation extraction from ...</td>\n",
       "      <td>['Mistral|SimTab-MoE-CT', 'Mistral|webtable-Mo...</td>\n",
       "      <td>Mistral|SimTab-MoE-CT</td>\n",
       "      <td>['Mistral|webtable-MoE-CT', 'Mistral|semi_text...</td>\n",
       "      <td>['Mistral|semi_text_w-MoE-CT', 'Mistral|restau...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29587 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     pos  \\\n",
       "0      ['Mistral|amazon_google-MoE-CT', 'Mistral|semi...   \n",
       "1      ['Mistral|amazon_google-MoE-CT', 'Mistral|semi...   \n",
       "2      ['Mistral|amazon_google-MoE-CT', 'Mistral|semi...   \n",
       "3      ['Mistral|amazon_google-MoE-CT', 'Mistral|semi...   \n",
       "4      ['Mistral|amazon_google-MoE-CT', 'Mistral|semi...   \n",
       "...                                                  ...   \n",
       "29582  ['Mistral|semi_text_w-MoE-CT', 'Mistral|webtab...   \n",
       "29583  ['Mistral|semi_text_w-MoE-CT', 'Mistral|webtab...   \n",
       "29584  ['Mistral|amazon_google-MoE-CT', 'Mistral|webt...   \n",
       "29585                        ['Mistral|webtable-MoE-CT']   \n",
       "29586  ['Mistral|amazon_google-MoE-CT', 'Mistral|semi...   \n",
       "\n",
       "                                                     neg  \\\n",
       "0                                                     []   \n",
       "1                                                     []   \n",
       "2                                                     []   \n",
       "3                                                     []   \n",
       "4                                                     []   \n",
       "...                                                  ...   \n",
       "29582  ['Mistral|amazon_google-MoE-CT', 'Mistral|rest...   \n",
       "29583  ['Mistral|amazon_google-MoE-CT', 'Mistral|SimT...   \n",
       "29584  ['Mistral|semi_text_w-MoE-CT', 'Mistral|restau...   \n",
       "29585  ['Mistral|amazon_google-MoE-CT', 'Mistral|semi...   \n",
       "29586  ['Mistral|restaurant-MoE-CT', 'Mistral|walmart...   \n",
       "\n",
       "                             expert  \\\n",
       "0      Mistral|amazon_google-MoE-CT   \n",
       "1      Mistral|amazon_google-MoE-CT   \n",
       "2      Mistral|amazon_google-MoE-CT   \n",
       "3      Mistral|amazon_google-MoE-CT   \n",
       "4      Mistral|amazon_google-MoE-CT   \n",
       "...                             ...   \n",
       "29582  Mistral|amazon_google-MoE-CT   \n",
       "29583  Mistral|amazon_google-MoE-CT   \n",
       "29584  Mistral|amazon_google-MoE-CT   \n",
       "29585  Mistral|amazon_google-MoE-CT   \n",
       "29586  Mistral|amazon_google-MoE-CT   \n",
       "\n",
       "                                                   query  \\\n",
       "0      You are an expert in detecting if two text des...   \n",
       "1      You are an expert in detecting if two text des...   \n",
       "2      You are an expert in detecting if two text des...   \n",
       "3      You are an expert in detecting if two text des...   \n",
       "4      You are an expert in detecting if two text des...   \n",
       "...                                                  ...   \n",
       "29582  You are an expert in relation extraction from ...   \n",
       "29583  You are an expert in relation extraction from ...   \n",
       "29584  You are an expert in relation extraction from ...   \n",
       "29585  You are an expert in relation extraction from ...   \n",
       "29586  You are an expert in relation extraction from ...   \n",
       "\n",
       "                                          expert_predict  \\\n",
       "0      ['Mistral|amazon_google-MoE-CT', 'Mistral|semi...   \n",
       "1      ['Mistral|amazon_google-MoE-CT', 'Mistral|SimT...   \n",
       "2      ['Mistral|amazon_google-MoE-CT', 'Mistral|walm...   \n",
       "3      ['Mistral|amazon_google-MoE-CT', 'Mistral|semi...   \n",
       "4      ['Mistral|amazon_google-MoE-CT', 'Mistral|semi...   \n",
       "...                                                  ...   \n",
       "29582  ['Mistral|SimTab-MoE-CT', 'Mistral|webtable-Mo...   \n",
       "29583  ['Mistral|SimTab-MoE-CT', 'Mistral|webtable-Mo...   \n",
       "29584  ['Mistral|SimTab-MoE-CT', 'Mistral|webtable-Mo...   \n",
       "29585  ['Mistral|SimTab-MoE-CT', 'Mistral|webtable-Mo...   \n",
       "29586  ['Mistral|SimTab-MoE-CT', 'Mistral|webtable-Mo...   \n",
       "\n",
       "                             domain  \\\n",
       "0      Mistral|amazon_google-MoE-CT   \n",
       "1      Mistral|amazon_google-MoE-CT   \n",
       "2      Mistral|amazon_google-MoE-CT   \n",
       "3      Mistral|amazon_google-MoE-CT   \n",
       "4      Mistral|amazon_google-MoE-CT   \n",
       "...                             ...   \n",
       "29582         Mistral|SimTab-MoE-CT   \n",
       "29583         Mistral|SimTab-MoE-CT   \n",
       "29584         Mistral|SimTab-MoE-CT   \n",
       "29585         Mistral|SimTab-MoE-CT   \n",
       "29586         Mistral|SimTab-MoE-CT   \n",
       "\n",
       "                                           cross-dataset  \\\n",
       "0      ['Mistral|semi_text_w-MoE-CT', 'Mistral|SimTab...   \n",
       "1      ['Mistral|SimTab-MoE-CT', 'Mistral|semi_text_w...   \n",
       "2      ['Mistral|walmart-MoE-CT', 'Mistral|semi_text_...   \n",
       "3      ['Mistral|semi_text_w-MoE-CT', 'Mistral|SimTab...   \n",
       "4      ['Mistral|semi_text_w-MoE-CT', 'Mistral|SimTab...   \n",
       "...                                                  ...   \n",
       "29582  ['Mistral|webtable-MoE-CT', 'Mistral|restauran...   \n",
       "29583  ['Mistral|webtable-MoE-CT', 'Mistral|restauran...   \n",
       "29584  ['Mistral|webtable-MoE-CT', 'Mistral|restauran...   \n",
       "29585  ['Mistral|webtable-MoE-CT', 'Mistral|restauran...   \n",
       "29586  ['Mistral|webtable-MoE-CT', 'Mistral|semi_text...   \n",
       "\n",
       "                                              cross-task  \n",
       "0      ['Mistral|SimTab-MoE-CT', 'Mistral|walmart-MoE...  \n",
       "1      ['Mistral|SimTab-MoE-CT', 'Mistral|walmart-MoE...  \n",
       "2      ['Mistral|walmart-MoE-CT', 'Mistral|SimTab-MoE...  \n",
       "3      ['Mistral|SimTab-MoE-CT', 'Mistral|walmart-MoE...  \n",
       "4      ['Mistral|SimTab-MoE-CT', 'Mistral|walmart-MoE...  \n",
       "...                                                  ...  \n",
       "29582  ['Mistral|restaurant-MoE-CT', 'Mistral|semi_te...  \n",
       "29583  ['Mistral|restaurant-MoE-CT', 'Mistral|semi_te...  \n",
       "29584  ['Mistral|restaurant-MoE-CT', 'Mistral|semi_te...  \n",
       "29585  ['Mistral|restaurant-MoE-CT', 'Mistral|semi_te...  \n",
       "29586  ['Mistral|semi_text_w-MoE-CT', 'Mistral|restau...  \n",
       "\n",
       "[29587 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MoE_list_update_top_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MoE_list_update_top_2 = pd.read_csv('Router/MoE_list_update_top_2.csv',index_col=0)\n",
    "def AST(row):\n",
    "    CD = row['cross-dataset']\n",
    "    row['cross-dataset'] = eval(CD)\n",
    "    CT = row['cross-task']\n",
    "    row['cross-task'] = eval(CT)\n",
    "    return row\n",
    "MoE_list_update_top_2 = MoE_list_update_top_2.apply(AST,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_list = []\n",
    "for index,row in MoE_list_update_top_2.iterrows():\n",
    "    expert_list.append(set(row['cross-dataset'])) if set(row['cross-dataset']) not in expert_list else None\n",
    "    expert_list.append(set(row['cross-task'])) if set(row['cross-task']) not in expert_list else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_path_dict = {}\n",
    "for e in expert_list:\n",
    "    expert_0,expert_1 = list(e)\n",
    "    folder_path = 'lora_weight/MoE_CT/add/Mistral/merge/%s#%s' % (expert_0,expert_1)\n",
    "    folder_path_rev = 'lora_weight/MoE_CT/add/Mistral/merge/%s#%s' % (expert_1,expert_0)\n",
    "\n",
    "    if(os.path.exists(folder_path)):\n",
    "        lora_path_dict['%s#%s' % (expert_0,expert_1)] = folder_path\n",
    "        lora_path_dict['%s#%s' % (expert_1,expert_0)] = folder_path\n",
    "    elif(os.path.exists(folder_path_rev)):\n",
    "        lora_path_dict['%s#%s' % (expert_0,expert_1)] = folder_path_rev\n",
    "        lora_path_dict['%s#%s' % (expert_1,expert_0)] = folder_path_rev\n",
    "    else:\n",
    "        print(folder_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_path_dict = {'hospital':'/home/yanmy/LLaMA-Factory-main/lora_weight/MoE/hospital-train-MoE-Llama-2-7b',\n",
    "#                   'beer':'/home/yanmy/LLaMA-Factory-main/lora_weight/MoE/beer-train-MoE-Llama-2-7b',\n",
    "#                   'rayyan':'/home/yanmy/LLaMA-Factory-main/lora_weight/MoE/rayyan-train-MoE-Llama-2-7b',\n",
    "#                   'merge-linear':'/home/yanmy/LLaMA-Factory-main/lora_weight/MoE/data-cleaning-adapter',\n",
    "#                   'merge-cat':'/home/yanmy/LLaMA-Factory-main/lora_weight/MoE/data-cleaning-adapter-cat',\n",
    "#                   'merge-hospital-linear':'/home/yanmy/LLaMA-Factory-main/lora_weight/MoE/data-cleaning-adapter-hospital',\n",
    "#                   'Mixtral-baseline':'/home/yanmy/LLaMA-Factory-main/lora_weight/Mixtral-baseline-MTL',\n",
    "#                   'jellyfish-baseline':'/home/yanmy/LLaMA-Factory-main/lora_weight/jellyfish-baseline-MTL',\n",
    "#                   'MoE-CT-amazon-google':'/home/yanmy/LLaMA-Factory-main/lora_weight/MoE_CT/add/Mistral/amazon_google-MoE-CT'}\n",
    "# def create_multi_lora_call(df,lora_id_list=['']):\n",
    "#     multi_lora_call = []\n",
    "#     if(lora_id_list!=['']): ## Lora_ID被指定\n",
    "#         # text_list = df['instruction'].to_list()\n",
    "#         for lora_id in lora_id_list:\n",
    "#             multi_lora_call.extend([[row['instruction'],lora_id,index] for index,row in df.iterrows()])\n",
    "#     else:\n",
    "#         # text_list = df['instruction'].to_list()\n",
    "#         # multi_lora_call = df['instruction'].to_list()\n",
    "#         # for index,row in df.iterrows():\n",
    "#         multi_lora_call = [[row['instruction'],row['lora_id'],index] for index,row in df.iterrows()]\n",
    "#     return multi_lora_call\n",
    "\n",
    "def create_multi_lora_call(df,lora_id_list=['']):\n",
    "    multi_lora_call = []\n",
    "    for index,row in df.iterrows():\n",
    "        lora_id = '#'.join(row['cross-task'])\n",
    "        multi_lora_call.append([row['query'],lora_id,index])\n",
    "    return multi_lora_call\n",
    "def create_test_prompts(multi_lora_call: list,lora_path: dict)-> List[Tuple[str, SamplingParams]]:\n",
    "    output_list = []\n",
    "    lora_all = list(lora_path.keys())\n",
    "    for m in multi_lora_call:\n",
    "        m_output = (\"[INST] %s [/INST]\" % m[0],\n",
    "         SamplingParams(temperature=0.0,\n",
    "                        # top_p=1,\n",
    "                        # prompt_logprobs=1,\n",
    "                        max_tokens=32),\n",
    "         LoRARequest(m[1], lora_all.index(m[1]) + 1, lora_path[m[1]]),\n",
    "         m[2])\n",
    "        output_list.append(m_output)\n",
    "    return output_list\n",
    "# multi_lora_call = create_multi_lora_call(hospital_test,lora_id='merge-hospital-linear')\n",
    "lora_id_list=list(lora_path_dict.keys())\n",
    "multi_lora_call = create_multi_lora_call(MoE_list_update_top_2,lora_id_list=list(lora_path_dict.keys()))\n",
    "test_prompts_input = create_test_prompts(multi_lora_call=multi_lora_call,lora_path=lora_path_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def process_requests(engine: LLMEngine,\n",
    "                     test_prompts: List[Tuple[str, SamplingParams,\n",
    "                                              Optional[LoRARequest]]]):\n",
    "    \"\"\"Continuously process a list of prompts and handle the outputs.\"\"\"\n",
    "    request_id = 0\n",
    "    output_list = []\n",
    "    output_request = []\n",
    "    while test_prompts or engine.has_unfinished_requests():\n",
    "        \n",
    "        if test_prompts:\n",
    "            prompt, sampling_params, lora_request, index = test_prompts.pop(0)\n",
    "            engine.add_request(str(index),\n",
    "                               prompt,\n",
    "                               \n",
    "                               sampling_params,\n",
    "                               lora_request=lora_request)\n",
    "            # print(index)\n",
    "            request_id += 1\n",
    "        # print(request_id)\n",
    "        request_outputs: List[RequestOutput] = engine.step()\n",
    "        # output_request.extend(request_outputs)\n",
    "        for request_output in request_outputs:\n",
    "            if request_output.finished:\n",
    "                output_list.append(request_output)\n",
    "    # output_list.reverse()\n",
    "    return output_list\n",
    "lora_id_list=list(lora_path_dict.keys())\n",
    "multi_lora_call = create_multi_lora_call(MoE_list_update_top_2,lora_id_list=list(lora_path_dict.keys()))\n",
    "test_prompts_input = create_test_prompts(multi_lora_call=multi_lora_call,lora_path=lora_path_dict)\n",
    "start_time = datetime.now()\n",
    "result_all = process_requests(model, test_prompts=test_prompts_input)\n",
    "end_time = datetime.now()\n",
    "print((end_time-start_time).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "54.178117 Mixtral\n",
    "37.780711 jellyfish\n",
    "\n",
    "4100 For MoE Architecture base on Code\n",
    "4059"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(result_all[0].prompt)\n",
    "# result_all[33]\n",
    "output_ins = {}\n",
    "output_predict = {}\n",
    "for lora_id in lora_id_list:\n",
    "    # output_ins[lora_id] = [''] * int(len(result_all) / len(lora_path_dict))\n",
    "    # output_predict[lora_id] = [''] * int(len(result_all) / len(lora_path_dict))\n",
    "    output_ins[lora_id] = [''] * int(len(result_all) )\n",
    "    output_predict[lora_id] = [''] * int(len(result_all) )\n",
    "# output_lora_id = [''] * len(result_all)\n",
    "for request in result_all:\n",
    "    request_id = int(request.request_id)\n",
    "    request_ins = request.prompt.strip()\n",
    "    request_lora = request.lora_request.lora_name\n",
    "    request_output = request.outputs[0].text.strip()\n",
    "    output_ins[request_lora][request_id] = request_ins\n",
    "    output_predict[request_lora][request_id] = request_output\n",
    "    # output_lora_id[request_id] = request_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in relation extraction from wikipedia web table to knowledge graph. Given table title and column pair for Table 1, please choose the most proper type from the provided options. Return in json format.\n",
      "\n",
      "Column: col0\n",
      "\n",
      "Options:[\"OfficeHolder\", \"Congressman\", \"Senator\"]\n",
      "\n",
      "Output Format Example:\n",
      "\n",
      "{\"type\": \"\"}\n",
      "\n",
      "Table 1:\n",
      "\n",
      "{\"col0\": \"M. N. Khan\", \"col1\": \"Malik Noor Khan\", \"col2\": \"2011-12-15\", \"col3\": \"1941\"}\n",
      "{\"col0\": \"M. Marye\", \"col1\": \"Madison Ellis Marye\", \"col2\": \"2016-02-23\", \"col3\": \"1944\"}\n",
      "{\"col0\": \"J. T. Lozano\", \"col1\": \"Jorge Tadeo Lozano de Peralta y González Manrique\", \"col2\": \"1816-07-06\", \"col3\": \"1772\"}\n",
      "{\"col0\": \"F. Knox\", \"col1\": \"William Franklin Knox\", \"col2\": \"1944-04-28\", \"col3\": \"1898\"}\n",
      "{\"col0\": \"F. Church\", \"col1\": \"Frank Forrester Church III\", \"col2\": \"1984-04-07\", \"col3\": \"1943\"}\n",
      "\n",
      "Reference tables:\n",
      "\n",
      "{'Table': '{\"col0\": \"American Popular Revolutionary Alliance\", \"col1\": \"Social democracy\", \"col2\": \"J. d. Castillo\", \"col3\": \"1924\", \"col4\": \"1924-05-07\"}\\n{\"col0\": \"Democratic Party\", \"col1\": \"Liberalism\", \"col2\": \"Y. Noda\", \"col3\": \"2016\", \"col4\": \"2016-03-27\"}', 'Column': 'col2', 'type': 'OfficeHolder'}\n",
      "\n",
      "{'Table': '{\"col0\": \"P. Dillingham\", \"col1\": \"Shutesbury, Massachusetts\", \"col2\": \"W. P. Dillingham\", \"col3\": \"W. P. Dillingham\", \"col4\": \"1799-08-10\"}\\n{\"col0\": \"H. H. Crapo\", \"col1\": \"Dartmouth, Massachusetts\", \"col2\": \"W. C. Durant\", \"col3\": \"W. W. Crapo\", \"col4\": \"1804-05-24\"}', 'Column': 'col3', 'type': 'Congressman'}\n",
      "\n",
      "{'Table': '{\"col0\": \"E. L. Wingo\", \"col1\": \"Edwin H Baker Pratt\", \"col2\": \"Arkansas\", \"col3\": \"1883-04-13\", \"col4\": \"1883-04-13\"}\\n{\"col0\": \"E. R. Potter\", \"col1\": \"E. R. Potter\", \"col2\": \"Kingston, Rhode Island\", \"col3\": \"1764-11-05\", \"col4\": \"1764-11-05\"}', 'Column': 'col0', 'type': 'Senator'}\n",
      "\n",
      "{'Table': '{\"col0\": \"W. M. Whistler\", \"col1\": \"England\", \"col2\": \"J. M. Whistler\", \"col3\": \"1900-02-27\"}\\n{\"col0\": \"W. F. Taylor\", \"col1\": \"Brisbane\", \"col2\": \"E. Bell\", \"col3\": \"1927-06-29\"}', 'Column': 'col2', 'type': 'OfficeHolder'}\n",
      "\n",
      "{'Table': '{\"col0\": \"Pedro Nel Ospina Vázquez\", \"col1\": \"Medellín\", \"col2\": \"M. O. Rodríguez\", \"col3\": \"1927-07-01\"}\\n{\"col0\": \"R. S. Iqbal\", \"col1\": \"Okara District\", \"col2\": \"Ranghar\", \"col3\": \"2010-09-29\"}', 'Column': 'col0', 'type': 'OfficeHolder'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(MoE_list_update_top_2.iloc[-1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_all[12000].outputs[0].text.strip()\n",
    "# print(result_all[-500].prompt.strip())\n",
    "# print(result_all[-500].outputs[0].text.strip())\n",
    "dict_output_MoE = {}\n",
    "for result in result_all:\n",
    "    prompt = result.prompt.replace('[INST] ','').replace(' [/INST]','')\n",
    "    output = result.outputs[0].text.strip()\n",
    "    dict_output_MoE[prompt] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_example = list(dict_output_MoE.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "MoE_list_update_top_2['prediction'] = MoE_list_update_top_2['query'].map(dict_output_MoE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "cound_index = []\n",
    "for index,row in MoE_list_update_top_2.iterrows():\n",
    "    predict = row['prediction']\n",
    "    try:\n",
    "        eval(predict)\n",
    "    except:\n",
    "        cound_index.append(index)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3735"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cound_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "MoE_list_update_top_2.to_csv('Router/MoE_list_update_top_2_cross_task.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
