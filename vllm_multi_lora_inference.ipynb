{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2+cu121'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 21:51:04,281\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-04 21:51:05 llm_engine.py:72] Initializing an LLM engine with config: model='/data/home/wangys/LLaMA-Factory-main/Mistral-7B-Instruct-v0.2', tokenizer='/data/home/wangys/LLaMA-Factory-main/Mistral-7B-Instruct-v0.2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W socket.cpp:436] [c10d] The server socket cannot be initialized on [::]:49463 (errno: 97 - Address family not supported by protocol).\n",
      "[W socket.cpp:663] [c10d] The client socket cannot be initialized to connect to [12-43]:49463 (errno: 97 - Address family not supported by protocol).\n",
      "\u001b[36m(RayWorkerVllm pid=3163548)\u001b[0m [W socket.cpp:663] [c10d] The client socket cannot be initialized to connect to [12-43]:49463 (errno: 97 - Address family not supported by protocol).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3155540/1054521233.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m                              max_num_seqs=256,enforce_eager=True,tensor_parallel_size=8,max_model_len=8192)\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mLLMEngine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_engine_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3155540/1054521233.py\u001b[0m in \u001b[0;36minitialize_engine\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                              \u001b[0mmax_cpu_loras\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                              max_num_seqs=256,enforce_eager=True,tensor_parallel_size=8,max_model_len=8192)\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mLLMEngine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_engine_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_engine_args\u001b[0;34m(cls, engine_args)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0mplacement_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparallel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;31m# Create the LLM engine.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         engine = cls(*engine_configs,\n\u001b[0m\u001b[1;32m    357\u001b[0m                      \u001b[0mplacement_group\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                      log_stats=not engine_args.disable_log_stats)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, lora_config, placement_group, log_stats)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Profile the memory usage and initialize the cache.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# Create the scheduler.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36m_init_cache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \"\"\"\n\u001b[1;32m    307\u001b[0m         \u001b[0;31m# Get the maximum number of blocks that can be allocated on GPU and CPU.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         num_blocks = self._run_workers(\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;34m\"profile_num_available_blocks\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mblock_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36m_run_workers\u001b[0;34m(self, method, driver_args, driver_kwargs, max_concurrent_workers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0;31m# Start the driver worker after all the ray workers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m         driver_worker_output = getattr(self.driver_worker,\n\u001b[0m\u001b[1;32m    984\u001b[0m                                        method)(*driver_args, **driver_kwargs)\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/vllm/worker/worker.py\u001b[0m in \u001b[0;36mprofile_num_available_blocks\u001b[0;34m(self, block_size, gpu_memory_utilization, cpu_swap_space, cache_dtype)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# Execute a forward pass with dummy inputs to profile the memory usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# of the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# Calculate the number of blocks that can be allocated with the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/vllm/worker/model_runner.py\u001b[0m in \u001b[0;36mprofile_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0mnum_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0mkv_caches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_caches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/vllm/worker/model_runner.py\u001b[0m in \u001b[0;36mexecute_model\u001b[0;34m(self, seq_group_metadata_list, kv_caches)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0mmodel_executable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m         hidden_states = model_executable(\n\u001b[0m\u001b[1;32m    535\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mpositions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_positions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/vllm/model_executor/models/mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, positions, kv_caches, input_metadata)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0minput_metadata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInputMetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     ) -> torch.Tensor:\n\u001b[0;32m--> 303\u001b[0;31m         hidden_states = self.model(input_ids, positions, kv_caches,\n\u001b[0m\u001b[1;32m    304\u001b[0m                                    input_metadata)\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/vllm/model_executor/models/mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, positions, kv_caches, input_metadata)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0minput_metadata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInputMetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     ) -> torch.Tensor:\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/vllm/lora/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    263\u001b[0m         )\n\u001b[1;32m    264\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         full_output = self.base_layer.forward(\n\u001b[0m\u001b[1;32m    266\u001b[0m             x.add_(indices * added_tokens_mask))\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0moutput_parallel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# Reduce across all the model parallel GPUs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_model_parallel_all_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_parallel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/vllm/model_executor/parallel_utils/communication_op.py\u001b[0m in \u001b[0;36mtensor_model_parallel_all_reduce\u001b[0;34m(input_)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     torch.distributed.all_reduce(input_,\n\u001b[0m\u001b[1;32m     35\u001b[0m                                  group=get_tensor_model_parallel_group())\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/distributed/c10d_logger.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py\u001b[0m in \u001b[0;36mall_reduce\u001b[0;34m(tensor, op, group, async_op)\u001b[0m\n\u001b[1;32m   2048\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2050\u001b[0;31m     \u001b[0mwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2052\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0masync_op\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from typing import Optional, List, Tuple\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from vllm import EngineArgs, LLMEngine, SamplingParams, RequestOutput\n",
    "from vllm.lora.request import LoRARequest\n",
    "def initialize_engine() -> LLMEngine:\n",
    "    \"\"\"Initialize the LLMEngine.\"\"\"\n",
    "    # max_loras: controls the number of LoRAs that can be used in the same\n",
    "    #   batch. Larger numbers will cause higher memory usage, as each LoRA\n",
    "    #   slot requires its own preallocated tensor.\n",
    "    # max_lora_rank: controls the maximum supported rank of all LoRAs. Larger\n",
    "    #   numbers will cause higher memory usage. If you know that all LoRAs will\n",
    "    #   use the same rank, it is recommended to set this as low as possible.\n",
    "    # max_cpu_loras: controls the size of the CPU LoRA cache.\n",
    "    engine_args = EngineArgs(model=\"/data/home/wangys/LLaMA-Factory-main/Mistral-7B-Instruct-v0.2\",\n",
    "                             enable_lora=True,\n",
    "                             max_loras=32,\n",
    "                             max_lora_rank=64,\n",
    "                             max_cpu_loras=32,\n",
    "                             max_num_seqs=256,enforce_eager=True,tensor_parallel_size=8,max_model_len=8192)\n",
    "    return LLMEngine.from_engine_args(engine_args)\n",
    "model = initialize_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>expert</th>\n",
       "      <th>query</th>\n",
       "      <th>expert_predict</th>\n",
       "      <th>domain</th>\n",
       "      <th>cross-dataset</th>\n",
       "      <th>cross-task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|semi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>You are an expert in detecting if two text des...</td>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|semi...</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>['Mistral|semi_text_w-MoE-CT', 'Mistral|SimTab...</td>\n",
       "      <td>['Mistral|SimTab-MoE-CT', 'Mistral|walmart-MoE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|semi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>You are an expert in detecting if two text des...</td>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|SimT...</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>['Mistral|SimTab-MoE-CT', 'Mistral|semi_text_w...</td>\n",
       "      <td>['Mistral|SimTab-MoE-CT', 'Mistral|walmart-MoE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|semi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>You are an expert in detecting if two text des...</td>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|walm...</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>['Mistral|walmart-MoE-CT', 'Mistral|semi_text_...</td>\n",
       "      <td>['Mistral|walmart-MoE-CT', 'Mistral|SimTab-MoE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|semi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>You are an expert in detecting if two text des...</td>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|semi...</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>['Mistral|semi_text_w-MoE-CT', 'Mistral|SimTab...</td>\n",
       "      <td>['Mistral|SimTab-MoE-CT', 'Mistral|walmart-MoE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|semi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>You are an expert in detecting if two text des...</td>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|semi...</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>['Mistral|semi_text_w-MoE-CT', 'Mistral|SimTab...</td>\n",
       "      <td>['Mistral|SimTab-MoE-CT', 'Mistral|walmart-MoE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29582</th>\n",
       "      <td>['Mistral|semi_text_w-MoE-CT', 'Mistral|webtab...</td>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|rest...</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>You are an expert in relation extraction from ...</td>\n",
       "      <td>['Mistral|SimTab-MoE-CT', 'Mistral|webtable-Mo...</td>\n",
       "      <td>Mistral|SimTab-MoE-CT</td>\n",
       "      <td>['Mistral|webtable-MoE-CT', 'Mistral|restauran...</td>\n",
       "      <td>['Mistral|restaurant-MoE-CT', 'Mistral|semi_te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29583</th>\n",
       "      <td>['Mistral|semi_text_w-MoE-CT', 'Mistral|webtab...</td>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|SimT...</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>You are an expert in relation extraction from ...</td>\n",
       "      <td>['Mistral|SimTab-MoE-CT', 'Mistral|webtable-Mo...</td>\n",
       "      <td>Mistral|SimTab-MoE-CT</td>\n",
       "      <td>['Mistral|webtable-MoE-CT', 'Mistral|restauran...</td>\n",
       "      <td>['Mistral|restaurant-MoE-CT', 'Mistral|semi_te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29584</th>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|webt...</td>\n",
       "      <td>['Mistral|semi_text_w-MoE-CT', 'Mistral|restau...</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>You are an expert in relation extraction from ...</td>\n",
       "      <td>['Mistral|SimTab-MoE-CT', 'Mistral|webtable-Mo...</td>\n",
       "      <td>Mistral|SimTab-MoE-CT</td>\n",
       "      <td>['Mistral|webtable-MoE-CT', 'Mistral|restauran...</td>\n",
       "      <td>['Mistral|restaurant-MoE-CT', 'Mistral|semi_te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29585</th>\n",
       "      <td>['Mistral|webtable-MoE-CT']</td>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|semi...</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>You are an expert in relation extraction from ...</td>\n",
       "      <td>['Mistral|SimTab-MoE-CT', 'Mistral|webtable-Mo...</td>\n",
       "      <td>Mistral|SimTab-MoE-CT</td>\n",
       "      <td>['Mistral|webtable-MoE-CT', 'Mistral|restauran...</td>\n",
       "      <td>['Mistral|restaurant-MoE-CT', 'Mistral|semi_te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29586</th>\n",
       "      <td>['Mistral|amazon_google-MoE-CT', 'Mistral|semi...</td>\n",
       "      <td>['Mistral|restaurant-MoE-CT', 'Mistral|walmart...</td>\n",
       "      <td>Mistral|amazon_google-MoE-CT</td>\n",
       "      <td>You are an expert in relation extraction from ...</td>\n",
       "      <td>['Mistral|SimTab-MoE-CT', 'Mistral|webtable-Mo...</td>\n",
       "      <td>Mistral|SimTab-MoE-CT</td>\n",
       "      <td>['Mistral|webtable-MoE-CT', 'Mistral|semi_text...</td>\n",
       "      <td>['Mistral|semi_text_w-MoE-CT', 'Mistral|restau...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29587 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     pos  \\\n",
       "0      ['Mistral|amazon_google-MoE-CT', 'Mistral|semi...   \n",
       "1      ['Mistral|amazon_google-MoE-CT', 'Mistral|semi...   \n",
       "2      ['Mistral|amazon_google-MoE-CT', 'Mistral|semi...   \n",
       "3      ['Mistral|amazon_google-MoE-CT', 'Mistral|semi...   \n",
       "4      ['Mistral|amazon_google-MoE-CT', 'Mistral|semi...   \n",
       "...                                                  ...   \n",
       "29582  ['Mistral|semi_text_w-MoE-CT', 'Mistral|webtab...   \n",
       "29583  ['Mistral|semi_text_w-MoE-CT', 'Mistral|webtab...   \n",
       "29584  ['Mistral|amazon_google-MoE-CT', 'Mistral|webt...   \n",
       "29585                        ['Mistral|webtable-MoE-CT']   \n",
       "29586  ['Mistral|amazon_google-MoE-CT', 'Mistral|semi...   \n",
       "\n",
       "                                                     neg  \\\n",
       "0                                                     []   \n",
       "1                                                     []   \n",
       "2                                                     []   \n",
       "3                                                     []   \n",
       "4                                                     []   \n",
       "...                                                  ...   \n",
       "29582  ['Mistral|amazon_google-MoE-CT', 'Mistral|rest...   \n",
       "29583  ['Mistral|amazon_google-MoE-CT', 'Mistral|SimT...   \n",
       "29584  ['Mistral|semi_text_w-MoE-CT', 'Mistral|restau...   \n",
       "29585  ['Mistral|amazon_google-MoE-CT', 'Mistral|semi...   \n",
       "29586  ['Mistral|restaurant-MoE-CT', 'Mistral|walmart...   \n",
       "\n",
       "                             expert  \\\n",
       "0      Mistral|amazon_google-MoE-CT   \n",
       "1      Mistral|amazon_google-MoE-CT   \n",
       "2      Mistral|amazon_google-MoE-CT   \n",
       "3      Mistral|amazon_google-MoE-CT   \n",
       "4      Mistral|amazon_google-MoE-CT   \n",
       "...                             ...   \n",
       "29582  Mistral|amazon_google-MoE-CT   \n",
       "29583  Mistral|amazon_google-MoE-CT   \n",
       "29584  Mistral|amazon_google-MoE-CT   \n",
       "29585  Mistral|amazon_google-MoE-CT   \n",
       "29586  Mistral|amazon_google-MoE-CT   \n",
       "\n",
       "                                                   query  \\\n",
       "0      You are an expert in detecting if two text des...   \n",
       "1      You are an expert in detecting if two text des...   \n",
       "2      You are an expert in detecting if two text des...   \n",
       "3      You are an expert in detecting if two text des...   \n",
       "4      You are an expert in detecting if two text des...   \n",
       "...                                                  ...   \n",
       "29582  You are an expert in relation extraction from ...   \n",
       "29583  You are an expert in relation extraction from ...   \n",
       "29584  You are an expert in relation extraction from ...   \n",
       "29585  You are an expert in relation extraction from ...   \n",
       "29586  You are an expert in relation extraction from ...   \n",
       "\n",
       "                                          expert_predict  \\\n",
       "0      ['Mistral|amazon_google-MoE-CT', 'Mistral|semi...   \n",
       "1      ['Mistral|amazon_google-MoE-CT', 'Mistral|SimT...   \n",
       "2      ['Mistral|amazon_google-MoE-CT', 'Mistral|walm...   \n",
       "3      ['Mistral|amazon_google-MoE-CT', 'Mistral|semi...   \n",
       "4      ['Mistral|amazon_google-MoE-CT', 'Mistral|semi...   \n",
       "...                                                  ...   \n",
       "29582  ['Mistral|SimTab-MoE-CT', 'Mistral|webtable-Mo...   \n",
       "29583  ['Mistral|SimTab-MoE-CT', 'Mistral|webtable-Mo...   \n",
       "29584  ['Mistral|SimTab-MoE-CT', 'Mistral|webtable-Mo...   \n",
       "29585  ['Mistral|SimTab-MoE-CT', 'Mistral|webtable-Mo...   \n",
       "29586  ['Mistral|SimTab-MoE-CT', 'Mistral|webtable-Mo...   \n",
       "\n",
       "                             domain  \\\n",
       "0      Mistral|amazon_google-MoE-CT   \n",
       "1      Mistral|amazon_google-MoE-CT   \n",
       "2      Mistral|amazon_google-MoE-CT   \n",
       "3      Mistral|amazon_google-MoE-CT   \n",
       "4      Mistral|amazon_google-MoE-CT   \n",
       "...                             ...   \n",
       "29582         Mistral|SimTab-MoE-CT   \n",
       "29583         Mistral|SimTab-MoE-CT   \n",
       "29584         Mistral|SimTab-MoE-CT   \n",
       "29585         Mistral|SimTab-MoE-CT   \n",
       "29586         Mistral|SimTab-MoE-CT   \n",
       "\n",
       "                                           cross-dataset  \\\n",
       "0      ['Mistral|semi_text_w-MoE-CT', 'Mistral|SimTab...   \n",
       "1      ['Mistral|SimTab-MoE-CT', 'Mistral|semi_text_w...   \n",
       "2      ['Mistral|walmart-MoE-CT', 'Mistral|semi_text_...   \n",
       "3      ['Mistral|semi_text_w-MoE-CT', 'Mistral|SimTab...   \n",
       "4      ['Mistral|semi_text_w-MoE-CT', 'Mistral|SimTab...   \n",
       "...                                                  ...   \n",
       "29582  ['Mistral|webtable-MoE-CT', 'Mistral|restauran...   \n",
       "29583  ['Mistral|webtable-MoE-CT', 'Mistral|restauran...   \n",
       "29584  ['Mistral|webtable-MoE-CT', 'Mistral|restauran...   \n",
       "29585  ['Mistral|webtable-MoE-CT', 'Mistral|restauran...   \n",
       "29586  ['Mistral|webtable-MoE-CT', 'Mistral|semi_text...   \n",
       "\n",
       "                                              cross-task  \n",
       "0      ['Mistral|SimTab-MoE-CT', 'Mistral|walmart-MoE...  \n",
       "1      ['Mistral|SimTab-MoE-CT', 'Mistral|walmart-MoE...  \n",
       "2      ['Mistral|walmart-MoE-CT', 'Mistral|SimTab-MoE...  \n",
       "3      ['Mistral|SimTab-MoE-CT', 'Mistral|walmart-MoE...  \n",
       "4      ['Mistral|SimTab-MoE-CT', 'Mistral|walmart-MoE...  \n",
       "...                                                  ...  \n",
       "29582  ['Mistral|restaurant-MoE-CT', 'Mistral|semi_te...  \n",
       "29583  ['Mistral|restaurant-MoE-CT', 'Mistral|semi_te...  \n",
       "29584  ['Mistral|restaurant-MoE-CT', 'Mistral|semi_te...  \n",
       "29585  ['Mistral|restaurant-MoE-CT', 'Mistral|semi_te...  \n",
       "29586  ['Mistral|semi_text_w-MoE-CT', 'Mistral|restau...  \n",
       "\n",
       "[29587 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MoE_list_update_top_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MoE_list_update_top_2 = pd.read_csv('Router/MoE_list_update_top_2.csv',index_col=0)\n",
    "def AST(row):\n",
    "    CD = row['cross-dataset']\n",
    "    row['cross-dataset'] = eval(CD)\n",
    "    CT = row['cross-task']\n",
    "    row['cross-task'] = eval(CT)\n",
    "    return row\n",
    "MoE_list_update_top_2 = MoE_list_update_top_2.apply(AST,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_list = []\n",
    "for index,row in MoE_list_update_top_2.iterrows():\n",
    "    expert_list.append(set(row['cross-dataset'])) if set(row['cross-dataset']) not in expert_list else None\n",
    "    expert_list.append(set(row['cross-task'])) if set(row['cross-task']) not in expert_list else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_path_dict = {}\n",
    "for e in expert_list:\n",
    "    expert_0,expert_1 = list(e)\n",
    "    folder_path = '/data/home/wangys/LLaMA-Factory-main/lora_weight/MoE_CT/add/Mistral/merge/%s#%s' % (expert_0,expert_1)\n",
    "    folder_path_rev = '/data/home/wangys/LLaMA-Factory-main/lora_weight/MoE_CT/add/Mistral/merge/%s#%s' % (expert_1,expert_0)\n",
    "\n",
    "    if(os.path.exists(folder_path)):\n",
    "        lora_path_dict['%s#%s' % (expert_0,expert_1)] = folder_path\n",
    "        lora_path_dict['%s#%s' % (expert_1,expert_0)] = folder_path\n",
    "    elif(os.path.exists(folder_path_rev)):\n",
    "        lora_path_dict['%s#%s' % (expert_0,expert_1)] = folder_path_rev\n",
    "        lora_path_dict['%s#%s' % (expert_1,expert_0)] = folder_path_rev\n",
    "    else:\n",
    "        print(folder_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_path_dict = {'hospital':'/home/yanmy/LLaMA-Factory-main/lora_weight/MoE/hospital-train-MoE-Llama-2-7b',\n",
    "#                   'beer':'/home/yanmy/LLaMA-Factory-main/lora_weight/MoE/beer-train-MoE-Llama-2-7b',\n",
    "#                   'rayyan':'/home/yanmy/LLaMA-Factory-main/lora_weight/MoE/rayyan-train-MoE-Llama-2-7b',\n",
    "#                   'merge-linear':'/home/yanmy/LLaMA-Factory-main/lora_weight/MoE/data-cleaning-adapter',\n",
    "#                   'merge-cat':'/home/yanmy/LLaMA-Factory-main/lora_weight/MoE/data-cleaning-adapter-cat',\n",
    "#                   'merge-hospital-linear':'/home/yanmy/LLaMA-Factory-main/lora_weight/MoE/data-cleaning-adapter-hospital',\n",
    "#                   'Mixtral-baseline':'/home/yanmy/LLaMA-Factory-main/lora_weight/Mixtral-baseline-MTL',\n",
    "#                   'jellyfish-baseline':'/home/yanmy/LLaMA-Factory-main/lora_weight/jellyfish-baseline-MTL',\n",
    "#                   'MoE-CT-amazon-google':'/home/yanmy/LLaMA-Factory-main/lora_weight/MoE_CT/add/Mistral/amazon_google-MoE-CT'}\n",
    "# def create_multi_lora_call(df,lora_id_list=['']):\n",
    "#     multi_lora_call = []\n",
    "#     if(lora_id_list!=['']): ## Lora_ID被指定\n",
    "#         # text_list = df['instruction'].to_list()\n",
    "#         for lora_id in lora_id_list:\n",
    "#             multi_lora_call.extend([[row['instruction'],lora_id,index] for index,row in df.iterrows()])\n",
    "#     else:\n",
    "#         # text_list = df['instruction'].to_list()\n",
    "#         # multi_lora_call = df['instruction'].to_list()\n",
    "#         # for index,row in df.iterrows():\n",
    "#         multi_lora_call = [[row['instruction'],row['lora_id'],index] for index,row in df.iterrows()]\n",
    "#     return multi_lora_call\n",
    "\n",
    "def create_multi_lora_call(df,lora_id_list=['']):\n",
    "    multi_lora_call = []\n",
    "    for index,row in df.iterrows():\n",
    "        lora_id = '#'.join(row['cross-task'])\n",
    "        multi_lora_call.append([row['query'],lora_id,index])\n",
    "    return multi_lora_call\n",
    "def create_test_prompts(multi_lora_call: list,lora_path: dict)-> List[Tuple[str, SamplingParams]]:\n",
    "    output_list = []\n",
    "    lora_all = list(lora_path.keys())\n",
    "    for m in multi_lora_call:\n",
    "        m_output = (\"[INST] %s [/INST]\" % m[0],\n",
    "         SamplingParams(temperature=0.0,\n",
    "                        # top_p=1,\n",
    "                        # prompt_logprobs=1,\n",
    "                        max_tokens=32),\n",
    "         LoRARequest(m[1], lora_all.index(m[1]) + 1, lora_path[m[1]]),\n",
    "         m[2])\n",
    "        output_list.append(m_output)\n",
    "    return output_list\n",
    "# multi_lora_call = create_multi_lora_call(hospital_test,lora_id='merge-hospital-linear')\n",
    "lora_id_list=list(lora_path_dict.keys())\n",
    "multi_lora_call = create_multi_lora_call(MoE_list_update_top_2,lora_id_list=list(lora_path_dict.keys()))\n",
    "test_prompts_input = create_test_prompts(multi_lora_call=multi_lora_call,lora_path=lora_path_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-04 00:17:42 llm_engine.py:877] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:17:47 llm_engine.py:877] Avg prompt throughput: 8703.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:17:53 llm_engine.py:877] Avg prompt throughput: 8734.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 49 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:17:58 llm_engine.py:877] Avg prompt throughput: 8711.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 73 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:18:03 llm_engine.py:877] Avg prompt throughput: 8726.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 97 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:18:08 llm_engine.py:877] Avg prompt throughput: 8727.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 121 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:18:13 llm_engine.py:877] Avg prompt throughput: 8731.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 145 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:18:18 llm_engine.py:877] Avg prompt throughput: 7327.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 165 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:18:23 llm_engine.py:877] Avg prompt throughput: 8712.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 189 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:18:28 llm_engine.py:877] Avg prompt throughput: 8700.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 213 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:18:33 llm_engine.py:877] Avg prompt throughput: 8712.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:18:39 llm_engine.py:877] Avg prompt throughput: 6721.9 tokens/s, Avg generation throughput: 1034.1 tokens/s, Running: 222 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 10.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:18:44 llm_engine.py:877] Avg prompt throughput: 9993.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 246 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:18:49 llm_engine.py:877] Avg prompt throughput: 8425.9 tokens/s, Avg generation throughput: 89.7 tokens/s, Running: 51 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:18:54 llm_engine.py:877] Avg prompt throughput: 8716.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 75 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:18:59 llm_engine.py:877] Avg prompt throughput: 8721.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 99 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:19:04 llm_engine.py:877] Avg prompt throughput: 8749.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 123 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:19:10 llm_engine.py:877] Avg prompt throughput: 8744.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 147 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:19:15 llm_engine.py:877] Avg prompt throughput: 8729.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 171 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:19:20 llm_engine.py:877] Avg prompt throughput: 8714.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 195 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:19:25 llm_engine.py:877] Avg prompt throughput: 8724.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 219 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:19:30 llm_engine.py:877] Avg prompt throughput: 8698.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 243 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:19:35 llm_engine.py:877] Avg prompt throughput: 7238.0 tokens/s, Avg generation throughput: 576.0 tokens/s, Running: 197 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:19:40 llm_engine.py:877] Avg prompt throughput: 8712.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 221 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:19:45 llm_engine.py:877] Avg prompt throughput: 8693.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:19:51 llm_engine.py:877] Avg prompt throughput: 8405.4 tokens/s, Avg generation throughput: 97.7 tokens/s, Running: 80 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:19:56 llm_engine.py:877] Avg prompt throughput: 8705.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 104 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:20:01 llm_engine.py:877] Avg prompt throughput: 8744.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 128 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:20:06 llm_engine.py:877] Avg prompt throughput: 8698.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 152 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:20:11 llm_engine.py:877] Avg prompt throughput: 6758.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 171 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:20:16 llm_engine.py:877] Avg prompt throughput: 8702.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 195 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:20:21 llm_engine.py:877] Avg prompt throughput: 8713.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 219 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:20:26 llm_engine.py:877] Avg prompt throughput: 8672.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 243 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:20:32 llm_engine.py:877] Avg prompt throughput: 8227.9 tokens/s, Avg generation throughput: 334.5 tokens/s, Running: 246 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:20:37 llm_engine.py:877] Avg prompt throughput: 8438.4 tokens/s, Avg generation throughput: 90.1 tokens/s, Running: 201 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:20:42 llm_engine.py:877] Avg prompt throughput: 8700.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 225 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:20:47 llm_engine.py:877] Avg prompt throughput: 8699.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:20:52 llm_engine.py:877] Avg prompt throughput: 8469.2 tokens/s, Avg generation throughput: 73.1 tokens/s, Running: 104 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:20:57 llm_engine.py:877] Avg prompt throughput: 8720.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 128 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:21:02 llm_engine.py:877] Avg prompt throughput: 8708.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 152 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:21:08 llm_engine.py:877] Avg prompt throughput: 8714.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 176 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:21:13 llm_engine.py:877] Avg prompt throughput: 8696.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 200 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:21:18 llm_engine.py:877] Avg prompt throughput: 8727.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 224 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:21:23 llm_engine.py:877] Avg prompt throughput: 8653.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:21:28 llm_engine.py:877] Avg prompt throughput: 8220.5 tokens/s, Avg generation throughput: 238.0 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:21:33 llm_engine.py:877] Avg prompt throughput: 8477.3 tokens/s, Avg generation throughput: 283.6 tokens/s, Running: 181 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:21:38 llm_engine.py:877] Avg prompt throughput: 8740.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 205 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:21:43 llm_engine.py:877] Avg prompt throughput: 8699.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 229 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:21:48 llm_engine.py:877] Avg prompt throughput: 8694.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:21:54 llm_engine.py:877] Avg prompt throughput: 8430.6 tokens/s, Avg generation throughput: 58.7 tokens/s, Running: 130 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:21:59 llm_engine.py:877] Avg prompt throughput: 8718.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 154 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:22:04 llm_engine.py:877] Avg prompt throughput: 8720.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 178 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:22:09 llm_engine.py:877] Avg prompt throughput: 8702.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 202 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:22:14 llm_engine.py:877] Avg prompt throughput: 8747.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 226 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:22:19 llm_engine.py:877] Avg prompt throughput: 8707.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 250 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:22:24 llm_engine.py:877] Avg prompt throughput: 8177.2 tokens/s, Avg generation throughput: 211.9 tokens/s, Running: 229 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:22:29 llm_engine.py:877] Avg prompt throughput: 8693.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:22:35 llm_engine.py:877] Avg prompt throughput: 8423.7 tokens/s, Avg generation throughput: 58.6 tokens/s, Running: 176 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:22:40 llm_engine.py:877] Avg prompt throughput: 8698.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 200 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:22:45 llm_engine.py:877] Avg prompt throughput: 8720.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 224 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:22:50 llm_engine.py:877] Avg prompt throughput: 8719.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:22:55 llm_engine.py:877] Avg prompt throughput: 8425.5 tokens/s, Avg generation throughput: 77.5 tokens/s, Running: 157 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:23:00 llm_engine.py:877] Avg prompt throughput: 8718.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 181 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:23:05 llm_engine.py:877] Avg prompt throughput: 8686.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 205 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:23:10 llm_engine.py:877] Avg prompt throughput: 8702.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 229 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:23:16 llm_engine.py:877] Avg prompt throughput: 8704.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:23:21 llm_engine.py:877] Avg prompt throughput: 8202.1 tokens/s, Avg generation throughput: 180.6 tokens/s, Running: 211 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:23:26 llm_engine.py:877] Avg prompt throughput: 8695.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 235 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:23:31 llm_engine.py:877] Avg prompt throughput: 6482.0 tokens/s, Avg generation throughput: 226.9 tokens/s, Running: 152 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 7.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:23:37 llm_engine.py:877] Avg prompt throughput: 9581.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 176 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:23:42 llm_engine.py:877] Avg prompt throughput: 8719.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 200 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:23:47 llm_engine.py:877] Avg prompt throughput: 8711.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 224 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:23:52 llm_engine.py:877] Avg prompt throughput: 8676.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:23:57 llm_engine.py:877] Avg prompt throughput: 8416.3 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 179 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:24:02 llm_engine.py:877] Avg prompt throughput: 8716.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 203 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:24:07 llm_engine.py:877] Avg prompt throughput: 8710.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 227 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:24:12 llm_engine.py:877] Avg prompt throughput: 8702.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:24:18 llm_engine.py:877] Avg prompt throughput: 8450.9 tokens/s, Avg generation throughput: 66.4 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:24:23 llm_engine.py:877] Avg prompt throughput: 8439.2 tokens/s, Avg generation throughput: 53.4 tokens/s, Running: 217 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:24:28 llm_engine.py:877] Avg prompt throughput: 8681.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 241 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:24:33 llm_engine.py:877] Avg prompt throughput: 8392.9 tokens/s, Avg generation throughput: 142.1 tokens/s, Running: 166 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:24:38 llm_engine.py:877] Avg prompt throughput: 8742.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 190 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:24:43 llm_engine.py:877] Avg prompt throughput: 8693.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 214 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:24:48 llm_engine.py:877] Avg prompt throughput: 8665.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 238 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:24:53 llm_engine.py:877] Avg prompt throughput: 8464.4 tokens/s, Avg generation throughput: 226.7 tokens/s, Running: 180 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:24:59 llm_engine.py:877] Avg prompt throughput: 8701.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 204 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:25:04 llm_engine.py:877] Avg prompt throughput: 8701.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 228 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:25:09 llm_engine.py:877] Avg prompt throughput: 8725.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 252 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:25:14 llm_engine.py:877] Avg prompt throughput: 8446.3 tokens/s, Avg generation throughput: 62.4 tokens/s, Running: 252 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:25:19 llm_engine.py:877] Avg prompt throughput: 8407.7 tokens/s, Avg generation throughput: 61.9 tokens/s, Running: 202 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:25:24 llm_engine.py:877] Avg prompt throughput: 8727.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 226 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:25:29 llm_engine.py:877] Avg prompt throughput: 8712.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 250 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:25:34 llm_engine.py:877] Avg prompt throughput: 8421.5 tokens/s, Avg generation throughput: 69.5 tokens/s, Running: 180 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:25:39 llm_engine.py:877] Avg prompt throughput: 8660.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 204 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:25:45 llm_engine.py:877] Avg prompt throughput: 8689.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 228 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:25:50 llm_engine.py:877] Avg prompt throughput: 8700.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 252 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:25:55 llm_engine.py:877] Avg prompt throughput: 8410.6 tokens/s, Avg generation throughput: 62.3 tokens/s, Running: 204 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:26:00 llm_engine.py:877] Avg prompt throughput: 8464.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 228 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:26:05 llm_engine.py:877] Avg prompt throughput: 9078.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 242 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:26:11 llm_engine.py:877] Avg prompt throughput: 9021.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:26:16 llm_engine.py:877] Avg prompt throughput: 9224.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 234 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:26:21 llm_engine.py:877] Avg prompt throughput: 8982.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:26:26 llm_engine.py:877] Avg prompt throughput: 8688.6 tokens/s, Avg generation throughput: 97.5 tokens/s, Running: 192 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:26:31 llm_engine.py:877] Avg prompt throughput: 8936.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 207 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:26:36 llm_engine.py:877] Avg prompt throughput: 8879.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 221 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:26:42 llm_engine.py:877] Avg prompt throughput: 8863.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 236 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:26:47 llm_engine.py:877] Avg prompt throughput: 8949.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:26:52 llm_engine.py:877] Avg prompt throughput: 8670.9 tokens/s, Avg generation throughput: 77.5 tokens/s, Running: 182 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:26:57 llm_engine.py:877] Avg prompt throughput: 8899.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 196 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:27:02 llm_engine.py:877] Avg prompt throughput: 8966.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 210 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:27:07 llm_engine.py:877] Avg prompt throughput: 9003.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 224 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:27:12 llm_engine.py:877] Avg prompt throughput: 6946.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 236 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:27:18 llm_engine.py:877] Avg prompt throughput: 8921.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:27:23 llm_engine.py:877] Avg prompt throughput: 8598.4 tokens/s, Avg generation throughput: 78.3 tokens/s, Running: 215 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:27:28 llm_engine.py:877] Avg prompt throughput: 8926.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 229 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:27:33 llm_engine.py:877] Avg prompt throughput: 8972.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:27:38 llm_engine.py:877] Avg prompt throughput: 7747.9 tokens/s, Avg generation throughput: 283.9 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 20.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:27:44 llm_engine.py:877] Avg prompt throughput: 9862.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:27:49 llm_engine.py:877] Avg prompt throughput: 8603.3 tokens/s, Avg generation throughput: 96.4 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 20.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:27:54 llm_engine.py:877] Avg prompt throughput: 8841.9 tokens/s, Avg generation throughput: 180.7 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 19.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:27:59 llm_engine.py:877] Avg prompt throughput: 8761.4 tokens/s, Avg generation throughput: 71.7 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:28:05 llm_engine.py:877] Avg prompt throughput: 8448.4 tokens/s, Avg generation throughput: 206.6 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 20.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:28:10 llm_engine.py:877] Avg prompt throughput: 7890.4 tokens/s, Avg generation throughput: 171.4 tokens/s, Running: 243 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 19.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:28:15 llm_engine.py:877] Avg prompt throughput: 9086.5 tokens/s, Avg generation throughput: 511.3 tokens/s, Running: 229 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 18.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:28:20 llm_engine.py:877] Avg prompt throughput: 9095.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:28:26 llm_engine.py:877] Avg prompt throughput: 8611.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 20.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:28:31 llm_engine.py:877] Avg prompt throughput: 9335.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 217 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:28:36 llm_engine.py:877] Avg prompt throughput: 8977.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 232 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:28:41 llm_engine.py:877] Avg prompt throughput: 8805.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 247 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:28:46 llm_engine.py:877] Avg prompt throughput: 7879.7 tokens/s, Avg generation throughput: 140.1 tokens/s, Running: 195 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:28:51 llm_engine.py:877] Avg prompt throughput: 8931.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 210 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:28:57 llm_engine.py:877] Avg prompt throughput: 8886.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 225 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:29:02 llm_engine.py:877] Avg prompt throughput: 8889.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 240 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:29:07 llm_engine.py:877] Avg prompt throughput: 8837.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:29:12 llm_engine.py:877] Avg prompt throughput: 8569.7 tokens/s, Avg generation throughput: 59.7 tokens/s, Running: 230 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:29:17 llm_engine.py:877] Avg prompt throughput: 8796.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:29:22 llm_engine.py:877] Avg prompt throughput: 7959.3 tokens/s, Avg generation throughput: 241.6 tokens/s, Running: 252 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 19.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:29:27 llm_engine.py:877] Avg prompt throughput: 9200.3 tokens/s, Avg generation throughput: 72.9 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:29:33 llm_engine.py:877] Avg prompt throughput: 8912.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 252 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:29:38 llm_engine.py:877] Avg prompt throughput: 8615.8 tokens/s, Avg generation throughput: 70.9 tokens/s, Running: 243 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:29:43 llm_engine.py:877] Avg prompt throughput: 7767.8 tokens/s, Avg generation throughput: 320.6 tokens/s, Running: 238 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 18.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:29:48 llm_engine.py:877] Avg prompt throughput: 9010.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:29:53 llm_engine.py:877] Avg prompt throughput: 8414.4 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 19.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:29:59 llm_engine.py:877] Avg prompt throughput: 8720.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:30:04 llm_engine.py:877] Avg prompt throughput: 8386.7 tokens/s, Avg generation throughput: 173.3 tokens/s, Running: 247 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 19.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:30:09 llm_engine.py:877] Avg prompt throughput: 6561.1 tokens/s, Avg generation throughput: 341.9 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 19.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:30:15 llm_engine.py:877] Avg prompt throughput: 8107.4 tokens/s, Avg generation throughput: 219.7 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 19.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:30:20 llm_engine.py:877] Avg prompt throughput: 8886.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 250 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:30:25 llm_engine.py:877] Avg prompt throughput: 8655.7 tokens/s, Avg generation throughput: 90.5 tokens/s, Running: 239 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:30:30 llm_engine.py:877] Avg prompt throughput: 8813.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:30:35 llm_engine.py:877] Avg prompt throughput: 8622.8 tokens/s, Avg generation throughput: 56.8 tokens/s, Running: 230 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:30:40 llm_engine.py:877] Avg prompt throughput: 9072.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:30:46 llm_engine.py:877] Avg prompt throughput: 8730.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 20.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:30:51 llm_engine.py:877] Avg prompt throughput: 9194.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 222 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:30:56 llm_engine.py:877] Avg prompt throughput: 8891.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:31:01 llm_engine.py:877] Avg prompt throughput: 8969.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:31:06 llm_engine.py:877] Avg prompt throughput: 8609.1 tokens/s, Avg generation throughput: 80.9 tokens/s, Running: 235 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:31:11 llm_engine.py:877] Avg prompt throughput: 8787.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 250 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:31:17 llm_engine.py:877] Avg prompt throughput: 8547.0 tokens/s, Avg generation throughput: 83.2 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:31:22 llm_engine.py:877] Avg prompt throughput: 8245.4 tokens/s, Avg generation throughput: 55.9 tokens/s, Running: 241 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 19.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:31:27 llm_engine.py:877] Avg prompt throughput: 9810.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:31:32 llm_engine.py:877] Avg prompt throughput: 8760.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 240 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:31:37 llm_engine.py:877] Avg prompt throughput: 8974.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:31:43 llm_engine.py:877] Avg prompt throughput: 8640.3 tokens/s, Avg generation throughput: 55.8 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:31:48 llm_engine.py:877] Avg prompt throughput: 7963.5 tokens/s, Avg generation throughput: 100.5 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:31:53 llm_engine.py:877] Avg prompt throughput: 8693.8 tokens/s, Avg generation throughput: 64.0 tokens/s, Running: 246 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:31:59 llm_engine.py:877] Avg prompt throughput: 7806.3 tokens/s, Avg generation throughput: 139.7 tokens/s, Running: 247 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:32:04 llm_engine.py:877] Avg prompt throughput: 8565.2 tokens/s, Avg generation throughput: 136.5 tokens/s, Running: 246 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 19.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:32:09 llm_engine.py:877] Avg prompt throughput: 7942.6 tokens/s, Avg generation throughput: 130.2 tokens/s, Running: 247 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:32:14 llm_engine.py:877] Avg prompt throughput: 8494.3 tokens/s, Avg generation throughput: 149.5 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 19.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:32:19 llm_engine.py:877] Avg prompt throughput: 8670.5 tokens/s, Avg generation throughput: 208.8 tokens/s, Running: 246 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 19.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:32:25 llm_engine.py:877] Avg prompt throughput: 8593.9 tokens/s, Avg generation throughput: 115.7 tokens/s, Running: 243 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:32:31 llm_engine.py:877] Avg prompt throughput: 8060.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:32:36 llm_engine.py:877] Avg prompt throughput: 8786.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:32:41 llm_engine.py:877] Avg prompt throughput: 9021.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 252 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:32:46 llm_engine.py:877] Avg prompt throughput: 8553.6 tokens/s, Avg generation throughput: 73.2 tokens/s, Running: 228 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:32:51 llm_engine.py:877] Avg prompt throughput: 8925.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 243 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:32:56 llm_engine.py:877] Avg prompt throughput: 8077.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:33:01 llm_engine.py:877] Avg prompt throughput: 8762.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:33:07 llm_engine.py:877] Avg prompt throughput: 7821.9 tokens/s, Avg generation throughput: 215.5 tokens/s, Running: 252 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 19.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:33:12 llm_engine.py:877] Avg prompt throughput: 9595.8 tokens/s, Avg generation throughput: 71.6 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:33:17 llm_engine.py:877] Avg prompt throughput: 8893.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 252 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:33:22 llm_engine.py:877] Avg prompt throughput: 8671.9 tokens/s, Avg generation throughput: 72.2 tokens/s, Running: 243 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:33:28 llm_engine.py:877] Avg prompt throughput: 6607.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:33:33 llm_engine.py:877] Avg prompt throughput: 9519.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:33:38 llm_engine.py:877] Avg prompt throughput: 8774.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 19.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:33:43 llm_engine.py:877] Avg prompt throughput: 9167.6 tokens/s, Avg generation throughput: 306.8 tokens/s, Running: 240 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 18.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:33:49 llm_engine.py:877] Avg prompt throughput: 9067.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:33:54 llm_engine.py:877] Avg prompt throughput: 8644.9 tokens/s, Avg generation throughput: 56.1 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:33:59 llm_engine.py:877] Avg prompt throughput: 8679.4 tokens/s, Avg generation throughput: 63.0 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:34:05 llm_engine.py:877] Avg prompt throughput: 7901.8 tokens/s, Avg generation throughput: 163.2 tokens/s, Running: 246 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:34:10 llm_engine.py:877] Avg prompt throughput: 8567.7 tokens/s, Avg generation throughput: 151.3 tokens/s, Running: 246 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:34:15 llm_engine.py:877] Avg prompt throughput: 8026.5 tokens/s, Avg generation throughput: 138.1 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:34:21 llm_engine.py:877] Avg prompt throughput: 8479.1 tokens/s, Avg generation throughput: 57.4 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:34:26 llm_engine.py:877] Avg prompt throughput: 8147.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:34:31 llm_engine.py:877] Avg prompt throughput: 8760.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 240 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:34:36 llm_engine.py:877] Avg prompt throughput: 9689.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:34:42 llm_engine.py:877] Avg prompt throughput: 8576.8 tokens/s, Avg generation throughput: 52.4 tokens/s, Running: 239 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:34:47 llm_engine.py:877] Avg prompt throughput: 9052.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:34:52 llm_engine.py:877] Avg prompt throughput: 8692.2 tokens/s, Avg generation throughput: 58.5 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:34:57 llm_engine.py:877] Avg prompt throughput: 7966.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:35:02 llm_engine.py:877] Avg prompt throughput: 8348.1 tokens/s, Avg generation throughput: 110.3 tokens/s, Running: 232 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 18.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:35:08 llm_engine.py:877] Avg prompt throughput: 8976.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 247 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:35:13 llm_engine.py:877] Avg prompt throughput: 8662.6 tokens/s, Avg generation throughput: 140.7 tokens/s, Running: 241 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:35:18 llm_engine.py:877] Avg prompt throughput: 9027.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:35:23 llm_engine.py:877] Avg prompt throughput: 7947.2 tokens/s, Avg generation throughput: 56.9 tokens/s, Running: 240 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:35:28 llm_engine.py:877] Avg prompt throughput: 8858.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:35:33 llm_engine.py:877] Avg prompt throughput: 8689.4 tokens/s, Avg generation throughput: 55.8 tokens/s, Running: 252 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:35:38 llm_engine.py:877] Avg prompt throughput: 8670.7 tokens/s, Avg generation throughput: 70.1 tokens/s, Running: 242 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:35:44 llm_engine.py:877] Avg prompt throughput: 7880.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:35:49 llm_engine.py:877] Avg prompt throughput: 9562.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:35:55 llm_engine.py:877] Avg prompt throughput: 8713.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 247 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:36:00 llm_engine.py:877] Avg prompt throughput: 7920.7 tokens/s, Avg generation throughput: 134.9 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 19.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:36:05 llm_engine.py:877] Avg prompt throughput: 9078.5 tokens/s, Avg generation throughput: 202.6 tokens/s, Running: 241 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:36:10 llm_engine.py:877] Avg prompt throughput: 8927.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:36:16 llm_engine.py:877] Avg prompt throughput: 8595.8 tokens/s, Avg generation throughput: 99.1 tokens/s, Running: 250 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 19.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:36:21 llm_engine.py:877] Avg prompt throughput: 8905.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 252 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:36:27 llm_engine.py:877] Avg prompt throughput: 8562.8 tokens/s, Avg generation throughput: 70.3 tokens/s, Running: 234 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 18.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:36:32 llm_engine.py:877] Avg prompt throughput: 8776.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:36:37 llm_engine.py:877] Avg prompt throughput: 8748.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:36:42 llm_engine.py:877] Avg prompt throughput: 8697.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:36:47 llm_engine.py:877] Avg prompt throughput: 6641.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 252 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:36:52 llm_engine.py:877] Avg prompt throughput: 8701.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:36:58 llm_engine.py:877] Avg prompt throughput: 7903.7 tokens/s, Avg generation throughput: 105.3 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:37:03 llm_engine.py:877] Avg prompt throughput: 8802.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 243 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:37:08 llm_engine.py:877] Avg prompt throughput: 8741.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 19.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:37:13 llm_engine.py:877] Avg prompt throughput: 8742.5 tokens/s, Avg generation throughput: 63.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:37:18 llm_engine.py:877] Avg prompt throughput: 6736.0 tokens/s, Avg generation throughput: 267.0 tokens/s, Running: 252 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:37:23 llm_engine.py:877] Avg prompt throughput: 7090.5 tokens/s, Avg generation throughput: 217.6 tokens/s, Running: 241 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:37:29 llm_engine.py:877] Avg prompt throughput: 7841.7 tokens/s, Avg generation throughput: 147.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:37:34 llm_engine.py:877] Avg prompt throughput: 7676.1 tokens/s, Avg generation throughput: 105.4 tokens/s, Running: 240 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:37:39 llm_engine.py:877] Avg prompt throughput: 7815.0 tokens/s, Avg generation throughput: 76.9 tokens/s, Running: 227 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:37:44 llm_engine.py:877] Avg prompt throughput: 7950.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:37:49 llm_engine.py:877] Avg prompt throughput: 7967.6 tokens/s, Avg generation throughput: 55.7 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:37:54 llm_engine.py:877] Avg prompt throughput: 7728.6 tokens/s, Avg generation throughput: 58.0 tokens/s, Running: 225 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:37:59 llm_engine.py:877] Avg prompt throughput: 7946.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 235 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:38:04 llm_engine.py:877] Avg prompt throughput: 7998.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:38:09 llm_engine.py:877] Avg prompt throughput: 7865.4 tokens/s, Avg generation throughput: 52.9 tokens/s, Running: 234 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:38:14 llm_engine.py:877] Avg prompt throughput: 7989.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:38:19 llm_engine.py:877] Avg prompt throughput: 7881.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:38:24 llm_engine.py:877] Avg prompt throughput: 8047.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:38:29 llm_engine.py:877] Avg prompt throughput: 7887.4 tokens/s, Avg generation throughput: 60.1 tokens/s, Running: 234 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:38:34 llm_engine.py:877] Avg prompt throughput: 8054.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 234 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:38:39 llm_engine.py:877] Avg prompt throughput: 8023.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:38:44 llm_engine.py:877] Avg prompt throughput: 8009.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:38:49 llm_engine.py:877] Avg prompt throughput: 8052.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 246 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:38:54 llm_engine.py:877] Avg prompt throughput: 7742.2 tokens/s, Avg generation throughput: 64.9 tokens/s, Running: 228 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:38:59 llm_engine.py:877] Avg prompt throughput: 8066.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 228 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:39:04 llm_engine.py:877] Avg prompt throughput: 8032.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:39:10 llm_engine.py:877] Avg prompt throughput: 8080.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 238 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:39:15 llm_engine.py:877] Avg prompt throughput: 8066.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 241 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:39:20 llm_engine.py:877] Avg prompt throughput: 8104.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:39:25 llm_engine.py:877] Avg prompt throughput: 8027.1 tokens/s, Avg generation throughput: 51.2 tokens/s, Running: 227 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:39:30 llm_engine.py:877] Avg prompt throughput: 8116.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:39:35 llm_engine.py:877] Avg prompt throughput: 8068.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 241 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:39:40 llm_engine.py:877] Avg prompt throughput: 8039.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:39:45 llm_engine.py:877] Avg prompt throughput: 7924.0 tokens/s, Avg generation throughput: 62.9 tokens/s, Running: 228 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:39:50 llm_engine.py:877] Avg prompt throughput: 7960.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 231 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:39:55 llm_engine.py:877] Avg prompt throughput: 8058.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:40:00 llm_engine.py:877] Avg prompt throughput: 8101.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 246 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:40:05 llm_engine.py:877] Avg prompt throughput: 7741.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:40:10 llm_engine.py:877] Avg prompt throughput: 5959.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:40:16 llm_engine.py:877] Avg prompt throughput: 7470.2 tokens/s, Avg generation throughput: 56.8 tokens/s, Running: 216 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:40:21 llm_engine.py:877] Avg prompt throughput: 7951.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 236 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:40:26 llm_engine.py:877] Avg prompt throughput: 8005.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 235 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:40:31 llm_engine.py:877] Avg prompt throughput: 8087.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 233 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:40:36 llm_engine.py:877] Avg prompt throughput: 8077.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:40:41 llm_engine.py:877] Avg prompt throughput: 7877.5 tokens/s, Avg generation throughput: 57.7 tokens/s, Running: 211 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:40:46 llm_engine.py:877] Avg prompt throughput: 8126.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 232 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:40:51 llm_engine.py:877] Avg prompt throughput: 8027.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 241 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:40:56 llm_engine.py:877] Avg prompt throughput: 8048.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 238 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:41:01 llm_engine.py:877] Avg prompt throughput: 7815.1 tokens/s, Avg generation throughput: 81.1 tokens/s, Running: 217 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:41:06 llm_engine.py:877] Avg prompt throughput: 8228.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 222 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:41:11 llm_engine.py:877] Avg prompt throughput: 8103.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 239 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:41:16 llm_engine.py:877] Avg prompt throughput: 8040.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:41:21 llm_engine.py:877] Avg prompt throughput: 8076.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 242 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:41:26 llm_engine.py:877] Avg prompt throughput: 7694.3 tokens/s, Avg generation throughput: 74.6 tokens/s, Running: 216 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:41:31 llm_engine.py:877] Avg prompt throughput: 8078.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 224 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:41:36 llm_engine.py:877] Avg prompt throughput: 8192.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 239 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:41:42 llm_engine.py:877] Avg prompt throughput: 7927.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:41:47 llm_engine.py:877] Avg prompt throughput: 8105.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 236 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:41:52 llm_engine.py:877] Avg prompt throughput: 8045.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:41:57 llm_engine.py:877] Avg prompt throughput: 8239.3 tokens/s, Avg generation throughput: 52.1 tokens/s, Running: 209 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:42:02 llm_engine.py:877] Avg prompt throughput: 8191.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 225 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:42:07 llm_engine.py:877] Avg prompt throughput: 8167.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 236 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:42:12 llm_engine.py:877] Avg prompt throughput: 7967.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 235 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:42:17 llm_engine.py:877] Avg prompt throughput: 8043.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:42:22 llm_engine.py:877] Avg prompt throughput: 7884.1 tokens/s, Avg generation throughput: 54.9 tokens/s, Running: 215 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:42:27 llm_engine.py:877] Avg prompt throughput: 7998.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 233 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:42:32 llm_engine.py:877] Avg prompt throughput: 8159.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:42:37 llm_engine.py:877] Avg prompt throughput: 8026.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 238 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:42:42 llm_engine.py:877] Avg prompt throughput: 7865.8 tokens/s, Avg generation throughput: 80.4 tokens/s, Running: 218 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:42:47 llm_engine.py:877] Avg prompt throughput: 8358.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 221 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:42:52 llm_engine.py:877] Avg prompt throughput: 8139.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 231 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:42:58 llm_engine.py:877] Avg prompt throughput: 8073.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 241 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:43:03 llm_engine.py:877] Avg prompt throughput: 8004.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 239 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:43:08 llm_engine.py:877] Avg prompt throughput: 7901.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 219 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:43:13 llm_engine.py:877] Avg prompt throughput: 8006.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 225 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:43:18 llm_engine.py:877] Avg prompt throughput: 8052.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:43:23 llm_engine.py:877] Avg prompt throughput: 8030.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:43:28 llm_engine.py:877] Avg prompt throughput: 6029.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:43:33 llm_engine.py:877] Avg prompt throughput: 8025.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:43:38 llm_engine.py:877] Avg prompt throughput: 8206.6 tokens/s, Avg generation throughput: 51.4 tokens/s, Running: 218 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:43:43 llm_engine.py:877] Avg prompt throughput: 8116.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 230 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:43:48 llm_engine.py:877] Avg prompt throughput: 8048.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:43:53 llm_engine.py:877] Avg prompt throughput: 8045.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 239 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:43:58 llm_engine.py:877] Avg prompt throughput: 8090.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:44:03 llm_engine.py:877] Avg prompt throughput: 7900.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:44:08 llm_engine.py:877] Avg prompt throughput: 7666.4 tokens/s, Avg generation throughput: 54.2 tokens/s, Running: 221 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:44:13 llm_engine.py:877] Avg prompt throughput: 7990.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 231 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:44:19 llm_engine.py:877] Avg prompt throughput: 8094.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 233 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:44:24 llm_engine.py:877] Avg prompt throughput: 8218.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:44:29 llm_engine.py:877] Avg prompt throughput: 8081.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:44:34 llm_engine.py:877] Avg prompt throughput: 8509.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:44:39 llm_engine.py:877] Avg prompt throughput: 7989.3 tokens/s, Avg generation throughput: 55.1 tokens/s, Running: 222 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:44:44 llm_engine.py:877] Avg prompt throughput: 7895.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 226 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:44:49 llm_engine.py:877] Avg prompt throughput: 8003.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 238 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:44:54 llm_engine.py:877] Avg prompt throughput: 8067.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:44:59 llm_engine.py:877] Avg prompt throughput: 7962.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:45:04 llm_engine.py:877] Avg prompt throughput: 7999.6 tokens/s, Avg generation throughput: 51.6 tokens/s, Running: 219 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:45:09 llm_engine.py:877] Avg prompt throughput: 7942.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 227 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:45:14 llm_engine.py:877] Avg prompt throughput: 8043.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 243 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:45:19 llm_engine.py:877] Avg prompt throughput: 7688.2 tokens/s, Avg generation throughput: 68.2 tokens/s, Running: 210 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:45:25 llm_engine.py:877] Avg prompt throughput: 7451.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 213 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 4.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:45:30 llm_engine.py:877] Avg prompt throughput: 8281.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 221 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:45:35 llm_engine.py:877] Avg prompt throughput: 8049.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 225 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:45:40 llm_engine.py:877] Avg prompt throughput: 8079.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 239 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:45:45 llm_engine.py:877] Avg prompt throughput: 8083.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:45:50 llm_engine.py:877] Avg prompt throughput: 8377.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:45:55 llm_engine.py:877] Avg prompt throughput: 7998.0 tokens/s, Avg generation throughput: 56.3 tokens/s, Running: 218 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:46:00 llm_engine.py:877] Avg prompt throughput: 8005.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 222 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:46:05 llm_engine.py:877] Avg prompt throughput: 8023.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 238 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:46:10 llm_engine.py:877] Avg prompt throughput: 7939.1 tokens/s, Avg generation throughput: 78.2 tokens/s, Running: 214 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:46:15 llm_engine.py:877] Avg prompt throughput: 8139.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 217 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:46:20 llm_engine.py:877] Avg prompt throughput: 8122.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 226 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:46:25 llm_engine.py:877] Avg prompt throughput: 8098.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 232 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:46:30 llm_engine.py:877] Avg prompt throughput: 8109.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 247 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:46:35 llm_engine.py:877] Avg prompt throughput: 7928.8 tokens/s, Avg generation throughput: 64.1 tokens/s, Running: 220 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:46:40 llm_engine.py:877] Avg prompt throughput: 8087.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 223 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:46:45 llm_engine.py:877] Avg prompt throughput: 8118.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 234 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:46:50 llm_engine.py:877] Avg prompt throughput: 8092.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 239 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:46:55 llm_engine.py:877] Avg prompt throughput: 6043.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 243 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:47:00 llm_engine.py:877] Avg prompt throughput: 7907.3 tokens/s, Avg generation throughput: 68.6 tokens/s, Running: 221 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:47:05 llm_engine.py:877] Avg prompt throughput: 8066.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 222 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:47:10 llm_engine.py:877] Avg prompt throughput: 8026.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 228 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:47:15 llm_engine.py:877] Avg prompt throughput: 8081.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 231 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:47:20 llm_engine.py:877] Avg prompt throughput: 8087.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 238 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:47:25 llm_engine.py:877] Avg prompt throughput: 7988.9 tokens/s, Avg generation throughput: 76.1 tokens/s, Running: 216 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:47:30 llm_engine.py:877] Avg prompt throughput: 7987.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 221 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:47:36 llm_engine.py:877] Avg prompt throughput: 8085.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 229 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:47:41 llm_engine.py:877] Avg prompt throughput: 8043.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 232 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:47:46 llm_engine.py:877] Avg prompt throughput: 7997.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 240 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:47:51 llm_engine.py:877] Avg prompt throughput: 7852.8 tokens/s, Avg generation throughput: 75.9 tokens/s, Running: 207 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 4.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:47:56 llm_engine.py:877] Avg prompt throughput: 8335.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:48:01 llm_engine.py:877] Avg prompt throughput: 8133.8 tokens/s, Avg generation throughput: 52.0 tokens/s, Running: 224 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:48:06 llm_engine.py:877] Avg prompt throughput: 8061.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 225 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:48:11 llm_engine.py:877] Avg prompt throughput: 7990.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 233 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:48:16 llm_engine.py:877] Avg prompt throughput: 7917.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 252 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:48:21 llm_engine.py:877] Avg prompt throughput: 7992.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:48:26 llm_engine.py:877] Avg prompt throughput: 7857.1 tokens/s, Avg generation throughput: 53.2 tokens/s, Running: 221 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:48:31 llm_engine.py:877] Avg prompt throughput: 8071.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 221 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:48:36 llm_engine.py:877] Avg prompt throughput: 7975.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 231 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:48:41 llm_engine.py:877] Avg prompt throughput: 7894.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:48:46 llm_engine.py:877] Avg prompt throughput: 7876.1 tokens/s, Avg generation throughput: 60.1 tokens/s, Running: 212 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 4.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:48:51 llm_engine.py:877] Avg prompt throughput: 8112.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 221 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:48:56 llm_engine.py:877] Avg prompt throughput: 8225.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 221 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:49:01 llm_engine.py:877] Avg prompt throughput: 8004.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 231 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:49:07 llm_engine.py:877] Avg prompt throughput: 7951.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 247 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:49:12 llm_engine.py:877] Avg prompt throughput: 7885.1 tokens/s, Avg generation throughput: 63.8 tokens/s, Running: 212 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 4.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:49:17 llm_engine.py:877] Avg prompt throughput: 8237.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 223 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:49:22 llm_engine.py:877] Avg prompt throughput: 8022.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 222 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:49:27 llm_engine.py:877] Avg prompt throughput: 8008.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:49:32 llm_engine.py:877] Avg prompt throughput: 8027.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:49:37 llm_engine.py:877] Avg prompt throughput: 7948.7 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 214 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:49:42 llm_engine.py:877] Avg prompt throughput: 8021.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 229 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:49:47 llm_engine.py:877] Avg prompt throughput: 8031.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 227 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:49:52 llm_engine.py:877] Avg prompt throughput: 8002.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:49:57 llm_engine.py:877] Avg prompt throughput: 8031.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:50:02 llm_engine.py:877] Avg prompt throughput: 7941.0 tokens/s, Avg generation throughput: 53.0 tokens/s, Running: 211 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:50:08 llm_engine.py:877] Avg prompt throughput: 6094.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 250 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:50:13 llm_engine.py:877] Avg prompt throughput: 7734.8 tokens/s, Avg generation throughput: 57.9 tokens/s, Running: 212 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:50:18 llm_engine.py:877] Avg prompt throughput: 7741.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 221 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:50:23 llm_engine.py:877] Avg prompt throughput: 8493.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 233 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:50:28 llm_engine.py:877] Avg prompt throughput: 7974.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:50:33 llm_engine.py:877] Avg prompt throughput: 7973.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:50:38 llm_engine.py:877] Avg prompt throughput: 7758.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:50:43 llm_engine.py:877] Avg prompt throughput: 8053.0 tokens/s, Avg generation throughput: 51.7 tokens/s, Running: 224 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:50:48 llm_engine.py:877] Avg prompt throughput: 8231.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 238 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:50:53 llm_engine.py:877] Avg prompt throughput: 8013.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:50:58 llm_engine.py:877] Avg prompt throughput: 7925.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:51:03 llm_engine.py:877] Avg prompt throughput: 7797.6 tokens/s, Avg generation throughput: 60.1 tokens/s, Running: 210 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:51:09 llm_engine.py:877] Avg prompt throughput: 7960.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 227 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:51:14 llm_engine.py:877] Avg prompt throughput: 7999.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 238 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:51:19 llm_engine.py:877] Avg prompt throughput: 8028.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:51:24 llm_engine.py:877] Avg prompt throughput: 8029.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 252 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:51:29 llm_engine.py:877] Avg prompt throughput: 7929.0 tokens/s, Avg generation throughput: 58.8 tokens/s, Running: 217 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:51:34 llm_engine.py:877] Avg prompt throughput: 7925.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 235 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:51:39 llm_engine.py:877] Avg prompt throughput: 8059.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 250 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:51:44 llm_engine.py:877] Avg prompt throughput: 7898.3 tokens/s, Avg generation throughput: 59.9 tokens/s, Running: 212 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 4.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:51:49 llm_engine.py:877] Avg prompt throughput: 8236.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 220 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:51:54 llm_engine.py:877] Avg prompt throughput: 8028.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 223 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:51:59 llm_engine.py:877] Avg prompt throughput: 8109.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 241 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:52:04 llm_engine.py:877] Avg prompt throughput: 7810.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:52:09 llm_engine.py:877] Avg prompt throughput: 7980.3 tokens/s, Avg generation throughput: 52.3 tokens/s, Running: 218 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:52:14 llm_engine.py:877] Avg prompt throughput: 8142.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 224 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:52:19 llm_engine.py:877] Avg prompt throughput: 8067.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 224 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:52:25 llm_engine.py:877] Avg prompt throughput: 8160.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 234 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:52:30 llm_engine.py:877] Avg prompt throughput: 8092.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 246 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:52:35 llm_engine.py:877] Avg prompt throughput: 8050.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:52:40 llm_engine.py:877] Avg prompt throughput: 7810.8 tokens/s, Avg generation throughput: 56.6 tokens/s, Running: 215 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:52:45 llm_engine.py:877] Avg prompt throughput: 8002.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 217 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:52:50 llm_engine.py:877] Avg prompt throughput: 8084.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 230 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:52:55 llm_engine.py:877] Avg prompt throughput: 8103.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 246 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:53:00 llm_engine.py:877] Avg prompt throughput: 8081.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:53:05 llm_engine.py:877] Avg prompt throughput: 8247.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:53:10 llm_engine.py:877] Avg prompt throughput: 8109.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:53:15 llm_engine.py:877] Avg prompt throughput: 8006.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:53:20 llm_engine.py:877] Avg prompt throughput: 7870.6 tokens/s, Avg generation throughput: 55.6 tokens/s, Running: 232 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:53:26 llm_engine.py:877] Avg prompt throughput: 5879.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 236 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:53:31 llm_engine.py:877] Avg prompt throughput: 7974.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 236 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:53:36 llm_engine.py:877] Avg prompt throughput: 8062.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 238 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:53:41 llm_engine.py:877] Avg prompt throughput: 8034.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 238 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:53:46 llm_engine.py:877] Avg prompt throughput: 8075.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 247 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:53:51 llm_engine.py:877] Avg prompt throughput: 7906.7 tokens/s, Avg generation throughput: 62.4 tokens/s, Running: 222 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:53:56 llm_engine.py:877] Avg prompt throughput: 8003.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 225 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:54:01 llm_engine.py:877] Avg prompt throughput: 8027.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 232 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:54:06 llm_engine.py:877] Avg prompt throughput: 8207.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 232 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:54:11 llm_engine.py:877] Avg prompt throughput: 7957.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:54:17 llm_engine.py:877] Avg prompt throughput: 7908.4 tokens/s, Avg generation throughput: 64.0 tokens/s, Running: 223 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:54:22 llm_engine.py:877] Avg prompt throughput: 7945.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 226 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:54:27 llm_engine.py:877] Avg prompt throughput: 8052.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 227 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:54:32 llm_engine.py:877] Avg prompt throughput: 8169.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 231 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:54:37 llm_engine.py:877] Avg prompt throughput: 8092.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:54:42 llm_engine.py:877] Avg prompt throughput: 7597.5 tokens/s, Avg generation throughput: 64.8 tokens/s, Running: 220 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:54:47 llm_engine.py:877] Avg prompt throughput: 8028.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 225 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:54:52 llm_engine.py:877] Avg prompt throughput: 8146.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 225 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:54:57 llm_engine.py:877] Avg prompt throughput: 8041.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 225 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:55:02 llm_engine.py:877] Avg prompt throughput: 7999.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 246 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:55:07 llm_engine.py:877] Avg prompt throughput: 7985.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:55:12 llm_engine.py:877] Avg prompt throughput: 8208.4 tokens/s, Avg generation throughput: 51.8 tokens/s, Running: 227 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:55:17 llm_engine.py:877] Avg prompt throughput: 8034.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 230 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:55:22 llm_engine.py:877] Avg prompt throughput: 8043.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 232 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:55:28 llm_engine.py:877] Avg prompt throughput: 8154.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:55:33 llm_engine.py:877] Avg prompt throughput: 7997.7 tokens/s, Avg generation throughput: 56.5 tokens/s, Running: 225 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:55:38 llm_engine.py:877] Avg prompt throughput: 7786.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 227 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:55:43 llm_engine.py:877] Avg prompt throughput: 7973.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 233 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:55:48 llm_engine.py:877] Avg prompt throughput: 8023.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 235 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:55:53 llm_engine.py:877] Avg prompt throughput: 8065.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:55:58 llm_engine.py:877] Avg prompt throughput: 7952.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:56:03 llm_engine.py:877] Avg prompt throughput: 7904.5 tokens/s, Avg generation throughput: 52.5 tokens/s, Running: 226 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:56:08 llm_engine.py:877] Avg prompt throughput: 8063.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 232 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:56:13 llm_engine.py:877] Avg prompt throughput: 8129.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 233 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:56:18 llm_engine.py:877] Avg prompt throughput: 8030.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 243 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:56:23 llm_engine.py:877] Avg prompt throughput: 7987.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 250 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:56:28 llm_engine.py:877] Avg prompt throughput: 7896.0 tokens/s, Avg generation throughput: 59.0 tokens/s, Running: 223 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:56:33 llm_engine.py:877] Avg prompt throughput: 8057.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 228 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:56:38 llm_engine.py:877] Avg prompt throughput: 8039.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 230 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:56:44 llm_engine.py:877] Avg prompt throughput: 8096.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 240 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:56:49 llm_engine.py:877] Avg prompt throughput: 5918.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:56:54 llm_engine.py:877] Avg prompt throughput: 8067.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:56:59 llm_engine.py:877] Avg prompt throughput: 8169.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:57:04 llm_engine.py:877] Avg prompt throughput: 8003.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:57:09 llm_engine.py:877] Avg prompt throughput: 8085.4 tokens/s, Avg generation throughput: 51.8 tokens/s, Running: 220 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:57:14 llm_engine.py:877] Avg prompt throughput: 8172.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 228 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:57:19 llm_engine.py:877] Avg prompt throughput: 8095.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 241 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:57:24 llm_engine.py:877] Avg prompt throughput: 8071.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 240 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:57:29 llm_engine.py:877] Avg prompt throughput: 8032.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 247 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:57:34 llm_engine.py:877] Avg prompt throughput: 8081.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:57:39 llm_engine.py:877] Avg prompt throughput: 7769.0 tokens/s, Avg generation throughput: 58.9 tokens/s, Running: 220 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:57:44 llm_engine.py:877] Avg prompt throughput: 8039.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 235 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:57:49 llm_engine.py:877] Avg prompt throughput: 7954.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 236 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:57:54 llm_engine.py:877] Avg prompt throughput: 8094.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:57:59 llm_engine.py:877] Avg prompt throughput: 8071.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:58:04 llm_engine.py:877] Avg prompt throughput: 8106.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:58:10 llm_engine.py:877] Avg prompt throughput: 7747.2 tokens/s, Avg generation throughput: 53.8 tokens/s, Running: 238 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:58:15 llm_engine.py:877] Avg prompt throughput: 8084.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 234 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:58:20 llm_engine.py:877] Avg prompt throughput: 8019.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:58:25 llm_engine.py:877] Avg prompt throughput: 7965.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 246 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:58:30 llm_engine.py:877] Avg prompt throughput: 7843.2 tokens/s, Avg generation throughput: 62.9 tokens/s, Running: 223 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:58:35 llm_engine.py:877] Avg prompt throughput: 8468.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 239 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:58:40 llm_engine.py:877] Avg prompt throughput: 7965.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:58:45 llm_engine.py:877] Avg prompt throughput: 7972.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 250 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:58:50 llm_engine.py:877] Avg prompt throughput: 7972.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 252 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:58:55 llm_engine.py:877] Avg prompt throughput: 7843.7 tokens/s, Avg generation throughput: 57.7 tokens/s, Running: 224 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:59:00 llm_engine.py:877] Avg prompt throughput: 8098.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 238 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:59:05 llm_engine.py:877] Avg prompt throughput: 8069.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:59:10 llm_engine.py:877] Avg prompt throughput: 8098.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:59:15 llm_engine.py:877] Avg prompt throughput: 8015.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:59:20 llm_engine.py:877] Avg prompt throughput: 7798.9 tokens/s, Avg generation throughput: 55.5 tokens/s, Running: 222 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:59:25 llm_engine.py:877] Avg prompt throughput: 8137.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 242 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:59:30 llm_engine.py:877] Avg prompt throughput: 7910.9 tokens/s, Avg generation throughput: 69.0 tokens/s, Running: 208 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:59:35 llm_engine.py:877] Avg prompt throughput: 8040.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 214 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:59:40 llm_engine.py:877] Avg prompt throughput: 7982.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 228 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:59:46 llm_engine.py:877] Avg prompt throughput: 7775.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 242 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:59:51 llm_engine.py:877] Avg prompt throughput: 7923.9 tokens/s, Avg generation throughput: 73.2 tokens/s, Running: 224 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 00:59:56 llm_engine.py:877] Avg prompt throughput: 7874.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 219 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:00:01 llm_engine.py:877] Avg prompt throughput: 8085.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 215 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:00:06 llm_engine.py:877] Avg prompt throughput: 8172.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 222 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:00:11 llm_engine.py:877] Avg prompt throughput: 5621.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 215 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:00:16 llm_engine.py:877] Avg prompt throughput: 8053.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 232 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:00:21 llm_engine.py:877] Avg prompt throughput: 7957.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:00:26 llm_engine.py:877] Avg prompt throughput: 8069.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:00:31 llm_engine.py:877] Avg prompt throughput: 8013.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 243 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:00:36 llm_engine.py:877] Avg prompt throughput: 8026.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:00:41 llm_engine.py:877] Avg prompt throughput: 7931.4 tokens/s, Avg generation throughput: 53.8 tokens/s, Running: 225 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:00:46 llm_engine.py:877] Avg prompt throughput: 8244.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:00:51 llm_engine.py:877] Avg prompt throughput: 8010.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:00:56 llm_engine.py:877] Avg prompt throughput: 8005.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 243 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:01:01 llm_engine.py:877] Avg prompt throughput: 8102.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:01:06 llm_engine.py:877] Avg prompt throughput: 8066.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:01:11 llm_engine.py:877] Avg prompt throughput: 8014.1 tokens/s, Avg generation throughput: 53.4 tokens/s, Running: 239 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:01:17 llm_engine.py:877] Avg prompt throughput: 8108.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 231 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:01:22 llm_engine.py:877] Avg prompt throughput: 8115.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 229 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:01:27 llm_engine.py:877] Avg prompt throughput: 7581.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:01:32 llm_engine.py:877] Avg prompt throughput: 8080.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 241 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:01:37 llm_engine.py:877] Avg prompt throughput: 7867.6 tokens/s, Avg generation throughput: 76.7 tokens/s, Running: 230 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:01:42 llm_engine.py:877] Avg prompt throughput: 8085.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 228 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:01:47 llm_engine.py:877] Avg prompt throughput: 7948.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 228 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:01:52 llm_engine.py:877] Avg prompt throughput: 7956.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:01:57 llm_engine.py:877] Avg prompt throughput: 8013.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 243 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:02:02 llm_engine.py:877] Avg prompt throughput: 7804.1 tokens/s, Avg generation throughput: 67.6 tokens/s, Running: 227 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:02:07 llm_engine.py:877] Avg prompt throughput: 8026.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 223 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:02:12 llm_engine.py:877] Avg prompt throughput: 8120.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 222 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:02:17 llm_engine.py:877] Avg prompt throughput: 8004.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 235 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:02:22 llm_engine.py:877] Avg prompt throughput: 8098.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:02:28 llm_engine.py:877] Avg prompt throughput: 8008.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:02:33 llm_engine.py:877] Avg prompt throughput: 7865.1 tokens/s, Avg generation throughput: 53.0 tokens/s, Running: 226 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:02:38 llm_engine.py:877] Avg prompt throughput: 7985.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 221 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:02:43 llm_engine.py:877] Avg prompt throughput: 8039.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 234 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:02:48 llm_engine.py:877] Avg prompt throughput: 8054.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:02:53 llm_engine.py:877] Avg prompt throughput: 8127.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:02:58 llm_engine.py:877] Avg prompt throughput: 8130.6 tokens/s, Avg generation throughput: 51.4 tokens/s, Running: 227 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:03:03 llm_engine.py:877] Avg prompt throughput: 8013.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 218 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:03:08 llm_engine.py:877] Avg prompt throughput: 8010.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 232 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:03:13 llm_engine.py:877] Avg prompt throughput: 8028.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 243 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:03:18 llm_engine.py:877] Avg prompt throughput: 7912.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:03:23 llm_engine.py:877] Avg prompt throughput: 7958.2 tokens/s, Avg generation throughput: 52.8 tokens/s, Running: 216 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:03:28 llm_engine.py:877] Avg prompt throughput: 7969.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 210 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:03:35 llm_engine.py:877] Avg prompt throughput: 5802.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 220 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:03:40 llm_engine.py:877] Avg prompt throughput: 8031.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 235 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:03:45 llm_engine.py:877] Avg prompt throughput: 8019.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:03:50 llm_engine.py:877] Avg prompt throughput: 8004.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:03:55 llm_engine.py:877] Avg prompt throughput: 7870.1 tokens/s, Avg generation throughput: 55.2 tokens/s, Running: 201 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:04:00 llm_engine.py:877] Avg prompt throughput: 8124.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:04:05 llm_engine.py:877] Avg prompt throughput: 7941.6 tokens/s, Avg generation throughput: 60.2 tokens/s, Running: 227 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:04:10 llm_engine.py:877] Avg prompt throughput: 7868.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:04:15 llm_engine.py:877] Avg prompt throughput: 8018.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 243 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:04:20 llm_engine.py:877] Avg prompt throughput: 8108.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 246 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:04:25 llm_engine.py:877] Avg prompt throughput: 7985.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 239 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:04:30 llm_engine.py:877] Avg prompt throughput: 7997.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:04:35 llm_engine.py:877] Avg prompt throughput: 8260.9 tokens/s, Avg generation throughput: 51.7 tokens/s, Running: 218 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:04:40 llm_engine.py:877] Avg prompt throughput: 8087.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 228 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:04:45 llm_engine.py:877] Avg prompt throughput: 8090.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 236 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:04:50 llm_engine.py:877] Avg prompt throughput: 8015.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 228 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:04:55 llm_engine.py:877] Avg prompt throughput: 8046.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:05:00 llm_engine.py:877] Avg prompt throughput: 8095.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:05:05 llm_engine.py:877] Avg prompt throughput: 7990.4 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 221 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:05:11 llm_engine.py:877] Avg prompt throughput: 8032.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 227 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:05:16 llm_engine.py:877] Avg prompt throughput: 8104.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 218 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:05:21 llm_engine.py:877] Avg prompt throughput: 8090.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 241 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:05:26 llm_engine.py:877] Avg prompt throughput: 8093.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:05:31 llm_engine.py:877] Avg prompt throughput: 7898.1 tokens/s, Avg generation throughput: 54.1 tokens/s, Running: 227 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:05:36 llm_engine.py:877] Avg prompt throughput: 7965.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 234 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:05:41 llm_engine.py:877] Avg prompt throughput: 8005.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 226 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:05:46 llm_engine.py:877] Avg prompt throughput: 8040.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 247 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:05:51 llm_engine.py:877] Avg prompt throughput: 7947.0 tokens/s, Avg generation throughput: 65.0 tokens/s, Running: 219 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.5%, CPU KV cache usage: 0.0%\n",
      "WARNING 02-04 01:05:54 tokenizer.py:80] No tokenizer found in /data/home/wangys/LLaMA-Factory-main/lora_weight/MoE_CT/add/Mistral/merge/Mistral|walmart-MoE-CT#Mistral|restaurant-MoE-CT, using base model tokenizer instead. (Exception: /data/home/wangys/LLaMA-Factory-main/lora_weight/MoE_CT/add/Mistral/merge/Mistral|walmart-MoE-CT#Mistral|restaurant-MoE-CT does not appear to have a file named config.json. Checkout 'https://huggingface.co//data/home/wangys/LLaMA-Factory-main/lora_weight/MoE_CT/add/Mistral/merge/Mistral|walmart-MoE-CT#Mistral|restaurant-MoE-CT/None' for available files.)\n",
      "INFO 02-04 01:05:56 llm_engine.py:877] Avg prompt throughput: 7249.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 224 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:06:01 llm_engine.py:877] Avg prompt throughput: 8048.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 230 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:06:06 llm_engine.py:877] Avg prompt throughput: 7917.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 222 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:06:11 llm_engine.py:877] Avg prompt throughput: 8049.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:06:16 llm_engine.py:877] Avg prompt throughput: 8105.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:06:21 llm_engine.py:877] Avg prompt throughput: 7935.2 tokens/s, Avg generation throughput: 53.8 tokens/s, Running: 231 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:06:26 llm_engine.py:877] Avg prompt throughput: 8095.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 233 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:06:31 llm_engine.py:877] Avg prompt throughput: 8026.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 223 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:06:36 llm_engine.py:877] Avg prompt throughput: 8102.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 242 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:06:41 llm_engine.py:877] Avg prompt throughput: 8076.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:06:47 llm_engine.py:877] Avg prompt throughput: 7971.0 tokens/s, Avg generation throughput: 57.2 tokens/s, Running: 232 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:06:52 llm_engine.py:877] Avg prompt throughput: 8162.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 240 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:06:57 llm_engine.py:877] Avg prompt throughput: 7992.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 233 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:07:02 llm_engine.py:877] Avg prompt throughput: 5710.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:07:07 llm_engine.py:877] Avg prompt throughput: 8024.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 252 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:07:12 llm_engine.py:877] Avg prompt throughput: 7962.1 tokens/s, Avg generation throughput: 56.0 tokens/s, Running: 230 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:07:17 llm_engine.py:877] Avg prompt throughput: 8075.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 242 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:07:22 llm_engine.py:877] Avg prompt throughput: 8140.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:07:27 llm_engine.py:877] Avg prompt throughput: 8079.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 250 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:07:32 llm_engine.py:877] Avg prompt throughput: 7966.9 tokens/s, Avg generation throughput: 58.7 tokens/s, Running: 230 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:07:37 llm_engine.py:877] Avg prompt throughput: 8088.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 236 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:07:42 llm_engine.py:877] Avg prompt throughput: 8023.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:07:47 llm_engine.py:877] Avg prompt throughput: 8059.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 247 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:07:52 llm_engine.py:877] Avg prompt throughput: 7917.1 tokens/s, Avg generation throughput: 62.6 tokens/s, Running: 223 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:07:57 llm_engine.py:877] Avg prompt throughput: 8051.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 240 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:08:02 llm_engine.py:877] Avg prompt throughput: 8148.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 247 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:08:07 llm_engine.py:877] Avg prompt throughput: 7988.3 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 207 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:08:12 llm_engine.py:877] Avg prompt throughput: 7725.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:08:17 llm_engine.py:877] Avg prompt throughput: 7902.1 tokens/s, Avg generation throughput: 53.8 tokens/s, Running: 231 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:08:22 llm_engine.py:877] Avg prompt throughput: 8080.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:08:28 llm_engine.py:877] Avg prompt throughput: 7973.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:08:33 llm_engine.py:877] Avg prompt throughput: 7860.6 tokens/s, Avg generation throughput: 57.2 tokens/s, Running: 202 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:08:38 llm_engine.py:877] Avg prompt throughput: 8106.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:08:43 llm_engine.py:877] Avg prompt throughput: 8298.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 241 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:08:48 llm_engine.py:877] Avg prompt throughput: 8476.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 221 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:08:53 llm_engine.py:877] Avg prompt throughput: 8626.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:08:58 llm_engine.py:877] Avg prompt throughput: 8408.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:09:03 llm_engine.py:877] Avg prompt throughput: 8301.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:09:08 llm_engine.py:877] Avg prompt throughput: 8068.7 tokens/s, Avg generation throughput: 105.3 tokens/s, Running: 250 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:09:13 llm_engine.py:877] Avg prompt throughput: 7995.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 246 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:09:18 llm_engine.py:877] Avg prompt throughput: 7830.4 tokens/s, Avg generation throughput: 70.5 tokens/s, Running: 183 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:09:23 llm_engine.py:877] Avg prompt throughput: 8117.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 225 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:09:28 llm_engine.py:877] Avg prompt throughput: 8071.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:09:34 llm_engine.py:877] Avg prompt throughput: 8089.3 tokens/s, Avg generation throughput: 65.2 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:09:39 llm_engine.py:877] Avg prompt throughput: 8519.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:09:44 llm_engine.py:877] Avg prompt throughput: 8182.5 tokens/s, Avg generation throughput: 79.5 tokens/s, Running: 219 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:09:49 llm_engine.py:877] Avg prompt throughput: 8178.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:09:54 llm_engine.py:877] Avg prompt throughput: 8283.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:09:59 llm_engine.py:877] Avg prompt throughput: 7838.7 tokens/s, Avg generation throughput: 69.9 tokens/s, Running: 233 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:10:04 llm_engine.py:877] Avg prompt throughput: 7642.6 tokens/s, Avg generation throughput: 103.6 tokens/s, Running: 223 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:10:10 llm_engine.py:877] Avg prompt throughput: 5849.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:10:15 llm_engine.py:877] Avg prompt throughput: 7911.4 tokens/s, Avg generation throughput: 59.3 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:10:20 llm_engine.py:877] Avg prompt throughput: 7619.7 tokens/s, Avg generation throughput: 181.5 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:10:25 llm_engine.py:877] Avg prompt throughput: 7900.4 tokens/s, Avg generation throughput: 70.0 tokens/s, Running: 232 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:10:30 llm_engine.py:877] Avg prompt throughput: 7995.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:10:35 llm_engine.py:877] Avg prompt throughput: 7943.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:10:40 llm_engine.py:877] Avg prompt throughput: 8050.4 tokens/s, Avg generation throughput: 59.2 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:10:45 llm_engine.py:877] Avg prompt throughput: 7434.3 tokens/s, Avg generation throughput: 156.8 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:10:50 llm_engine.py:877] Avg prompt throughput: 7686.9 tokens/s, Avg generation throughput: 61.6 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:10:55 llm_engine.py:877] Avg prompt throughput: 7958.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:11:00 llm_engine.py:877] Avg prompt throughput: 7829.5 tokens/s, Avg generation throughput: 76.4 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:11:05 llm_engine.py:877] Avg prompt throughput: 7886.8 tokens/s, Avg generation throughput: 208.1 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:11:10 llm_engine.py:877] Avg prompt throughput: 7933.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 240 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:11:15 llm_engine.py:877] Avg prompt throughput: 8022.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 247 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:11:20 llm_engine.py:877] Avg prompt throughput: 7725.9 tokens/s, Avg generation throughput: 131.4 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:11:26 llm_engine.py:877] Avg prompt throughput: 7990.5 tokens/s, Avg generation throughput: 106.8 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:11:31 llm_engine.py:877] Avg prompt throughput: 8007.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:11:36 llm_engine.py:877] Avg prompt throughput: 7656.7 tokens/s, Avg generation throughput: 121.5 tokens/s, Running: 228 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:11:41 llm_engine.py:877] Avg prompt throughput: 7989.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:11:46 llm_engine.py:877] Avg prompt throughput: 7540.1 tokens/s, Avg generation throughput: 124.5 tokens/s, Running: 209 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:11:51 llm_engine.py:877] Avg prompt throughput: 8249.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:11:56 llm_engine.py:877] Avg prompt throughput: 7974.1 tokens/s, Avg generation throughput: 53.8 tokens/s, Running: 220 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:12:01 llm_engine.py:877] Avg prompt throughput: 8043.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 240 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:12:06 llm_engine.py:877] Avg prompt throughput: 7801.7 tokens/s, Avg generation throughput: 165.9 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:12:11 llm_engine.py:877] Avg prompt throughput: 8259.7 tokens/s, Avg generation throughput: 52.1 tokens/s, Running: 234 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:12:16 llm_engine.py:877] Avg prompt throughput: 8037.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:12:21 llm_engine.py:877] Avg prompt throughput: 7545.9 tokens/s, Avg generation throughput: 143.7 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:12:26 llm_engine.py:877] Avg prompt throughput: 7938.5 tokens/s, Avg generation throughput: 67.6 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:12:32 llm_engine.py:877] Avg prompt throughput: 8017.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 238 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:12:37 llm_engine.py:877] Avg prompt throughput: 7851.9 tokens/s, Avg generation throughput: 94.1 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 6.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:12:42 llm_engine.py:877] Avg prompt throughput: 8290.9 tokens/s, Avg generation throughput: 81.8 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:12:47 llm_engine.py:877] Avg prompt throughput: 7852.4 tokens/s, Avg generation throughput: 154.7 tokens/s, Running: 238 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:12:52 llm_engine.py:877] Avg prompt throughput: 7859.2 tokens/s, Avg generation throughput: 99.8 tokens/s, Running: 243 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:12:57 llm_engine.py:877] Avg prompt throughput: 8039.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:13:02 llm_engine.py:877] Avg prompt throughput: 7734.2 tokens/s, Avg generation throughput: 118.8 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:13:07 llm_engine.py:877] Avg prompt throughput: 8025.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:13:12 llm_engine.py:877] Avg prompt throughput: 7924.8 tokens/s, Avg generation throughput: 65.9 tokens/s, Running: 226 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:13:17 llm_engine.py:877] Avg prompt throughput: 5659.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:13:22 llm_engine.py:877] Avg prompt throughput: 7866.2 tokens/s, Avg generation throughput: 93.6 tokens/s, Running: 247 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:13:27 llm_engine.py:877] Avg prompt throughput: 7850.7 tokens/s, Avg generation throughput: 64.6 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:13:32 llm_engine.py:877] Avg prompt throughput: 7967.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 231 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:13:37 llm_engine.py:877] Avg prompt throughput: 7984.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:13:42 llm_engine.py:877] Avg prompt throughput: 7906.5 tokens/s, Avg generation throughput: 53.0 tokens/s, Running: 250 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:13:48 llm_engine.py:877] Avg prompt throughput: 7818.1 tokens/s, Avg generation throughput: 59.9 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:13:53 llm_engine.py:877] Avg prompt throughput: 8262.0 tokens/s, Avg generation throughput: 51.3 tokens/s, Running: 250 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:13:58 llm_engine.py:877] Avg prompt throughput: 7881.7 tokens/s, Avg generation throughput: 61.7 tokens/s, Running: 219 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:14:03 llm_engine.py:877] Avg prompt throughput: 7952.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 246 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 6.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:14:08 llm_engine.py:877] Avg prompt throughput: 7887.7 tokens/s, Avg generation throughput: 211.2 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:14:13 llm_engine.py:877] Avg prompt throughput: 8010.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 234 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:14:18 llm_engine.py:877] Avg prompt throughput: 7581.6 tokens/s, Avg generation throughput: 213.1 tokens/s, Running: 246 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:14:23 llm_engine.py:877] Avg prompt throughput: 7984.2 tokens/s, Avg generation throughput: 69.2 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:14:28 llm_engine.py:877] Avg prompt throughput: 8267.0 tokens/s, Avg generation throughput: 52.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:14:33 llm_engine.py:877] Avg prompt throughput: 7458.3 tokens/s, Avg generation throughput: 310.3 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:14:38 llm_engine.py:877] Avg prompt throughput: 7892.5 tokens/s, Avg generation throughput: 62.4 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:14:44 llm_engine.py:877] Avg prompt throughput: 7692.5 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 246 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:14:49 llm_engine.py:877] Avg prompt throughput: 7804.0 tokens/s, Avg generation throughput: 134.6 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:14:54 llm_engine.py:877] Avg prompt throughput: 7966.9 tokens/s, Avg generation throughput: 70.6 tokens/s, Running: 225 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:14:59 llm_engine.py:877] Avg prompt throughput: 8007.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:15:04 llm_engine.py:877] Avg prompt throughput: 7789.7 tokens/s, Avg generation throughput: 123.9 tokens/s, Running: 246 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:15:09 llm_engine.py:877] Avg prompt throughput: 7882.1 tokens/s, Avg generation throughput: 66.6 tokens/s, Running: 242 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:15:14 llm_engine.py:877] Avg prompt throughput: 7873.2 tokens/s, Avg generation throughput: 75.6 tokens/s, Running: 215 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:15:19 llm_engine.py:877] Avg prompt throughput: 8044.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:15:24 llm_engine.py:877] Avg prompt throughput: 8076.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 242 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:15:29 llm_engine.py:877] Avg prompt throughput: 8061.7 tokens/s, Avg generation throughput: 95.3 tokens/s, Running: 227 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:15:34 llm_engine.py:877] Avg prompt throughput: 8038.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 246 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:15:39 llm_engine.py:877] Avg prompt throughput: 8043.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 247 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:15:44 llm_engine.py:877] Avg prompt throughput: 8112.7 tokens/s, Avg generation throughput: 74.1 tokens/s, Running: 218 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:15:49 llm_engine.py:877] Avg prompt throughput: 8430.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:15:54 llm_engine.py:877] Avg prompt throughput: 8112.9 tokens/s, Avg generation throughput: 71.0 tokens/s, Running: 242 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:16:00 llm_engine.py:877] Avg prompt throughput: 8230.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:16:05 llm_engine.py:877] Avg prompt throughput: 8154.6 tokens/s, Avg generation throughput: 155.5 tokens/s, Running: 247 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:16:10 llm_engine.py:877] Avg prompt throughput: 7999.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:16:15 llm_engine.py:877] Avg prompt throughput: 8035.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:16:20 llm_engine.py:877] Avg prompt throughput: 7992.0 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 242 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:16:25 llm_engine.py:877] Avg prompt throughput: 7936.9 tokens/s, Avg generation throughput: 85.7 tokens/s, Running: 226 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:16:30 llm_engine.py:877] Avg prompt throughput: 5428.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 241 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:16:35 llm_engine.py:877] Avg prompt throughput: 8019.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:16:40 llm_engine.py:877] Avg prompt throughput: 8285.5 tokens/s, Avg generation throughput: 51.5 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:16:45 llm_engine.py:877] Avg prompt throughput: 8003.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:16:50 llm_engine.py:877] Avg prompt throughput: 8063.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 241 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:16:55 llm_engine.py:877] Avg prompt throughput: 7810.3 tokens/s, Avg generation throughput: 81.1 tokens/s, Running: 223 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:17:00 llm_engine.py:877] Avg prompt throughput: 8234.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:17:06 llm_engine.py:877] Avg prompt throughput: 7613.5 tokens/s, Avg generation throughput: 57.2 tokens/s, Running: 247 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:17:11 llm_engine.py:877] Avg prompt throughput: 8104.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:17:16 llm_engine.py:877] Avg prompt throughput: 7952.3 tokens/s, Avg generation throughput: 54.3 tokens/s, Running: 215 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:17:21 llm_engine.py:877] Avg prompt throughput: 8164.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:17:26 llm_engine.py:877] Avg prompt throughput: 7927.0 tokens/s, Avg generation throughput: 123.7 tokens/s, Running: 243 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:17:31 llm_engine.py:877] Avg prompt throughput: 7856.9 tokens/s, Avg generation throughput: 76.9 tokens/s, Running: 241 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:17:36 llm_engine.py:877] Avg prompt throughput: 8007.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 243 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:17:41 llm_engine.py:877] Avg prompt throughput: 7539.4 tokens/s, Avg generation throughput: 304.1 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:17:46 llm_engine.py:877] Avg prompt throughput: 7857.5 tokens/s, Avg generation throughput: 55.8 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:17:51 llm_engine.py:877] Avg prompt throughput: 7733.0 tokens/s, Avg generation throughput: 247.6 tokens/s, Running: 241 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:17:56 llm_engine.py:877] Avg prompt throughput: 7663.8 tokens/s, Avg generation throughput: 248.8 tokens/s, Running: 236 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:18:01 llm_engine.py:877] Avg prompt throughput: 7985.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:18:06 llm_engine.py:877] Avg prompt throughput: 7514.8 tokens/s, Avg generation throughput: 183.9 tokens/s, Running: 238 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:18:11 llm_engine.py:877] Avg prompt throughput: 8095.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:18:16 llm_engine.py:877] Avg prompt throughput: 8146.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 247 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:18:21 llm_engine.py:877] Avg prompt throughput: 7861.7 tokens/s, Avg generation throughput: 66.5 tokens/s, Running: 229 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:18:26 llm_engine.py:877] Avg prompt throughput: 8106.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:18:31 llm_engine.py:877] Avg prompt throughput: 7922.8 tokens/s, Avg generation throughput: 70.0 tokens/s, Running: 228 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:18:36 llm_engine.py:877] Avg prompt throughput: 7912.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 234 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:18:41 llm_engine.py:877] Avg prompt throughput: 8042.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:18:47 llm_engine.py:877] Avg prompt throughput: 7661.2 tokens/s, Avg generation throughput: 62.3 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:18:52 llm_engine.py:877] Avg prompt throughput: 7868.1 tokens/s, Avg generation throughput: 64.1 tokens/s, Running: 232 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:18:57 llm_engine.py:877] Avg prompt throughput: 7885.2 tokens/s, Avg generation throughput: 130.4 tokens/s, Running: 229 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:19:02 llm_engine.py:877] Avg prompt throughput: 7960.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 236 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:19:07 llm_engine.py:877] Avg prompt throughput: 7848.2 tokens/s, Avg generation throughput: 106.8 tokens/s, Running: 215 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:19:12 llm_engine.py:877] Avg prompt throughput: 8157.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 252 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:19:17 llm_engine.py:877] Avg prompt throughput: 7336.4 tokens/s, Avg generation throughput: 221.5 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:19:22 llm_engine.py:877] Avg prompt throughput: 7862.0 tokens/s, Avg generation throughput: 60.1 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:19:27 llm_engine.py:877] Avg prompt throughput: 7996.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 239 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:19:32 llm_engine.py:877] Avg prompt throughput: 8014.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:19:37 llm_engine.py:877] Avg prompt throughput: 5371.4 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 226 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:19:42 llm_engine.py:877] Avg prompt throughput: 8245.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 223 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:19:48 llm_engine.py:877] Avg prompt throughput: 7954.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 229 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:19:53 llm_engine.py:877] Avg prompt throughput: 8020.4 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:19:58 llm_engine.py:877] Avg prompt throughput: 7719.9 tokens/s, Avg generation throughput: 107.9 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:20:03 llm_engine.py:877] Avg prompt throughput: 7864.6 tokens/s, Avg generation throughput: 74.0 tokens/s, Running: 237 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:20:08 llm_engine.py:877] Avg prompt throughput: 5790.7 tokens/s, Avg generation throughput: 199.1 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:20:13 llm_engine.py:877] Avg prompt throughput: 8040.4 tokens/s, Avg generation throughput: 56.3 tokens/s, Running: 250 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:20:18 llm_engine.py:877] Avg prompt throughput: 7969.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 252 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:20:23 llm_engine.py:877] Avg prompt throughput: 7680.1 tokens/s, Avg generation throughput: 58.0 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:20:28 llm_engine.py:877] Avg prompt throughput: 7691.2 tokens/s, Avg generation throughput: 212.6 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:20:33 llm_engine.py:877] Avg prompt throughput: 7974.5 tokens/s, Avg generation throughput: 53.5 tokens/s, Running: 243 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:20:38 llm_engine.py:877] Avg prompt throughput: 7726.4 tokens/s, Avg generation throughput: 151.4 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:20:43 llm_engine.py:877] Avg prompt throughput: 7620.4 tokens/s, Avg generation throughput: 165.7 tokens/s, Running: 250 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:20:48 llm_engine.py:877] Avg prompt throughput: 7889.6 tokens/s, Avg generation throughput: 61.0 tokens/s, Running: 238 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:20:53 llm_engine.py:877] Avg prompt throughput: 8073.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:20:58 llm_engine.py:877] Avg prompt throughput: 7882.2 tokens/s, Avg generation throughput: 69.7 tokens/s, Running: 243 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:21:03 llm_engine.py:877] Avg prompt throughput: 7801.5 tokens/s, Avg generation throughput: 143.7 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:21:08 llm_engine.py:877] Avg prompt throughput: 7844.8 tokens/s, Avg generation throughput: 69.3 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:21:14 llm_engine.py:877] Avg prompt throughput: 7911.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:21:19 llm_engine.py:877] Avg prompt throughput: 7845.9 tokens/s, Avg generation throughput: 71.7 tokens/s, Running: 252 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:21:24 llm_engine.py:877] Avg prompt throughput: 7953.8 tokens/s, Avg generation throughput: 58.5 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:21:29 llm_engine.py:877] Avg prompt throughput: 7446.7 tokens/s, Avg generation throughput: 172.6 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:21:34 llm_engine.py:877] Avg prompt throughput: 7860.0 tokens/s, Avg generation throughput: 71.9 tokens/s, Running: 218 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:21:39 llm_engine.py:877] Avg prompt throughput: 7974.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 243 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:21:44 llm_engine.py:877] Avg prompt throughput: 7879.9 tokens/s, Avg generation throughput: 76.7 tokens/s, Running: 245 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:21:49 llm_engine.py:877] Avg prompt throughput: 7753.5 tokens/s, Avg generation throughput: 136.3 tokens/s, Running: 235 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:21:54 llm_engine.py:877] Avg prompt throughput: 7756.2 tokens/s, Avg generation throughput: 206.5 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:21:59 llm_engine.py:877] Avg prompt throughput: 7876.6 tokens/s, Avg generation throughput: 59.5 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:22:04 llm_engine.py:877] Avg prompt throughput: 7860.0 tokens/s, Avg generation throughput: 156.2 tokens/s, Running: 235 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:22:09 llm_engine.py:877] Avg prompt throughput: 7760.5 tokens/s, Avg generation throughput: 205.3 tokens/s, Running: 238 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:22:14 llm_engine.py:877] Avg prompt throughput: 8010.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 250 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:22:20 llm_engine.py:877] Avg prompt throughput: 7921.2 tokens/s, Avg generation throughput: 61.2 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:22:25 llm_engine.py:877] Avg prompt throughput: 7516.1 tokens/s, Avg generation throughput: 290.3 tokens/s, Running: 239 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:22:30 llm_engine.py:877] Avg prompt throughput: 7987.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 241 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:22:35 llm_engine.py:877] Avg prompt throughput: 7952.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:22:40 llm_engine.py:877] Avg prompt throughput: 7724.5 tokens/s, Avg generation throughput: 200.2 tokens/s, Running: 242 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:22:45 llm_engine.py:877] Avg prompt throughput: 7893.0 tokens/s, Avg generation throughput: 83.4 tokens/s, Running: 235 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:22:50 llm_engine.py:877] Avg prompt throughput: 7973.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:22:55 llm_engine.py:877] Avg prompt throughput: 7883.3 tokens/s, Avg generation throughput: 54.9 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:23:00 llm_engine.py:877] Avg prompt throughput: 7798.7 tokens/s, Avg generation throughput: 104.8 tokens/s, Running: 233 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:23:05 llm_engine.py:877] Avg prompt throughput: 7852.9 tokens/s, Avg generation throughput: 114.0 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:23:11 llm_engine.py:877] Avg prompt throughput: 5506.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:23:16 llm_engine.py:877] Avg prompt throughput: 7789.0 tokens/s, Avg generation throughput: 106.1 tokens/s, Running: 226 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:23:21 llm_engine.py:877] Avg prompt throughput: 7985.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 239 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:23:26 llm_engine.py:877] Avg prompt throughput: 7950.5 tokens/s, Avg generation throughput: 95.8 tokens/s, Running: 232 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:23:31 llm_engine.py:877] Avg prompt throughput: 7996.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:23:36 llm_engine.py:877] Avg prompt throughput: 8148.9 tokens/s, Avg generation throughput: 103.2 tokens/s, Running: 249 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:23:41 llm_engine.py:877] Avg prompt throughput: 7953.4 tokens/s, Avg generation throughput: 63.1 tokens/s, Running: 242 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:23:46 llm_engine.py:877] Avg prompt throughput: 7835.7 tokens/s, Avg generation throughput: 80.0 tokens/s, Running: 223 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.9%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:23:51 llm_engine.py:877] Avg prompt throughput: 7917.7 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 236 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:23:56 llm_engine.py:877] Avg prompt throughput: 7987.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:24:02 llm_engine.py:877] Avg prompt throughput: 7655.5 tokens/s, Avg generation throughput: 167.2 tokens/s, Running: 242 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:24:07 llm_engine.py:877] Avg prompt throughput: 7962.8 tokens/s, Avg generation throughput: 81.3 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.8%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:24:12 llm_engine.py:877] Avg prompt throughput: 7926.0 tokens/s, Avg generation throughput: 116.8 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:24:17 llm_engine.py:877] Avg prompt throughput: 7639.3 tokens/s, Avg generation throughput: 167.8 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:24:22 llm_engine.py:877] Avg prompt throughput: 7594.4 tokens/s, Avg generation throughput: 193.4 tokens/s, Running: 251 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:24:27 llm_engine.py:877] Avg prompt throughput: 7693.3 tokens/s, Avg generation throughput: 117.8 tokens/s, Running: 248 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:24:32 llm_engine.py:877] Avg prompt throughput: 7417.5 tokens/s, Avg generation throughput: 190.4 tokens/s, Running: 216 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:24:37 llm_engine.py:877] Avg prompt throughput: 8111.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 254 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.7%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:24:42 llm_engine.py:877] Avg prompt throughput: 7433.8 tokens/s, Avg generation throughput: 162.0 tokens/s, Running: 232 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:24:47 llm_engine.py:877] Avg prompt throughput: 7941.0 tokens/s, Avg generation throughput: 131.0 tokens/s, Running: 211 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:24:52 llm_engine.py:877] Avg prompt throughput: 8145.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:24:57 llm_engine.py:877] Avg prompt throughput: 8307.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 250 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:25:02 llm_engine.py:877] Avg prompt throughput: 7668.9 tokens/s, Avg generation throughput: 180.2 tokens/s, Running: 242 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:25:08 llm_engine.py:877] Avg prompt throughput: 7881.9 tokens/s, Avg generation throughput: 77.2 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.4%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:25:13 llm_engine.py:877] Avg prompt throughput: 8265.0 tokens/s, Avg generation throughput: 51.9 tokens/s, Running: 240 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%\n",
      "INFO 02-04 01:25:18 llm_engine.py:877] Avg prompt throughput: 8050.9 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 228 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.8%, CPU KV cache usage: 0.0%\n",
      "4059.0049\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "def process_requests(engine: LLMEngine,\n",
    "                     test_prompts: List[Tuple[str, SamplingParams,\n",
    "                                              Optional[LoRARequest]]]):\n",
    "    \"\"\"Continuously process a list of prompts and handle the outputs.\"\"\"\n",
    "    request_id = 0\n",
    "    output_list = []\n",
    "    output_request = []\n",
    "    while test_prompts or engine.has_unfinished_requests():\n",
    "        \n",
    "        if test_prompts:\n",
    "            prompt, sampling_params, lora_request, index = test_prompts.pop(0)\n",
    "            engine.add_request(str(index),\n",
    "                               prompt,\n",
    "                               \n",
    "                               sampling_params,\n",
    "                               lora_request=lora_request)\n",
    "            # print(index)\n",
    "            request_id += 1\n",
    "        # print(request_id)\n",
    "        request_outputs: List[RequestOutput] = engine.step()\n",
    "        # output_request.extend(request_outputs)\n",
    "        for request_output in request_outputs:\n",
    "            if request_output.finished:\n",
    "                output_list.append(request_output)\n",
    "    # output_list.reverse()\n",
    "    return output_list\n",
    "lora_id_list=list(lora_path_dict.keys())\n",
    "multi_lora_call = create_multi_lora_call(MoE_list_update_top_2,lora_id_list=list(lora_path_dict.keys()))\n",
    "test_prompts_input = create_test_prompts(multi_lora_call=multi_lora_call,lora_path=lora_path_dict)\n",
    "start_time = datetime.now()\n",
    "result_all = process_requests(model, test_prompts=test_prompts_input)\n",
    "end_time = datetime.now()\n",
    "print((end_time-start_time).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "54.178117 Mixtral\n",
    "37.780711 jellyfish\n",
    "\n",
    "4100 For MoE Architecture base on Code\n",
    "4059"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(result_all[0].prompt)\n",
    "# result_all[33]\n",
    "output_ins = {}\n",
    "output_predict = {}\n",
    "for lora_id in lora_id_list:\n",
    "    # output_ins[lora_id] = [''] * int(len(result_all) / len(lora_path_dict))\n",
    "    # output_predict[lora_id] = [''] * int(len(result_all) / len(lora_path_dict))\n",
    "    output_ins[lora_id] = [''] * int(len(result_all) )\n",
    "    output_predict[lora_id] = [''] * int(len(result_all) )\n",
    "# output_lora_id = [''] * len(result_all)\n",
    "for request in result_all:\n",
    "    request_id = int(request.request_id)\n",
    "    request_ins = request.prompt.strip()\n",
    "    request_lora = request.lora_request.lora_name\n",
    "    request_output = request.outputs[0].text.strip()\n",
    "    output_ins[request_lora][request_id] = request_ins\n",
    "    output_predict[request_lora][request_id] = request_output\n",
    "    # output_lora_id[request_id] = request_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in relation extraction from wikipedia web table to knowledge graph. Given table title and column pair for Table 1, please choose the most proper type from the provided options. Return in json format.\n",
      "\n",
      "Column: col0\n",
      "\n",
      "Options:[\"OfficeHolder\", \"Congressman\", \"Senator\"]\n",
      "\n",
      "Output Format Example:\n",
      "\n",
      "{\"type\": \"\"}\n",
      "\n",
      "Table 1:\n",
      "\n",
      "{\"col0\": \"M. N. Khan\", \"col1\": \"Malik Noor Khan\", \"col2\": \"2011-12-15\", \"col3\": \"1941\"}\n",
      "{\"col0\": \"M. Marye\", \"col1\": \"Madison Ellis Marye\", \"col2\": \"2016-02-23\", \"col3\": \"1944\"}\n",
      "{\"col0\": \"J. T. Lozano\", \"col1\": \"Jorge Tadeo Lozano de Peralta y González Manrique\", \"col2\": \"1816-07-06\", \"col3\": \"1772\"}\n",
      "{\"col0\": \"F. Knox\", \"col1\": \"William Franklin Knox\", \"col2\": \"1944-04-28\", \"col3\": \"1898\"}\n",
      "{\"col0\": \"F. Church\", \"col1\": \"Frank Forrester Church III\", \"col2\": \"1984-04-07\", \"col3\": \"1943\"}\n",
      "\n",
      "Reference tables:\n",
      "\n",
      "{'Table': '{\"col0\": \"American Popular Revolutionary Alliance\", \"col1\": \"Social democracy\", \"col2\": \"J. d. Castillo\", \"col3\": \"1924\", \"col4\": \"1924-05-07\"}\\n{\"col0\": \"Democratic Party\", \"col1\": \"Liberalism\", \"col2\": \"Y. Noda\", \"col3\": \"2016\", \"col4\": \"2016-03-27\"}', 'Column': 'col2', 'type': 'OfficeHolder'}\n",
      "\n",
      "{'Table': '{\"col0\": \"P. Dillingham\", \"col1\": \"Shutesbury, Massachusetts\", \"col2\": \"W. P. Dillingham\", \"col3\": \"W. P. Dillingham\", \"col4\": \"1799-08-10\"}\\n{\"col0\": \"H. H. Crapo\", \"col1\": \"Dartmouth, Massachusetts\", \"col2\": \"W. C. Durant\", \"col3\": \"W. W. Crapo\", \"col4\": \"1804-05-24\"}', 'Column': 'col3', 'type': 'Congressman'}\n",
      "\n",
      "{'Table': '{\"col0\": \"E. L. Wingo\", \"col1\": \"Edwin H Baker Pratt\", \"col2\": \"Arkansas\", \"col3\": \"1883-04-13\", \"col4\": \"1883-04-13\"}\\n{\"col0\": \"E. R. Potter\", \"col1\": \"E. R. Potter\", \"col2\": \"Kingston, Rhode Island\", \"col3\": \"1764-11-05\", \"col4\": \"1764-11-05\"}', 'Column': 'col0', 'type': 'Senator'}\n",
      "\n",
      "{'Table': '{\"col0\": \"W. M. Whistler\", \"col1\": \"England\", \"col2\": \"J. M. Whistler\", \"col3\": \"1900-02-27\"}\\n{\"col0\": \"W. F. Taylor\", \"col1\": \"Brisbane\", \"col2\": \"E. Bell\", \"col3\": \"1927-06-29\"}', 'Column': 'col2', 'type': 'OfficeHolder'}\n",
      "\n",
      "{'Table': '{\"col0\": \"Pedro Nel Ospina Vázquez\", \"col1\": \"Medellín\", \"col2\": \"M. O. Rodríguez\", \"col3\": \"1927-07-01\"}\\n{\"col0\": \"R. S. Iqbal\", \"col1\": \"Okara District\", \"col2\": \"Ranghar\", \"col3\": \"2010-09-29\"}', 'Column': 'col0', 'type': 'OfficeHolder'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(MoE_list_update_top_2.iloc[-1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_all[12000].outputs[0].text.strip()\n",
    "# print(result_all[-500].prompt.strip())\n",
    "# print(result_all[-500].outputs[0].text.strip())\n",
    "dict_output_MoE = {}\n",
    "for result in result_all:\n",
    "    prompt = result.prompt.replace('[INST] ','').replace(' [/INST]','')\n",
    "    output = result.outputs[0].text.strip()\n",
    "    dict_output_MoE[prompt] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_example = list(dict_output_MoE.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "MoE_list_update_top_2['prediction'] = MoE_list_update_top_2['query'].map(dict_output_MoE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "cound_index = []\n",
    "for index,row in MoE_list_update_top_2.iterrows():\n",
    "    predict = row['prediction']\n",
    "    try:\n",
    "        eval(predict)\n",
    "    except:\n",
    "        cound_index.append(index)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3735"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cound_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "MoE_list_update_top_2.to_csv('Router/MoE_list_update_top_2_cross_task.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
