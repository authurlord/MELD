{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6958a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from lm_polygraph.utils.model import WhiteboxModel, BlackboxModel\n",
    "from lm_polygraph.utils.manager import estimate_uncertainty\n",
    "from lm_polygraph.estimators import MaximumTokenProbability, LexicalSimilarity, SemanticEntropy, PointwiseMutualInformation, EigValLaplacian,MeanPointwiseMutualInformation,SAR,SentenceSAR,MaximumSequenceProbability,TokenSAR\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas(ProgressBar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1761e9a2",
   "metadata": {},
   "source": [
    "## Selected low-computational estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a53d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LexicalSimilarity\n",
    "PointwiseMutualInformation\n",
    "MaximumSequenceProbability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4569f2",
   "metadata": {},
   "source": [
    "## Selected high-computational estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd08957",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAR\n",
    "SemanticEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b93fe8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.61538462 -0.30769231 -0.71428571 -0.61538462 -0.66666667 -0.92307692\n",
      " -1.         -0.61538462 -0.66666667 -0.92307692]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from lm_polygraph.estimators import LexicalSimilarity\n",
    "\n",
    "# Put your 5-10 text samples from ChatGPT generation\n",
    "# samples = [\n",
    "#     'The capital of France is Paris.',\n",
    "#     'Paris is the capital city of France.',\n",
    "#     'The capital of France is Paris.',\n",
    "#     'In France, the capital is Paris.',\n",
    "#     'The capital city of France is Paris.',\n",
    "# ]\n",
    "\n",
    "samples = [\n",
    "    \n",
    "    'Paris is the capital city of France.',\n",
    "    'The capital of France is Paris.',\n",
    "    'In France, the capital is Paris.',\n",
    "    'The capital city of France is Paris.',\n",
    "    'The capital of France is Paris.',\n",
    "]\n",
    "\n",
    "estimator = LexicalSimilarity('rougeL')\n",
    "uncertainty = estimator({\n",
    "    'blackbox_sample_texts': [samples],\n",
    "})\n",
    "print(uncertainty[1])  # -0.7047619047619047"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87717ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SentenceSAR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 32\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Put your 5-10 text samples from ChatGPT generation\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# samples = [\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#     'The capital of France is Paris.',\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     'The capital city of France is Paris.',\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# ]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m samples \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     15\u001b[0m     [\n\u001b[1;32m     16\u001b[0m     \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m ]\n\u001b[1;32m     30\u001b[0m     ]\n\u001b[0;32m---> 32\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceSAR\u001b[49m()\n\u001b[1;32m     33\u001b[0m uncertainty \u001b[38;5;241m=\u001b[39m estimator(stats)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# print(uncertainty)  # -0.7047619047619047\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# uncertainty = estimator(stats[1])\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# print(uncertainty)  # -0.7047619047619047\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SentenceSAR' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from lm_polygraph.estimators import LexicalSimilarity\n",
    "# stats = np.load('../lm-polygraph/examples/sent_sar_1726031992.7997897.npy',allow_pickle=True).item()\n",
    "# Put your 5-10 text samples from ChatGPT generation\n",
    "# samples = [\n",
    "#     'The capital of France is Paris.',\n",
    "#     'Paris is the capital city of France.',\n",
    "#     'The capital of France is Paris.',\n",
    "#     'In France, the capital is Paris.',\n",
    "#     'The capital city of France is Paris.',\n",
    "# ]\n",
    "\n",
    "samples = [\n",
    "    [\n",
    "    \n",
    "    'Paris is the capital city of France.',\n",
    "    'The capital of France is Paris.',\n",
    "    'In France, the capital is Paris.',\n",
    "    'The capital city of France is Paris.',\n",
    "    'The capital of France is Paris.',\n",
    "],\n",
    "    [\n",
    "    'The capital of France is Paris.',\n",
    "    'Paris is the capital city of France.',\n",
    "    'The capital of France is Paris.',\n",
    "    'In France, the capital is Paris.',\n",
    "    'The capital city of France is Paris.',\n",
    "]\n",
    "    ]\n",
    "\n",
    "estimator = SentenceSAR()\n",
    "uncertainty = estimator()\n",
    "# print(uncertainty)  # -0.7047619047619047\n",
    "# uncertainty = estimator(stats[1])\n",
    "# print(uncertainty)  # -0.7047619047619047"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "367f88cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.6768064, 3.7241166, 3.7855449, 3.6245348, 3.7432592, 3.8125243,\n",
       "       3.628025 , 3.6888838, 3.7558572, 3.7589967], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncertainty[1].argmin()\n",
    "np.sum(stats['sample_sentence_similarity'][0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9df6752e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Barack Obama is a renowned American politician who served as the 44th President of the United States from 2009 to 2017. He was the first African American to hold the office. Obama was born on August 4, 1961, in Honolulu, Hawaii, and raised in Hawaii and Indonesia. He attended Columbia University in New York City and later earned his law degree from Harvard Law School.\\n\\nBefore his presidency,'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "stats['sample_texts'][0][uncertainty[1].argmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca1bb4c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Deberta' from 'lm_polygraph.utils' (/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/lm_polygraph/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlm_polygraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Eccentricity\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlm_polygraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstat_calculators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SemanticMatrixCalculator\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlm_polygraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Deberta\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Put your 5-10 text samples from ChatGPT generation\u001b[39;00m\n\u001b[1;32m      5\u001b[0m samples \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe capital of France is Paris.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParis is the capital city of France.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe capital city of France is Paris.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m ]\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Deberta' from 'lm_polygraph.utils' (/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/lm_polygraph/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "from lm_polygraph.estimators import Eccentricity\n",
    "from lm_polygraph.stat_calculators import SemanticMatrixCalculator\n",
    "from lm_polygraph.utils import Deberta\n",
    "# Put your 5-10 text samples from ChatGPT generation\n",
    "samples = [\n",
    "    'The capital of France is Paris.',\n",
    "    'Paris is the capital city of France.',\n",
    "    'The capital of France is Paris.',\n",
    "    'In France, the capital is Paris.',\n",
    "    'The capital city of France is Paris.',\n",
    "]\n",
    "\n",
    "stats = {\n",
    "    'blackbox_sample_texts': [samples]\n",
    "}\n",
    "# model = deberta()\n",
    "nli_calculator = SemanticMatrixCalculator(nli_model=Deberta)\n",
    "stats.update(nli_calculator(stats, None, None, None))\n",
    "# Now stats should contain 'semantic_matrix_entail' and 'semantic_matrix_contra'\n",
    "\n",
    "estimator = Eccentricity()\n",
    "uncertainty = estimator(stats)[0]\n",
    "print(uncertainty)  # 7.886122580038351e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "585dc860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1409d7",
   "metadata": {},
   "source": [
    "## From Here We Extract Enriched Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "dc9817c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = np.load('../Transfer-ER-Blocking/enrich_data/enrich_query_UE/AG_output_Mistral-7B-Instruct-v0.2_logprob.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa07a2",
   "metadata": {},
   "source": [
    "## 解析结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a92a4326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_wrap(text):\n",
    "    # 查找第一个 '{' 和最后一个 '}' 的索引\n",
    "    start_index = text.find('{')\n",
    "    end_index = text.rfind('}')\n",
    "\n",
    "    # 检查是否有匹配的 '{' 和 '}'\n",
    "    if start_index == -1 or end_index == -1 or start_index > end_index:\n",
    "        return \"No matching braces found.\"\n",
    "\n",
    "    # 提取第一个 '{' 和最后一个 '}' 之间的内容\n",
    "    extracted_content = text[start_index + 1:end_index]\n",
    "\n",
    "    # 在头部和尾部分别补上 '{' 和 '}'\n",
    "    wrapped_content = \"{\" + extracted_content + \"}\"\n",
    "\n",
    "    return wrapped_content\n",
    "def extract_and_wrap_multi_dict(text):\n",
    "    # 查找第一个 '{' 和最后一个 '}' 的索引\n",
    "    start_index = text.rfind('\\n\\n{\"Entity 1')\n",
    "    end_index = text.rfind('}')\n",
    "\n",
    "    # 检查是否有匹配的 '{' 和 '}'\n",
    "    if start_index == -1 or end_index == -1 or start_index > end_index:\n",
    "        return \"No matching braces found.\"\n",
    "\n",
    "    # 提取第一个 '{' 和最后一个 '}' 之间的内容\n",
    "    extracted_content = text[start_index + 1:end_index]\n",
    "\n",
    "    # 在头部和尾部分别补上 '{' 和 '}'\n",
    "    wrapped_content = extracted_content + \"}\"\n",
    "\n",
    "    return wrapped_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4f7e9e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'AG'\n",
    "\n",
    "enrich_query_path = '../Transfer-ER-Blocking/enrich_data/enrich_query/%s_input.csv' % data_name\n",
    "\n",
    "\n",
    "enrich_query = pd.read_csv(enrich_query_path,index_col=0)\n",
    "enrich_query['ltable_enrich'] = ''\n",
    "enrich_query['rtable_enrich'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "b6702197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e43fd2e696242b4a183e4b81b211cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from json_repair import repair_json\n",
    "import json\n",
    "count = 0\n",
    "LLM_output = query['text']\n",
    "dict_ltable = {}\n",
    "dict_rtable = {}\n",
    "for i in tqdm(range(len(enrich_query))):\n",
    "    ltable_id = int(enrich_query.iloc[i,0])\n",
    "    rtable_id = int(enrich_query.iloc[i,1])\n",
    "    if not dict_ltable.__contains__(ltable_id):\n",
    "        dict_ltable[ltable_id] = []\n",
    "    if not dict_rtable.__contains__(rtable_id):\n",
    "        dict_rtable[rtable_id] = [] ## 初始化\n",
    "# for i in tqdm(range(len(LLM_output))):\n",
    "    text = LLM_output[i]\n",
    "    if(text.__contains__('json')): ## 有```json```,只看```json包裹内的内容\n",
    "        a = repair_json(text.split('```json')[-1].split('```')[0])\n",
    "        temp = text.split('```json')[-1].split('```')[0]\n",
    "        # print(temp)\n",
    "        # print('str 1')\n",
    "    else:\n",
    "        a = repair_json(extract_and_wrap(text)) ##匹配第一个{与最后一个}\n",
    "        # print('str 2')\n",
    "    try:\n",
    "        dict_ouput = json.loads(a)\n",
    "        enrich_query.iloc[i,-2] = str(dict_ouput['Entity 1'])\n",
    "        enrich_query.iloc[i,-1] = str(dict_ouput['Entity 2'])\n",
    "        dict_ouput['Entity 1']\n",
    "        # print('case 1')\n",
    "    except:\n",
    "        try:\n",
    "            a = repair_json(extract_and_wrap_multi_dict(text)) ## 匹配最后一个\\n\\n{\"Entity 1 与最后一个'\n",
    "            dict_ouput = json.loads(a)\n",
    "            enrich_query.iloc[i,-2] = str(dict_ouput['Entity 1'])\n",
    "            enrich_query.iloc[i,-1] = str(dict_ouput['Entity 2'])\n",
    "            # print('case 2')\n",
    "        except:\n",
    "            count += 1\n",
    "    token_list = query['token'][i]\n",
    "    logprob_list = query['logprob'][i]\n",
    "    ## 开始Search，从左到右\n",
    "    # if(isinstance(dict_ouput,dict)): ## 检查dict_ouput是不是dict，这是一种检查方法\n",
    "    if(dict_ouput.__contains__('Entity 1') and dict_ouput.__contains__('Entity 2')):\n",
    "        start_place = 0\n",
    "        for key in list(dict_ouput['Entity 1'].keys()): ## 跳过原有属性n个,n=3\n",
    "            log_prob = []\n",
    "            element = dict_ouput['Entity 1'][key]\n",
    "            if not element=='': ##不是空字符\n",
    "                encoded_element = tokenizer.encode('\"{}'.format(element),add_special_tokens=False) ## 补充元素必定以\"，也就是token:345开始，切换tokenizer要对应切换\n",
    "                encoded_token_len = len(encoded_element)\n",
    "                search_place = [token_start for token_start in find_sublist_positions(encoded_element,token_list) if token_start>=start_place] ## 搜索位置大于start_place\n",
    "                if(search_place!=[]): ## 搜索不到\n",
    "                    # print(len(search_place))\n",
    "                    start_place = search_place[0]\n",
    "                    for token_start in search_place:\n",
    "                        prob_sequence = logprob_list[token_start+1:token_start+encoded_token_len]\n",
    "                        log_prob.append(prob_sequence)\n",
    "            dict_ouput['Entity 1']['{}_logprob'.format(key)] = log_prob\n",
    "        dict_ltable[ltable_id].append(dict_ouput['Entity 1']) ## 写入attribute-level logprob\n",
    "        for key in list(dict_ouput['Entity 2'].keys()): ## 跳过原有属性n个,n=3\n",
    "            log_prob = []\n",
    "            element = dict_ouput['Entity 2'][key]\n",
    "            if not element=='': ##不是空字符\n",
    "                encoded_element = tokenizer.encode('\"{}'.format(element),add_special_tokens=False) ## 补充元素必定以\"，也就是token:345开始，切换tokenizer要对应切换\n",
    "                encoded_token_len = len(encoded_element)\n",
    "                search_place = [token_start for token_start in find_sublist_positions(encoded_element,token_list) if token_start>=start_place] ## 搜索位置大于start_place\n",
    "                if(search_place!=[]): ## 搜索不到\n",
    "                    # print(len(search_place))\n",
    "                    start_place = search_place[0]\n",
    "                    for token_start in search_place:\n",
    "                        prob_sequence = logprob_list[token_start+1:token_start+encoded_token_len]\n",
    "                        log_prob.append(prob_sequence)\n",
    "            dict_ouput['Entity 2']['{}_logprob'.format(key)] = log_prob\n",
    "        dict_rtable[rtable_id].append(dict_ouput['Entity 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a134d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[9.5234375  1.87207031 2.3671875  2.39648438 2.30859375 2.9765625\n",
      "   2.51953125 3.47460938 1.83886719 5.55859375]\n",
      "  [2.38867188 9.53125    5.37890625 6.28515625 3.89257812 5.21484375\n",
      "   7.203125   5.10546875 5.6484375  7.71875   ]\n",
      "  [2.6953125  7.796875   9.5234375  9.5390625  6.30859375 8.375\n",
      "   9.515625   9.34375    9.5        7.375     ]\n",
      "  [4.37890625 5.9609375  9.3515625  9.5390625  6.5234375  9.015625\n",
      "   9.3515625  9.3671875  8.3046875  6.4765625 ]\n",
      "  [2.80859375 4.2890625  6.90234375 7.64453125 9.5390625  7.8828125\n",
      "   7.28125    6.44140625 6.70703125 8.875     ]\n",
      "  [2.15820312 6.07421875 8.0859375  8.265625   9.03125    9.5234375\n",
      "   9.3515625  8.         8.1015625  9.46875   ]\n",
      "  [2.74804688 5.75390625 9.5390625  9.5390625  6.98828125 9.40625\n",
      "   9.53125    9.53125    9.53125    6.390625  ]\n",
      "  [3.44726562 6.546875   9.2734375  9.515625   6.01171875 8.546875\n",
      "   9.453125   9.5234375  8.8984375  9.234375  ]\n",
      "  [1.58007812 5.93359375 9.2109375  8.8984375  7.203125   9.0625\n",
      "   9.4296875  9.2890625  9.53125    8.8671875 ]\n",
      "  [4.84375    5.25390625 6.08984375 7.046875   6.9296875  8.265625\n",
      "   6.1953125  6.89453125 7.2578125  9.53125   ]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from sentence_transformers import CrossEncoder\n",
    "from transformers import AutoTokenizer\n",
    "from FlagEmbedding import FlagReranker\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "stats = np.load('/data/home/wangys/lm-polygraph/examples/sar.npy',allow_pickle=True).item()\n",
    "\n",
    "def setup_cross_encoder(device='cuda', model_path = '/data/home/wangys/sentence_transformer_model/stsb-roberta-base'):\n",
    "    cross_encoder = CrossEncoder(model_path, device=device)\n",
    "    return cross_encoder\n",
    "\n",
    "def setup_reranker(device='cuda', model_path = '/data/home/wangys/sentence_transformer_model/bge-rerank-large'):\n",
    "    reranker = FlagReranker(model_path, use_fp16=True)\n",
    "    return reranker\n",
    "\n",
    "# def calculate_sample_sentence_similarity(dependencies, cross_encoder):\n",
    "#     batch_texts = dependencies[\"sample_texts\"]\n",
    "#     batch_pairs = []\n",
    "#     batch_invs = []\n",
    "#     batch_counts = []\n",
    "    \n",
    "#     for texts in batch_texts:\n",
    "#         unique_texts, inv = np.unique(texts, return_inverse=True)\n",
    "#         batch_pairs.append(list(itertools.product(unique_texts, unique_texts)))\n",
    "#         batch_invs.append(inv)\n",
    "#         batch_counts.append(len(unique_texts))\n",
    "\n",
    "#     sim_matrices = []\n",
    "#     for i, pairs in enumerate(batch_pairs):\n",
    "#         sim_scores = cross_encoder.predict(pairs)\n",
    "#         # sim_scores = cross_encoder.compute_score(pairs)\n",
    "#         unique_mat_shape = (batch_counts[i], batch_counts[i])\n",
    "#         sim_scores_matrix = sim_scores.reshape(unique_mat_shape)\n",
    "#         inv = batch_invs[i]\n",
    "#         sim_matrices.append(sim_scores_matrix[inv, :][:, inv])\n",
    "        \n",
    "#     sim_matrices = np.stack(sim_matrices)\n",
    "#     return sim_matrices\n",
    "\n",
    "def calculate_sample_sentence_similarity_reranker(dependencies, cross_encoder):\n",
    "    batch_texts = dependencies[\"sample_texts\"]\n",
    "    batch_pairs = []\n",
    "    batch_invs = []\n",
    "    batch_counts = []\n",
    "    \n",
    "    for texts in batch_texts:\n",
    "        unique_texts, inv = np.unique(texts, return_inverse=True)\n",
    "        batch_pairs.append(list(itertools.product(unique_texts, unique_texts)))\n",
    "        batch_invs.append(inv)\n",
    "        batch_counts.append(len(unique_texts))\n",
    "\n",
    "    sim_matrices = []\n",
    "    for i, pairs in enumerate(batch_pairs):\n",
    "        # sim_scores = cross_encoder.predict(pairs)\n",
    "        sim_scores = np.array(cross_encoder.compute_score(pairs))\n",
    "        unique_mat_shape = (batch_counts[i], batch_counts[i])\n",
    "        sim_scores_matrix = sim_scores.reshape(unique_mat_shape)\n",
    "        inv = batch_invs[i]\n",
    "        sim_matrices.append(sim_scores_matrix[inv, :][:, inv])\n",
    "        \n",
    "    sim_matrices = np.stack(sim_matrices)\n",
    "    return sim_matrices\n",
    "\n",
    "# Example usage in a Jupyter Notebook Cell\n",
    "device = 'cuda:0'  # or 'cpu'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/data/home/wangys/sentence_transformer_model/stsb-roberta-base\")\n",
    "# cross_encoder = setup_cross_encoder(device=device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/data/home/wangys/sentence_transformer_model/bge-rerank-large/\")\n",
    "\n",
    "# cross_encoder = setup_reranker(device=device)\n",
    "\n",
    "# Assuming dependencies is a dictionary containing 'sample_texts' key\n",
    "# For example:\n",
    "# dependencies = {'sample_texts': [['Hello world', 'Hello world', 'Goodbye world'], ['Sample text 1', 'Sample text 2']]}\n",
    "# dependencies = {\n",
    "#     'sample_texts': [\n",
    "#         ['Hello world', 'Goodbye world'],\n",
    "#         ['Sample text 1', 'Sample text 2']\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "sample_sentence_similarity = calculate_sample_sentence_similarity_reranker(stats, cross_encoder)\n",
    "print(sample_sentence_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "e742ba5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /data/home/wangys/sentence_transformer_model/stsb-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.5596442  0.5714566  0.5639174  0.55540067 0.5599863  0.5592013\n",
      "   0.5633068  0.55737704 0.5523158  0.5403635 ]\n",
      "  [0.56727153 0.5606382  0.56034434 0.5532132  0.568062   0.5576059\n",
      "   0.5560436  0.5576421  0.5453054  0.5368302 ]\n",
      "  [0.55708975 0.5598821  0.55567086 0.5485729  0.55802745 0.55198365\n",
      "   0.55195445 0.5495054  0.5401506  0.5239375 ]\n",
      "  [0.549484   0.55294096 0.5471738  0.53400093 0.55239207 0.5420458\n",
      "   0.5456812  0.5407787  0.5280566  0.5178532 ]\n",
      "  [0.5608775  0.5643278  0.5583287  0.5541518  0.55305284 0.55056393\n",
      "   0.55016094 0.5579378  0.54632753 0.5296941 ]\n",
      "  [0.55421287 0.55401003 0.5453452  0.5433912  0.5511032  0.53545564\n",
      "   0.5367594  0.54644996 0.53402567 0.52085215]\n",
      "  [0.5609234  0.5604612  0.5529131  0.5482103  0.5540797  0.54461753\n",
      "   0.54455215 0.5512292  0.53674155 0.5267632 ]\n",
      "  [0.5486426  0.55080146 0.54625505 0.5391846  0.5501874  0.5425313\n",
      "   0.54158264 0.5394819  0.5292997  0.5160477 ]\n",
      "  [0.53878725 0.5356601  0.5285249  0.5225351  0.5416198  0.5268245\n",
      "   0.5273742  0.5262032  0.5142763  0.49939612]\n",
      "  [0.52256185 0.5216273  0.5125939  0.50491726 0.51574326 0.50987464\n",
      "   0.50916475 0.51585805 0.5017794  0.48775706]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from sentence_transformers import CrossEncoder\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def setup_cross_encoder(device='cuda',path='/data/home/wangys/sentence_transformer_model/stsb-roberta-base'):\n",
    "    cross_encoder = CrossEncoder(path, device=device)\n",
    "    return cross_encoder\n",
    "\n",
    "def calculate_similarities(dependencies, cross_encoder, tokenizer):\n",
    "    batch_input_texts = dependencies[\"input_texts\"]\n",
    "    batch_greedy_tokens = dependencies[\"greedy_tokens\"]\n",
    "    batch_sample_tokens = dependencies[\"sample_tokens\"]\n",
    "    batch_texts = dependencies[\"sample_texts\"]\n",
    "    \n",
    "    special_tokens = list(tokenizer.all_special_tokens)\n",
    "\n",
    "    # Calculate sample_sentence_similarity\n",
    "    batch_pairs = []\n",
    "    batch_invs = []\n",
    "    batch_counts = []\n",
    "    for texts in batch_texts:\n",
    "        unique_texts, inv = np.unique(texts, return_inverse=True)\n",
    "        batch_pairs.append(list(itertools.product(unique_texts, unique_texts)))\n",
    "        batch_invs.append(inv)\n",
    "        batch_counts.append(len(unique_texts))\n",
    "\n",
    "    sim_matrices = []\n",
    "    for i, pairs in enumerate(batch_pairs):\n",
    "        sim_scores = cross_encoder.predict(pairs)\n",
    "        unique_mat_shape = (batch_counts[i], batch_counts[i])\n",
    "        sim_scores_matrix = sim_scores.reshape(unique_mat_shape)\n",
    "        inv = batch_invs[i]\n",
    "        sim_matrices.append(sim_scores_matrix[inv, :][:, inv])\n",
    "    sim_matrices = np.stack(sim_matrices)\n",
    "\n",
    "    # Calculate token_similarity\n",
    "    batch_token_scores = []\n",
    "    for input_texts, tokens in zip(batch_input_texts, batch_greedy_tokens):\n",
    "        if len(tokens) > 1:\n",
    "            is_special_tokens = np.isin(tokens, special_tokens)\n",
    "            cropped_tokens = list(itertools.combinations(tokens, len(tokens) - 1))[::-1]\n",
    "            raw_text = input_texts + \" \" + tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "            batches = [\n",
    "                (raw_text, input_texts + \" \" + tokenizer.decode(list(t), skip_special_tokens=True))\n",
    "                for t in cropped_tokens\n",
    "            ]\n",
    "            token_scores = cross_encoder.predict(batches)\n",
    "            token_scores[is_special_tokens] = 1\n",
    "        else:\n",
    "            token_scores = np.array([0.5] * len(tokens))\n",
    "        batch_token_scores.append(token_scores)\n",
    "\n",
    "    # Calculate sample_token_similarity\n",
    "    batch_samples_token_scores = []\n",
    "    for sample_tokens, input_texts in zip(batch_sample_tokens, batch_input_texts):\n",
    "        samples_token_scores = []\n",
    "        for tokens in sample_tokens:\n",
    "            if len(tokens) > 1:\n",
    "                is_special_tokens = np.isin(tokens, special_tokens)\n",
    "                cropped_tokens = list(itertools.combinations(tokens, len(tokens) - 1))[::-1]\n",
    "                raw_text = input_texts + \" \" + tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "                batches = [\n",
    "                    (raw_text, input_texts + \" \" + tokenizer.decode(list(t), skip_special_tokens=True))\n",
    "                    for t in cropped_tokens\n",
    "                ]\n",
    "                token_scores = cross_encoder.predict(batches)\n",
    "                token_scores[is_special_tokens] = 1\n",
    "            else:\n",
    "                token_scores = np.array([0.5] * len(tokens))\n",
    "            samples_token_scores.append(token_scores)\n",
    "        batch_samples_token_scores.append(samples_token_scores)\n",
    "\n",
    "    return {\n",
    "        \"sample_sentence_similarity\": sim_matrices,\n",
    "        \"sample_token_similarity\": batch_samples_token_scores,\n",
    "        \"token_similarity\": batch_token_scores,\n",
    "    }\n",
    "\n",
    "# Example usage in a Jupyter Notebook Cell\n",
    "device = 'cuda'  # or 'cpu'\n",
    "path = '/data/home/wangys/sentence_transformer_model/stsb-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "cross_encoder = setup_cross_encoder(device=device,path=path)\n",
    "\n",
    "# Assuming dependencies is a dictionary containing 'input_texts', 'greedy_tokens', 'sample_tokens', and 'sample_texts'\n",
    "# dependencies = {\n",
    "#     'input_texts': ['Hello world', 'Another example text'],\n",
    "#     'greedy_tokens': [[101, 7592, 2088, 102], [101, 2178, 1859, 3793, 102]],  # Token IDs\n",
    "#     'sample_tokens': [[[101, 7592, 2088, 102], [101, 7592, 102]], [[101, 2178, 1859, 3793, 102]]],\n",
    "#     'sample_texts': [['Hello world', 'Hi world'], ['Another example text']],\n",
    "# }\n",
    "\n",
    "results = calculate_similarities(stats, cross_encoder, tokenizer)\n",
    "print(results['sample_sentence_similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "759f3b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../Transfer-ER-Blocking/enrich_data/enrich_dict/dict_ltable_AG_logprob.npy',dict_ltable)\n",
    "np.save('../Transfer-ER-Blocking/enrich_data/enrich_dict/dict_rtable_AG_logprob.npy',dict_rtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "3d4512aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Enrich Entity 1 and Entity 2 with attributes: category/subcategory/platform/edition/type/modelno. Return in json format.\\n\\nOutput Format Example:\\n\\n{\"Entity 1\": {\"title\": \"\", \"manufacturer\": \"\", \"price\": \"\", \"category\": \"\", \"subcategory\": \"\", \"platform\": \"\", \"edition\": \"\", \"type\": \"\", \"modelno\": \"\"}, \"Entity 2\": {\"title\": \"\", \"manufacturer\": \"\", \"price\": \"\", \"category\": \"\", \"subcategory\": \"\", \"platform\": \"\", \"edition\": \"\", \"type\": \"\", \"modelno\": \"\"}}\\n\\nEntity 1:{\"title\": \"microsoft visio standard 2007 version upgrade\", \"manufacturer\": \"microsoft\", \"price\": 129.95}\\n\\nEntity 2:{\"title\": \"adobe cs3 design standard upgrade\", \"manufacturer\": \"\", \"price\": 413.99}'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.read_csv('../Transfer-ER-Blocking/enrich_data/enrich_query/AG_input.csv',index_col=0)\n",
    "a.iloc[0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8d8c3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('../model/Mistral-7B-Instruct-v0.2/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257a642c",
   "metadata": {},
   "source": [
    "[345,  8684,  1264,   345] caterogy\n",
    "[345,  1666,  8684,  1264,   345] sub-caterogy\n",
    "[345, 10470,  1264,   345] platform\n",
    "[345,   286,   685,  1264,   345] edition\n",
    "[345,  3549,  1510,  1264,   345] modelno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d2165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sublist_positions(sublist, mainlist):\n",
    "    positions = []\n",
    "    len_sublist = len(sublist)\n",
    "    # 遍历可能的起始位置\n",
    "    for i in range(len(mainlist) - len_sublist + 1):\n",
    "        # 判断从i开始的子列表是否与sublist相等\n",
    "        if mainlist[i:i + len_sublist] == sublist:\n",
    "            positions.append(i)\n",
    "    return positions\n",
    "category = [345,  8684,  1264,   345]\n",
    "sub_cat = [345,  1666,  8684,  1264,   345]\n",
    "platform = [345, 10470,  1264,   345]\n",
    "edition = [345,   286,   685,  1264,   345]\n",
    "modelno = [345,  3549,  1510,  1264,   345]\n",
    "for i in range(500):\n",
    "    # print(find_sublist_positions([345,8684],query['token'][i]))\n",
    "    sublist_all = [category,sub_cat,platform]\n",
    "    for sublist in sublist_all:\n",
    "        result = find_sublist_positions(sublist,query['token'][i])\n",
    "        if(len(result)!=2):\n",
    "            print(i,tokenizer.decode(sublist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "52175392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def find_concat_positions_optimized(lst, input_str):\n",
    "def find_concat_positions_optimized(lst, input_str):\n",
    "    positions = []\n",
    "    n = len(input_str)\n",
    "    len_lst = len(lst)\n",
    "    # 创建一个集合，包含input_str中的所有字符\n",
    "    input_str_chars = set(input_str)\n",
    "    \n",
    "    # 遍历列表中的每个可能的起始点\n",
    "    for i in range(len_lst):\n",
    "        # 检查当前元素是否为空，以及首字符是否在input_str的字符集中\n",
    "        if lst[i] and lst[i][0] in input_str_chars:\n",
    "            # 拼接从i开始的足够多的元素以达到input_str的长度\n",
    "            concatenated_str = ''.join(lst[i:i+n])\n",
    "            # 如果拼接后的字符串与input_str相等，记录起始位置\n",
    "            if concatenated_str == input_str:\n",
    "                positions.append(i)\n",
    "    \n",
    "    return positions\n",
    "find_concat_positions_optimized(query['decoded_token'][0][110:150],'Software')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e7233468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# def find_concat_positions_flexible(lst, input_str):\n",
    "#     positions = []\n",
    "#     n = len(input_str)\n",
    "#     len_lst = len(lst)\n",
    "    \n",
    "#     # 遍历列表中的每个可能的起始点\n",
    "#     for i in range(len_lst):\n",
    "#         concatenated_str = ''  # 初始化拼接字符串\n",
    "#         for j in range(i, len_lst):\n",
    "#             concatenated_str += lst[j].strip()  # 拼接时去除两端空格\n",
    "#             # 检查当前拼接字符串是否与input_str匹配\n",
    "#             if concatenated_str == input_str:\n",
    "#                 positions.append((i, j))  # 记录起始和结束索引\n",
    "#                 break  # 匹配成功后不需要继续拼接\n",
    "#             elif len(concatenated_str) > n:\n",
    "#                 break  # 如果拼接长度已经超过input_str的长度，不需要继续拼接\n",
    "\n",
    "#     return positions\n",
    "\n",
    "def find_concat_positions_flexible(lst, input_str):\n",
    "    positions = []\n",
    "    len_lst = len(lst)\n",
    "    \n",
    "    # 遍历列表中的每个可能的起始点\n",
    "    for i in range(len_lst):\n",
    "        concatenated_str = ''  # 初始化拼接字符串\n",
    "        concat_positions = []  # 用于记录参与拼接的元素索引\n",
    "        for j in range(i, len_lst):\n",
    "            # 将当前元素加入拼接，并去除两端的空格\n",
    "            current_piece = lst[j].strip()\n",
    "            if current_piece:  # 只有非空的元素才参与拼接\n",
    "                concatenated_str += current_piece\n",
    "                concat_positions.append(j)\n",
    "                # 检查当前拼接字符串是否与input_str匹配\n",
    "                if concatenated_str == input_str:\n",
    "                    positions.append((concat_positions[0], concat_positions[-1]))  # 记录起始和结束索引\n",
    "                    break  # 匹配成功后不需要继续拼接\n",
    "                elif len(concatenated_str) > len(input_str):\n",
    "                    break  # 如果拼接长度已经超过input_str的长度，不需要继续拼接\n",
    "\n",
    "    return positions\n",
    "\n",
    "# 示例\n",
    "lst = [',', '\\n', '   ', ' \"', 'category', '\":', ' \"', 'Soft', 'ware', '\",', '\\n', '   ', ' \"', 'sub', 'category', '\":', ' \"', 'Product', 'ivity', ' Software', '\",', '\\n', '   ', ' \"', 'platform', '\":', ' \"', 'Windows', '\",', '\\n', '   ', ' \"', 'ed', 'ition', '\":', ' \"', 'Standard', '\",', '\\n', '   ']\n",
    "input_str = 'Productivity Software'\n",
    "positions = find_concat_positions_flexible(query['decoded_token'][0], input_str)\n",
    "print(positions)  # 输出应该是 [(7, 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "4479aa4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[345, 6158, 28770, 1336, 20506, 3571, 28777]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('\"CS3DESIGNUPG',add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c145d636",
   "metadata": {},
   "source": [
    "encode需要加单边\"+内容不加另一边的\"，如'\"CS3DESIGNUPG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "e7a54674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[275]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_sublist_positions([345, 6158, 28770, 1336, 20506, 3571, 28777],query['token'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "048a8595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([345, 5075, 28705, 28740], [345, 5075, 28705, 28750])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query['text'][423]\n",
    "tokenizer.encode('\"Entity 1',add_special_tokens=False),tokenizer.encode('\"Entity 2',add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "05e5ac25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[63]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = dict_ouput['Entity 1']['platform']\n",
    "find_sublist_positions(tokenizer.encode('\"{}'.format(search),add_special_tokens=False),query['token'][0])\n",
    "find_sublist_positions([345, 5075, 28705, 28740],query['token'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e3271d",
   "metadata": {},
   "source": [
    "## 从左到右搜索\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "563dade9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5075, 28705, 28740]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query['token'][4][1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "f70e6a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 345, 5075, 28705, 28740]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.decode([5075, 28705, 28740])\n",
    "tokenizer.encode('\"Entity 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "45fda11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[233]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query['text'][259]\n",
    "token_example = '\"Education'\n",
    "find_sublist_positions(tokenizer.encode(token_example,add_special_tokens=False),query['token'][259])\n",
    "# tokenizer.decode(query['token'][4][1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "a9854303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 [51, 195]\n",
      "19 [54, 275, 293]\n",
      "21 [52, 158]\n",
      "34 [54, 259]\n",
      "41 [55, 186]\n",
      "43 [52, 210]\n",
      "64 [52, 179]\n",
      "66 [51, 209]\n",
      "73 [53, 197]\n",
      "90 [52, 173]\n",
      "92 [53, 186]\n",
      "94 [52, 212]\n",
      "96 [53, 168]\n",
      "124 [52, 203]\n",
      "128 [53, 189]\n",
      "142 [53, 168]\n",
      "147 [30, 240]\n",
      "176 [51, 174]\n",
      "178 [35, 310]\n",
      "193 [52, 160]\n",
      "201 [53, 210]\n",
      "209 [25, 75]\n",
      "217 [55, 206]\n",
      "224 [35, 160]\n",
      "225 [52, 143]\n",
      "240 [52, 172]\n",
      "244 [52, 193]\n",
      "248 [53, 156]\n",
      "259 [25, 76]\n",
      "270 [51, 174]\n",
      "283 [51, 179]\n",
      "308 [53, 170]\n",
      "324 [25, 78]\n",
      "334 [53, 194]\n",
      "336 [53, 189]\n",
      "339 [52, 173]\n",
      "340 [87, 316]\n",
      "347 [53, 193]\n",
      "349 [35, 137]\n",
      "357 [52, 122]\n",
      "362 [52, 184]\n",
      "367 [52, 144]\n",
      "373 [53, 202]\n",
      "375 [32, 150]\n",
      "398 [54, 196]\n",
      "404 [54, 255]\n",
      "406 [53, 186]\n",
      "415 [52, 180]\n",
      "417 [55, 196]\n",
      "450 [53, 171]\n",
      "456 [52, 186]\n",
      "470 [60, 291, 309]\n",
      "475 [52, 175]\n",
      "480 [53, 188]\n",
      "493 [52, 199]\n"
     ]
    }
   ],
   "source": [
    "entity_1 = tokenizer.encode('\"Entity 1',add_special_tokens=False) ## Entity 1指针开始位置\n",
    "# entity_1_whole = tokenizer.encode('Entity 1\":',add_special_tokens=False) ## Entity 1指针开始位置\n",
    "entity_2 = tokenizer.encode('\"Entity 2',add_special_tokens=False) ## Entity 2指针开始位置\n",
    "for i in range(500):\n",
    "    # if find_sublist_positions(entity_1,query['token'][i])==[] and find_sublist_positions(entity_1[1:],query['token'][i])==[]: ## 找不到\"Entity 1,可能因为左\"被忽略了，去掉token:345\n",
    "    #     print(i)\n",
    "    # if find_sublist_positions(entity_2,query['token'][i])==[] and find_sublist_positions(entity_2[1:],query['token'][i])==[]: ## 找不到\"Entity 1：\n",
    "    #     print(i)\n",
    "    result = find_sublist_positions(entity_1[1:],query['token'][i])\n",
    "    if(len(result)!=1):\n",
    "        print(i,result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "33c7f043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def find_concat_positions_flexible(lst, input_str):\n",
    "    positions = []\n",
    "    len_lst = len(lst)\n",
    "    \n",
    "    # 遍历列表中的每个可能的起始点\n",
    "    for i in range(len_lst):\n",
    "        concatenated_str = ''  # 初始化拼接字符串\n",
    "        concat_positions = []  # 用于记录参与拼接的元素索引\n",
    "        for j in range(i, len_lst):\n",
    "            current_piece = lst[j].strip()  # 去除当前字符串两端空格\n",
    "            if current_piece:  # 只处理非空字符串\n",
    "                # 如果拼接后的字符串仍然不超过目标字符串，才继续拼接\n",
    "                if len(concatenated_str + current_piece) <= len(input_str):\n",
    "                    concatenated_str += current_piece\n",
    "                    concat_positions.append(j)\n",
    "                    # 完全匹配后返回结果\n",
    "                    if concatenated_str == input_str:\n",
    "                        positions.append((concat_positions[0], concat_positions[-1]))\n",
    "                        break\n",
    "                else:\n",
    "                    break  # 超出字符串长度，中断内层循环\n",
    "\n",
    "    return positions\n",
    "\n",
    "# 示例\n",
    "lst = [',', '\\n', '   ', ' \"', 'category', '\":', ' \"', 'Soft', 'ware', '\",', '\\n', '   ', ' \"', 'sub', 'category', '\":', ' \"', 'Product', 'ivity', ' Software', '\",', '\\n', '   ', ' \"', 'platform', '\":', ' \"', 'Windows', '\",', '\\n', '   ', ' \"', 'ed', 'ition', '\":', ' \"', 'Standard', '\",', '\\n', '   ']\n",
    "input_str = 'Productivity Software'\n",
    "positions = find_concat_positions_flexible(query['decoded_token'][0], input_str)\n",
    "print(positions)  # 希望返回 [(16, 18)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0f9392e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' To enrich Entity 1 and Entity 2 with the required attributes, we first need to obtain the necessary information. I\\'ll assume we have access to a database or API to fetch this data. Here\\'s an example of how the output could look like:\\n\\n```json\\n{\\n  \"Entity 1\": {\\n    \"title\": \"Microsoft Visio Standard 2007 Version Upgrade\",\\n    \"manufacturer\": \"Microsoft\",\\n    \"price\": 129.95,\\n    \"category\": \"Software\",\\n    \"subcategory\": \"Productivity Software\",\\n    \"platform\": \"Windows\",\\n    \"edition\": \"Standard\",\\n    \"type\": \"Upgrade\",\\n    \"modelno\": \"VISSTAND2007\"\\n  },\\n  \"Entity 2\": {\\n    \"title\": \"Adobe CS3 Design Standard Upgrade\",\\n    \"manufacturer\": \"Adobe\",\\n    \"price\": 413.99,\\n    \"category\": \"Software\",\\n    \"subcategory\": \"Design Software\",\\n    \"platform\": \"Mac, Windows\",\\n    \"edition\": \"Standard\",\\n    \"type\": \"Upgrade\",\\n    \"modelno\": \"CS3DESIGNUPG\"\\n  }\\n}\\n```\\n\\nKeep in mind that the provided information is just an example, and the actual values may vary depending on the specific entities and their attributes.'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query['token'][0][120:].index(8684)\n",
    "# query['decoded_token'][0][239:250]\n",
    "query['text'][0]\n",
    "# ''.join(query['decoded_token'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "94d0f495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positions of ' ['soft'] ' are:  []\n"
     ]
    }
   ],
   "source": [
    "def find_full_indices(decoded_token_list, target):\n",
    "    # 将目标字符串拼接成单个字符串以便匹配\n",
    "    target_concat = ''.join(target)\n",
    "    n = len(decoded_token_list)\n",
    "    for i in range(n):\n",
    "        # 尝试从每个位置开始匹配整个目标字符串\n",
    "        concat_str = ''\n",
    "        for j in range(i, n):\n",
    "            concat_str += decoded_token_list[j]\n",
    "            if concat_str == target_concat:\n",
    "                return (i, j)  # 返回开始和结束索引\n",
    "    return (-1, -1)  # 如果没有找到，返回(-1, -1)\n",
    "\n",
    "def find_positions(decoded_token_list, A, B, C, input_word):\n",
    "    # 确定A, B, C的完整范围\n",
    "    index_A_start, index_A_end = find_full_indices(decoded_token_list, A)\n",
    "    index_B_start, index_B_end = find_full_indices(decoded_token_list, B)\n",
    "    index_C_start, index_C_end = find_full_indices(decoded_token_list, C)\n",
    "    \n",
    "    # 如果找不到A, B, 或C\n",
    "    if -1 in [index_A_start, index_B_start, index_C_start]:\n",
    "        return []\n",
    "    \n",
    "    # 在B与C之间查找input_word\n",
    "    positions = []\n",
    "    input_word_concat = ''.join(input_word)\n",
    "    concat_text = ''.join(decoded_token_list[index_B_end + 1:index_C_start + 1])\n",
    "    start = 0\n",
    "    while True:\n",
    "        found_pos = concat_text.find(input_word_concat, start)\n",
    "        if found_pos == -1:\n",
    "            break\n",
    "        positions.append(found_pos + index_B_end + 1)  # 转换为decoded_token_list中的位置\n",
    "        start = found_pos + len(input_word_concat)\n",
    "    \n",
    "    return positions\n",
    "\n",
    "# 示例数据\n",
    "decoded_token_list = [\"soft\", \"ware\", \"engineer\", \"develops\", \"soft\", \"ware\"]\n",
    "A = [\"soft\", \"ware\"]\n",
    "B = [\"develops\"]\n",
    "C = [\"soft\", \"ware\"]\n",
    "input_word = [\"soft\"]\n",
    "\n",
    "# 调用函数\n",
    "positions = find_positions(decoded_token_list, A, B, C, input_word)\n",
    "print(\"Positions of '\", input_word, \"' are: \", positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2e13d4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([20465, 28705, 28740], [20465, 28705, 28750], [1, 8011], [1083, 8684], [6583])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Entity 1')[1:],tokenizer.encode('Entity 2')[1:],tokenizer.encode('category'),tokenizer.encode('subcategory')[1:],tokenizer.encode('Software')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "92402d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_matching_indexes(tokens, A_0, B, C, input_list):\n",
    "    # 找到A_0的最后一个元素在tokens中的位置\n",
    "    try:\n",
    "        start_index = tokens.index(A_0[-1]) + 1\n",
    "    except ValueError:\n",
    "        # 如果A_0的最后一个元素不在tokens中，返回空列表\n",
    "        return []\n",
    "\n",
    "    # 初始化结果列表\n",
    "    result_indexes = []\n",
    "\n",
    "    # 初始化搜索区间的起始和结束位置\n",
    "    start_search = start_index\n",
    "    end_search = len(tokens)\n",
    "\n",
    "    # 遍历tokens以查找B和C\n",
    "    while start_search < end_search:\n",
    "        try:\n",
    "            # 在剩余tokens中寻找B\n",
    "            start_b = tokens.index(B, start_search)\n",
    "            # 在B之后寻找C\n",
    "            end_c = tokens.index(C, start_b + len(B))\n",
    "        except ValueError:\n",
    "            # 如果找不到B或C，跳出循环\n",
    "            break\n",
    "        \n",
    "        # 检查B和C之间是否有足够的空间匹配input_list\n",
    "        if end_c - start_b >= len(input_list):\n",
    "            # 遍历B和C之间的区域，寻找input_list\n",
    "            for i in range(start_b + len(B), end_c - len(input_list) + 1):\n",
    "                # 检查当前位置开始的序列是否与input_list匹配\n",
    "                if tokens[i:i + len(input_list)] == input_list:\n",
    "                    result_indexes.append(i)\n",
    "\n",
    "        # 更新start_search为当前end_c之后，继续搜索下一个可能的位置\n",
    "        start_search = end_c + len(C)\n",
    "\n",
    "    return result_indexes\n",
    "find_matching_indexes(query['token'][0],[20465, 28705, 28740],[8684], [1666, 8684],[6583])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b2a5c6",
   "metadata": {},
   "source": [
    "'sample_log_likelihoods'的sum是'sample_log_probs'\n",
    "'logprobs'的sum是'cumulative_logprob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa19a693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-18.096956720289306, -18.096956720289306)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(sum(stats['sample_log_likelihoods'][0],[]))\n",
    "sum(stats['sample_log_likelihoods'][0][0]),stats['sample_log_probs'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fbc8981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50103766])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "def calculate_sentence_SAR(stats: Dict[str, np.ndarray], t: float = 0.001) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to estimate the sequence-level uncertainty of a language model following the method of\n",
    "    \"Sentence SAR\" as described in the paper https://arxiv.org/abs/2307.01379.\n",
    "    This method calculates the sum of the probability of the generated text and text relevance relative to all other generations.\n",
    "\n",
    "    Parameters:\n",
    "        stats (Dict[str, np.ndarray]): input statistics, which for multiple samples includes:\n",
    "            * corresponding log probabilities in 'sample_log_probs',\n",
    "            * matrix with cross-encoder similarities in 'sample_sentence_similarity'\n",
    "        t (float): A small threshold value to control the influence of relevance in uncertainty calculation.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: float sentenceSAR for each sample in input statistics.\n",
    "            Higher values indicate more uncertain samples.\n",
    "    \"\"\"\n",
    "    batch_sample_log_probs = stats[\"sample_log_probs\"]\n",
    "    batch_sample_sentence_similarity = stats[\"sample_sentence_similarity\"]\n",
    "\n",
    "    sentenceSAR = []\n",
    "    for sample_log_probs, sample_sentence_similarity in zip(\n",
    "        batch_sample_log_probs, batch_sample_sentence_similarity\n",
    "    ):\n",
    "        sample_probs = np.exp(np.array(sample_log_probs))  # Convert log probabilities to regular probabilities\n",
    "        R_s = (\n",
    "            sample_probs\n",
    "            * sample_sentence_similarity\n",
    "            * (1 - np.eye(sample_sentence_similarity.shape[0]))  # Exclude self similarity\n",
    "        )\n",
    "        sent_relevance = R_s.sum(-1) / t\n",
    "        E_s = -np.log(sent_relevance + sample_probs)\n",
    "        sentenceSAR.append(E_s.mean())\n",
    "\n",
    "    return np.array(sentenceSAR)\n",
    "calculate_sentence_SAR(stats=result)\n",
    "# Example usage:\n",
    "# Assume 'stats' is a dictionary containing 'sample_log_probs' and 'sample_sentence_similarity'\n",
    "# stats = {\n",
    "#     \"sample_log_probs\": np.array([...]),\n",
    "#     \"sample_sentence_similarity\": np.array([...])\n",
    "# }\n",
    "# result = calculate_sentence_SAR(stats)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4b424d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_texts', 'target_texts', 'target_tokens', 'input_tokens', 'greedy_log_probs', 'greedy_tokens', 'greedy_tokens_alternatives', 'greedy_texts', 'greedy_log_likelihoods', 'embeddings_decoder', 'sample_log_likelihoods', 'sample_log_probs', 'sample_tokens', 'sample_texts', 'sample_sentence_similarity', 'sample_token_similarity', 'token_similarity'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2b5ec87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.35613904])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = np.load('/data/home/wangys/lm-polygraph/examples/sar.npy',allow_pickle=True).item()\n",
    "batch_sample_log_likelihoods = stats[\"sample_log_likelihoods\"]\n",
    "batch_sample_token_similarity = stats[\"sample_token_similarity\"]\n",
    "batch_sample_sentence_similarity = stats[\"sample_sentence_similarity\"]\n",
    "t = 0.001\n",
    "SAR = []\n",
    "for batch_data in zip(\n",
    "    batch_sample_log_likelihoods,\n",
    "    batch_sample_token_similarity,\n",
    "    batch_sample_sentence_similarity,\n",
    "):\n",
    "    sample_log_likelihoods = batch_data[0]\n",
    "    sample_token_similarity = batch_data[1]\n",
    "    sample_sentence_similarity = batch_data[2]\n",
    "\n",
    "    tokenSAR = []\n",
    "    for log_likelihoods, token_similarity in zip(\n",
    "        sample_log_likelihoods, sample_token_similarity\n",
    "    ):\n",
    "        log_likelihoods = np.array(log_likelihoods)\n",
    "        R_t = 1 - token_similarity\n",
    "        R_t_norm = R_t / R_t.sum()\n",
    "        E_t = -log_likelihoods * R_t_norm\n",
    "        tokenSAR.append(E_t.sum())\n",
    "\n",
    "    tokenSAR = np.array(tokenSAR)\n",
    "    probs_token_sar = np.exp(-tokenSAR)\n",
    "    R_s = (\n",
    "        probs_token_sar\n",
    "        * sample_sentence_similarity\n",
    "        * (1 - np.eye(sample_sentence_similarity.shape[0]))\n",
    "    )\n",
    "    sent_relevance = R_s.sum(-1) / t\n",
    "    E_s = -np.log(sent_relevance + probs_token_sar)\n",
    "    SAR.append(E_s.mean())\n",
    "np.array(SAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3dc0e6-804f-490e-9b77-4f5b3cb0ad64",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e7a7afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c37eb897344f2481f4f3b763e7e027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    '/data/home/wangys/model/Mistral-7B-Instruct-v0.2',\n",
    "    device_map='cuda:7',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained('/data/home/wangys/model/Mistral-7B-Instruct-v0.2', token_type_ids=None,\n",
    "                clean_up_tokenization_spaces=False)\n",
    "\n",
    "model = WhiteboxModel(base_model,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad18648a-b1c7-4089-832e-84e17be8b203",
   "metadata": {},
   "source": [
    "### Token level UE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "030b389b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/home/wangys/model/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /data/home/wangys/sentence_transformer_model/stsb-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(uncertainty=-8.356139042348568, input_text='Who is George Bush?', generation_text=\"George Bush may refer to one of two U.S. presidents named George Bush. Here's a brief overview of each:\\n\\n1. George H.W. Bush (b. 1924): He was the 41st President of the United States, serving from 1989 to 1993. Bush was a naval aviator in World War II and later served as a member of Congress from Texas. He also served as the\", generation_tokens=[5163, 13668, 993, 3295, 298, 624, 302, 989, 500, 28723, 28735, 28723, 1258, 6640, 5160, 5163, 13668, 28723, 4003, 28742, 28713, 264, 6817, 23094, 302, 1430, 28747, 13, 13, 28740, 28723, 5163, 382, 28723, 28780, 28723, 13668, 325, 28726, 28723, 28705, 28740, 28774, 28750, 28781, 1329, 650, 403, 272, 28705, 28781, 28740, 303, 5120, 302, 272, 2969, 3543, 28725, 10732, 477, 28705, 28740, 28774, 28783, 28774, 298, 28705, 28740, 28774, 28774, 28770, 28723, 13668, 403, 264, 23850, 1182, 28710, 1028, 297, 3304, 3273, 3717, 304, 2062, 6117, 390, 264, 4292, 302, 8463, 477, 7826, 28723, 650, 835, 6117, 390], model_path=None, estimator='SAR')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = SAR()\n",
    "estimate_uncertainty(model, estimator, input_text='Who is George Bush?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "247f5d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/home/wangys/model/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(uncertainty=13.410937309265137, input_text='Who is George Bush?', generation_text=\"George Bush may refer to one of two U.S. presidents named George Bush. Here's a brief overview of each:\\n\\n1. George H.W. Bush (b. 1924): He was the 41st President of the United States, serving from 1989 to 1993. Bush was a naval aviator in World War II and later served as a member of Congress from Texas. He also served as the\", generation_tokens=[5163, 13668, 993, 3295, 298, 624, 302, 989, 500, 28723, 28735, 28723, 1258, 6640, 5160, 5163, 13668, 28723, 4003, 28742, 28713, 264, 6817, 23094, 302, 1430, 28747, 13, 13, 28740, 28723, 5163, 382, 28723, 28780, 28723, 13668, 325, 28726, 28723, 28705, 28740, 28774, 28750, 28781, 1329, 650, 403, 272, 28705, 28781, 28740, 303, 5120, 302, 272, 2969, 3543, 28725, 10732, 477, 28705, 28740, 28774, 28783, 28774, 298, 28705, 28740, 28774, 28774, 28770, 28723, 13668, 403, 264, 23850, 1182, 28710, 1028, 297, 3304, 3273, 3717, 304, 2062, 6117, 390, 264, 4292, 302, 8463, 477, 7826, 28723, 650, 835, 6117, 390], model_path=None, estimator='MaximumSequenceProbability')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = MaximumSequenceProbability()\n",
    "estimate_uncertainty(model, estimator, input_text='Who is George Bush?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4043671a-939f-421b-b06b-24abf557fdc9",
   "metadata": {},
   "source": [
    "### Sequence level UE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c2e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = SAR()\n",
    "estimate_uncertainty(model, estimator, input_text='Who is George Bush?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dd6ed2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/home/wangys/model/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(uncertainty=-17.852901331445374, input_text='Who is George Bush?', generation_text=\"George Bush may refer to one of two U.S. presidents named George Bush. Here's a brief overview of each:\\n\\n1. George H.W. Bush (b. 1924): He was the 41st President of the United States, serving from 1989 to 1993. Bush was a naval aviator in World War II and later served as a member of Congress from Texas. He also served as the\", generation_tokens=[5163, 13668, 993, 3295, 298, 624, 302, 989, 500, 28723, 28735, 28723, 1258, 6640, 5160, 5163, 13668, 28723, 4003, 28742, 28713, 264, 6817, 23094, 302, 1430, 28747, 13, 13, 28740, 28723, 5163, 382, 28723, 28780, 28723, 13668, 325, 28726, 28723, 28705, 28740, 28774, 28750, 28781, 1329, 650, 403, 272, 28705, 28781, 28740, 303, 5120, 302, 272, 2969, 3543, 28725, 10732, 477, 28705, 28740, 28774, 28783, 28774, 298, 28705, 28740, 28774, 28774, 28770, 28723, 13668, 403, 264, 23850, 1182, 28710, 1028, 297, 3304, 3273, 3717, 304, 2062, 6117, 390, 264, 4292, 302, 8463, 477, 7826, 28723, 650, 835, 6117, 390], model_path=None, estimator='MeanPointwiseMutualInformation')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = MeanPointwiseMutualInformation()\n",
    "estimate_uncertainty(model, estimator, input_text='Who is George Bush?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8492d549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[False False False False False False False False False False]\n",
      "  [False False False False False False False False False False]\n",
      "  [False False False False False False False False False False]\n",
      "  [False False False False False False False False False False]\n",
      "  [False False False False False False False False False False]\n",
      "  [False False False False False False False False False False]\n",
      "  [False False False False False False False False False False]\n",
      "  [False False False False False False False False False False]\n",
      "  [False False False False False False False False False False]\n",
      "  [False False False False False False False False False False]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([23.7672994])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Optional\n",
    "stats = np.load('/data/home/wangys/lm-polygraph/examples/semantic_entropy.npy',allow_pickle=True).item()\n",
    "def semantic_entropy(stats: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Estimates the semantic entropy for each sample in the input statistics.\n",
    "\n",
    "    Parameters:\n",
    "        stats (Dict[str, np.ndarray]): input statistics, which for multiple samples includes:\n",
    "            * generated samples in 'sample_texts',\n",
    "            * corresponding log probabilities in 'sample_log_probs',\n",
    "            * matrix with semantic similarities in 'semantic_matrix_entail'\n",
    "    Returns:\n",
    "        np.ndarray: float semantic entropy for each sample in input statistics.\n",
    "            Higher values indicate more uncertain samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    loglikelihoods_list = stats[\"sample_log_probs\"]\n",
    "    entailment_id = stats.get(\"entailment_id\", 1)\n",
    "    is_entailment = stats[\"semantic_matrix_entail\"] == entailment_id\n",
    "    print(is_entailment)\n",
    "\n",
    "    # Concatenating hypotheses with input texts\n",
    "    hyps_list = [[' '.join([input_text, hyp]) for hyp in stats[\"sample_texts\"][i]] for i, input_text in enumerate(stats[\"input_texts\"])]\n",
    "    \n",
    "    return batched_call(hyps_list, loglikelihoods_list, is_entailment)\n",
    "\n",
    "def batched_call(hyps_list: List[List[str]], loglikelihoods_list: List[List[float]], is_entailment: np.ndarray, log_weights: Optional[List[List[float]]] = None) -> np.array:\n",
    "    if log_weights is None:\n",
    "        log_weights = [None] * len(hyps_list)\n",
    "    \n",
    "    semantic_logits = {}\n",
    "    sample_to_class = {}\n",
    "    class_to_sample = defaultdict(list)\n",
    "\n",
    "    # Determine classes for hypotheses\n",
    "    for idx, hyps in enumerate(hyps_list):\n",
    "        sample_to_class[idx], class_to_sample[idx] = determine_classes(hyps, is_entailment[idx])\n",
    "\n",
    "        # Collect likelihoods per class\n",
    "        class_likelihoods = [np.array(loglikelihoods_list[idx])[np.array(class_idx)] for class_idx in class_to_sample[idx]]\n",
    "        class_lp = [np.logaddexp.reduce(likelihoods) for likelihoods in class_likelihoods]\n",
    "        \n",
    "        # Apply weights if provided\n",
    "        if log_weights[idx] is None:\n",
    "            log_weights[idx] = [0] * len(hyps)\n",
    "        \n",
    "        semantic_logits[idx] = -np.mean([class_lp[sample_to_class[idx][j]] * np.exp(log_weights[idx][j]) for j in range(len(hyps))])\n",
    "    \n",
    "    return np.array([semantic_logits[i] for i in range(len(hyps_list))])\n",
    "\n",
    "def determine_classes(hyps: List[str], is_entailment: np.ndarray) -> (Dict[int, int], Dict[int, List[int]]):\n",
    "    sample_to_class = {}\n",
    "    class_to_sample = defaultdict(list)\n",
    "\n",
    "    for i in range(len(hyps)):\n",
    "        if i == 0:\n",
    "            class_to_sample[0] = [0]\n",
    "            sample_to_class[0] = 0\n",
    "            continue\n",
    "        \n",
    "        for class_id, class_indices in class_to_sample.items():\n",
    "            class_text_id = class_indices[0]\n",
    "            if is_entailment[class_text_id, i] and is_entailment[i, class_text_id]:\n",
    "                class_to_sample[class_id].append(i)\n",
    "                sample_to_class[i] = class_id\n",
    "                break\n",
    "        else:\n",
    "            new_class_id = len(class_to_sample)\n",
    "            class_to_sample[new_class_id] = [i]\n",
    "            sample_to_class[i] = new_class_id\n",
    "    \n",
    "    return sample_to_class, class_to_sample\n",
    "\n",
    "# Example usage:\n",
    "# stats = {\n",
    "#     'sample_log_probs': np.array([[0.1, 0.2], [0.4, 0.5]]),\n",
    "#     'sample_texts': [['text1', 'text2'], ['text3', 'text4']],\n",
    "#     'input_texts': ['input1', 'input2'],\n",
    "#     'semantic_matrix_entail': np.array([[[1, 0], [0, 1]], [[1, 1], [1, 1]]]),\n",
    "#     'entailment_id': 1\n",
    "# }\n",
    "output = semantic_entropy(stats)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "182e43a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.99211544, 0.08175895, 0.4267827 , 0.80408055, 0.7590612 ,\n",
       "         0.24637915, 0.09461544, 0.12501796, 0.97024375, 0.8976873 ],\n",
       "        [0.74437773, 0.98674357, 0.11483058, 0.6593689 , 0.80901915,\n",
       "         0.15827736, 0.04327307, 0.12684904, 0.57116574, 0.8476537 ],\n",
       "        [0.3639897 , 0.01338555, 0.9920814 , 0.820661  , 0.14517976,\n",
       "         0.01991121, 0.01533491, 0.01187618, 0.41261953, 0.17829414],\n",
       "        [0.72689205, 0.1687846 , 0.37953278, 0.98945194, 0.93539363,\n",
       "         0.6241864 , 0.20936836, 0.29448467, 0.96973497, 0.9535518 ],\n",
       "        [0.67073053, 0.08700771, 0.00710694, 0.47592905, 0.99120855,\n",
       "         0.02243401, 0.07285031, 0.12850712, 0.19683176, 0.22344266],\n",
       "        [0.8139983 , 0.07905055, 0.04403748, 0.92957145, 0.5597203 ,\n",
       "         0.9840152 , 0.07504941, 0.11253431, 0.893689  , 0.31243303],\n",
       "        [0.35354203, 0.01060121, 0.00976979, 0.12878047, 0.6267275 ,\n",
       "         0.02977836, 0.9878634 , 0.6174651 , 0.04306053, 0.13417588],\n",
       "        [0.7687419 , 0.22557533, 0.00999194, 0.26547658, 0.4551683 ,\n",
       "         0.02739965, 0.87459517, 0.9909991 , 0.6738292 , 0.4181714 ],\n",
       "        [0.89923555, 0.03463945, 0.54197806, 0.8824918 , 0.36562064,\n",
       "         0.12789813, 0.04224234, 0.1323228 , 0.98833686, 0.5171668 ],\n",
       "        [0.838964  , 0.0928909 , 0.20372458, 0.81516707, 0.6540775 ,\n",
       "         0.02424784, 0.10503291, 0.1128617 , 0.42384624, 0.98891455]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats[\"semantic_matrix_entail\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8292b97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/home/wangys/model/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(uncertainty=-0.5354888443376555, input_text='Who is George Bush?', generation_text=\"George Bush may refer to one of two U.S. presidents named George Bush. Here's a brief overview of each:\\n\\n1. George H.W. Bush (b. 1924): He was the 41st President of the United States, serving from 1989 to 1993. Bush was a naval aviator in World War II and later served as a member of Congress from Texas. He also served as the\", generation_tokens=[5163, 13668, 993, 3295, 298, 624, 302, 989, 500, 28723, 28735, 28723, 1258, 6640, 5160, 5163, 13668, 28723, 4003, 28742, 28713, 264, 6817, 23094, 302, 1430, 28747, 13, 13, 28740, 28723, 5163, 382, 28723, 28780, 28723, 13668, 325, 28726, 28723, 28705, 28740, 28774, 28750, 28781, 1329, 650, 403, 272, 28705, 28781, 28740, 303, 5120, 302, 272, 2969, 3543, 28725, 10732, 477, 28705, 28740, 28774, 28783, 28774, 298, 28705, 28740, 28774, 28774, 28770, 28723, 13668, 403, 264, 23850, 1182, 28710, 1028, 297, 3304, 3273, 3717, 304, 2062, 6117, 390, 264, 4292, 302, 8463, 477, 7826, 28723, 650, 835, 6117, 390], model_path=None, estimator='LexicalSimilarity_rougeL')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = LexicalSimilarity('rougeL')\n",
    "estimate_uncertainty(model, estimator, input_text='Who is George Bush?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46725acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_texts', 'target_texts', 'target_tokens', 'input_tokens', 'greedy_log_probs', 'greedy_tokens', 'greedy_tokens_alternatives', 'greedy_texts', 'greedy_log_likelihoods', 'embeddings_decoder', 'sample_log_likelihoods', 'sample_log_probs', 'sample_tokens', 'sample_texts', 'sample_sentence_similarity', 'sample_token_similarity', 'token_similarity'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "result = np.load('/data/home/wangys/lm-polygraph/examples/sent_sar_1726031992.7997897.npy',allow_pickle=True).item()\n",
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73669d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Barack Obama is a politician who served as the 44th President of the United States from January 20, 2009, to January 20, 2017. He was the first African American to hold the office. Obama was born on August 4, 1961, in Honolulu, Hawaii, and raised in Hawaii and Indonesia. He attended Columbia University and later earned his law degree from Harvard Law School.\\n\\nBefore']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['greedy_texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83f08bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Barack Obama was the 44th President of the United States, serving from January 20, 2009, to January 20, 2017. He was the first African American to hold the office. Obama was born on August 4, 1961, in Honolulu, Hawaii, and raised in Hawaii and Indonesia. He attended Columbia University in New York City and earned his law degree from Harvard Law School.\\n\\nBefore'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['sample_texts'][0][-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44365569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-15.41031588, -15.78380363,  -8.50700831, -16.47936298,\n",
       "        -8.88130191, -12.57747051, -11.61657   ,  -6.4267327 ,\n",
       "       -18.7933634 , -30.27003519])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(result['sample_log_likelihoods'][0]).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a906db0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/home/wangys/model/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /data/home/wangys/sentence_transformer_model/stsb-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(uncertainty=0.5010376612319298, input_text='Who is Barack Obama?', generation_text='Barack Obama is a politician who served as the 44th President of the United States from January 20, 2009, to January 20, 2017. He was the first African American to hold the office. Obama was born on August 4, 1961, in Honolulu, Hawaii, and raised in Hawaii and Indonesia. He attended Columbia University and later earned his law degree from Harvard Law School.\\n\\nBefore', generation_tokens=[3011, 468, 11764, 349, 264, 17587, 693, 6117, 390, 272, 28705, 28781, 28781, 362, 5120, 302, 272, 2969, 3543, 477, 4624, 28705, 28750, 28734, 28725, 28705, 28750, 28734, 28734, 28774, 28725, 298, 4624, 28705, 28750, 28734, 28725, 28705, 28750, 28734, 28740, 28787, 28723, 650, 403, 272, 907, 8623, 2556, 298, 2400, 272, 4007, 28723, 11764, 403, 5381, 356, 3628, 28705, 28781, 28725, 28705, 28740, 28774, 28784, 28740, 28725, 297, 7673, 328, 20366, 28725, 26434, 28725, 304, 6333, 297, 26434, 304, 24450, 28723, 650, 12359, 14402, 2900, 304, 2062, 12839, 516, 2309, 6153, 477, 17553, 5802, 4228, 28723, 13, 13], model_path=None, estimator='SentenceSAR')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = SentenceSAR()\n",
    "estimate_uncertainty(model, estimator, input_text='Who is Barack Obama?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f3c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PointwiseMutualInformation()\n",
    "estimate_uncertainty(model, estimator, input_text='Once upon a time there was a little girl who liked to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc03fd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/home/wangys/model/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(uncertainty=-0.5423855887497974, input_text='Who is George Bush?', generation_text=\"George Bush may refer to one of two U.S. presidents named George Bush. Here's a brief overview of each:\\n\\n1. George H.W. Bush (b. 1924): He was the 41st President of the United States, serving from 1989 to 1993. Bush was a naval aviator in World War II and later served as a member of Congress from Texas. He also served as the\", generation_tokens=[5163, 13668, 993, 3295, 298, 624, 302, 989, 500, 28723, 28735, 28723, 1258, 6640, 5160, 5163, 13668, 28723, 4003, 28742, 28713, 264, 6817, 23094, 302, 1430, 28747, 13, 13, 28740, 28723, 5163, 382, 28723, 28780, 28723, 13668, 325, 28726, 28723, 28705, 28740, 28774, 28750, 28781, 1329, 650, 403, 272, 28705, 28781, 28740, 303, 5120, 302, 272, 2969, 3543, 28725, 10732, 477, 28705, 28740, 28774, 28783, 28774, 298, 28705, 28740, 28774, 28774, 28770, 28723, 13668, 403, 264, 23850, 1182, 28710, 1028, 297, 3304, 3273, 3717, 304, 2062, 6117, 390, 264, 4292, 302, 8463, 477, 7826, 28723, 650, 835, 6117, 390], model_path=None, estimator='LexicalSimilarity_rougeL')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ue_method = LexicalSimilarity()\n",
    "input_text = \"Who is George Bush?\"\n",
    "estimate_uncertainty(model, ue_method, input_text=input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7940d-9f83-4872-a0e3-d2e9a83d8a9e",
   "metadata": {},
   "source": [
    "### BlackBox UE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb84386",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BlackboxModel(\n",
    "    'YOUR_OPENAI_TOKEN',\n",
    "    'gpt-3.5-turbo'\n",
    ")\n",
    "estimator = EigValLaplacian(verbose=True)\n",
    "estimate_uncertainty(model, estimator, input_text='When did Albert Einstein die?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b63635",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_TOKEN = 'YOUR_API_TOKEN'\n",
    "# for example let's take google/t5-small-ssm-nq model\n",
    "MODEL_ID = 'google/t5-large-ssm-nqo'\n",
    "\n",
    "model = BlackboxModel.from_huggingface(hf_api_token=API_TOKEN, hf_model_id=MODEL_ID, openai_api_key = None, openai_model_path = None)\n",
    "ue_method = LexicalSimilarity()\n",
    "input_text = \"Who is George Bush?\"\n",
    "estimate_uncertainty(model, ue_method, input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec1991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example let's take bigscience/bloomz-560m model\n",
    "MODEL_ID = 'bigscience/bloomz-560m'\n",
    "\n",
    "model = BlackboxModel.from_huggingface(hf_api_token=API_TOKEN, hf_model_id=MODEL_ID, openai_api_key = None, openai_model_path = None)\n",
    "ue_method = LexicalSimilarity()\n",
    "input_text = \"Who is George Bush?\"\n",
    "estimate_uncertainty(model, ue_method, input_text=input_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
