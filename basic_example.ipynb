{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6958a441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from lm_polygraph.utils.model import WhiteboxModel, BlackboxModel\n",
    "from lm_polygraph.utils.manager import estimate_uncertainty\n",
    "from lm_polygraph.estimators import MaximumTokenProbability, LexicalSimilarity, SemanticEntropy, PointwiseMutualInformation, EigValLaplacian,MeanPointwiseMutualInformation,SAR,MaximumSequenceProbability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1761e9a2",
   "metadata": {},
   "source": [
    "## Selected low-computational estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a53d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LexicalSimilarity\n",
    "PointwiseMutualInformation\n",
    "MaximumSequenceProbability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4569f2",
   "metadata": {},
   "source": [
    "## Selected high-computational estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd08957",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAR\n",
    "SemanticEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "585dc860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1d246014",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = np.load('/data/home/wangys/lm-polygraph/examples/sar.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7f3ecf12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57795465, 0.5783484 , 0.578637  , 0.57867306, 0.5782978 ,\n",
       "        0.57811266, 0.5780282 , 0.5785949 , 0.5791121 , 0.5779701 ,\n",
       "        0.578808  , 0.58061755, 0.5733088 , 0.57541555, 0.57939076,\n",
       "        0.5785188 , 0.5784118 , 0.57817084, 0.5782121 , 0.5781876 ,\n",
       "        0.5775309 , 0.5781161 , 0.57702315, 0.56754553, 0.566034  ,\n",
       "        0.57412   , 0.5653516 , 0.5790489 , 0.5779917 , 0.5778659 ,\n",
       "        0.5677034 , 0.567616  , 0.567616  , 0.573672  , 0.5773175 ,\n",
       "        0.57708836, 0.5776825 , 0.57818127, 0.57827777, 0.5773349 ,\n",
       "        0.5781683 , 0.5753414 , 0.577256  , 0.57631296, 0.5708441 ,\n",
       "        0.5604796 , 0.5604796 , 0.5707252 , 0.5795488 , 0.57802165,\n",
       "        0.58168286, 0.567329  , 0.567329  , 0.567329  , 0.5805734 ,\n",
       "        0.5782185 , 0.578472  , 0.5796611 , 0.57767975, 0.57721233,\n",
       "        0.57695186, 0.57838976, 0.57776374, 0.5784785 , 0.5743262 ,\n",
       "        0.5824831 , 0.5731482 , 0.57099926, 0.5775634 , 0.5770057 ,\n",
       "        0.5699354 , 0.57480556, 0.58115304, 0.58080906, 0.57833683,\n",
       "        0.5771808 , 0.57494867, 0.5766933 , 0.577239  , 0.5788752 ,\n",
       "        0.5733676 , 0.5777733 , 0.5750386 , 0.5768497 , 0.5763777 ,\n",
       "        0.57748365, 0.57745063, 0.5766599 , 0.5768871 , 0.577467  ,\n",
       "        0.57775104, 0.5776062 , 0.57737875, 0.5770733 , 0.5769791 ,\n",
       "        0.577561  , 0.5775792 , 0.5775799 , 0.5769968 , 0.58000535],\n",
       "       [0.5231204 , 0.5224222 , 0.52159405, 0.5220148 , 0.52225316,\n",
       "        0.52205265, 0.5233459 , 0.5225892 , 0.5229503 , 0.52294034,\n",
       "        0.5233978 , 0.523309  , 0.5234866 , 0.5228519 , 0.5241666 ,\n",
       "        0.52372295, 0.5234838 , 0.5230865 , 0.5236742 , 0.5230509 ,\n",
       "        0.5238211 , 0.5226771 , 0.5234683 , 0.52408797, 0.52521265,\n",
       "        0.52482635, 0.52291244, 0.5239034 , 0.52370644, 0.5233758 ,\n",
       "        0.5233758 , 0.5220646 , 0.5231297 , 0.52327496, 0.5224855 ,\n",
       "        0.5223305 , 0.52348787, 0.52322453, 0.5229837 , 0.5237495 ,\n",
       "        0.52569985, 0.5225744 , 0.5223633 , 0.5191709 , 0.52892953,\n",
       "        0.5235801 , 0.52254945, 0.52433145, 0.52287275, 0.523913  ,\n",
       "        0.523276  , 0.5227082 , 0.5236956 , 0.5260645 , 0.52133876,\n",
       "        0.51947874, 0.5256742 , 0.5226556 , 0.522921  , 0.52263016,\n",
       "        0.52256596, 0.5222331 , 0.5228685 , 0.5224344 , 0.51877224,\n",
       "        0.518881  , 0.5237095 , 0.5206049 , 0.5236102 , 0.5247337 ,\n",
       "        0.51965994, 0.5218334 , 0.5218334 , 0.5224518 , 0.52356714,\n",
       "        0.5245755 , 0.5225987 , 0.5234087 , 0.5231124 , 0.52250946,\n",
       "        0.5228599 , 0.52281797, 0.52293754, 0.5224185 , 0.5223935 ,\n",
       "        0.52245796, 0.52113855, 0.5235674 , 0.5202342 , 0.5192943 ,\n",
       "        0.52300215, 0.5224953 , 0.5228063 , 0.5224704 , 0.52362716,\n",
       "        0.52388465, 0.5168076 , 0.5197529 , 0.52028143, 0.5244205 ],\n",
       "       [0.56323355, 0.5618354 , 0.5602922 , 0.56077856, 0.5633599 ,\n",
       "        0.5626395 , 0.5633987 , 0.5630238 , 0.56236154, 0.5622268 ,\n",
       "        0.56378436, 0.56253767, 0.56243116, 0.5637918 , 0.5630916 ,\n",
       "        0.5636676 , 0.56082755, 0.56368375, 0.56366795, 0.5621762 ,\n",
       "        0.5623296 , 0.56406873, 0.5627826 , 0.56221646, 0.5623655 ,\n",
       "        0.5642323 , 0.5630204 , 0.56382483, 0.56291145, 0.5629574 ,\n",
       "        0.56309366, 0.56223065, 0.5637516 , 0.56622815, 0.55929637,\n",
       "        0.5562992 , 0.56738406, 0.562947  , 0.5633621 , 0.5628744 ,\n",
       "        0.5628887 , 0.56285805, 0.5625361 , 0.5626809 , 0.5568909 ,\n",
       "        0.5540873 , 0.5606296 , 0.55713177, 0.5653279 , 0.5651854 ,\n",
       "        0.5557731 , 0.55443376, 0.55443376, 0.5607874 , 0.5644544 ,\n",
       "        0.5638498 , 0.56265116, 0.56346685, 0.5632818 , 0.5632855 ,\n",
       "        0.5633832 , 0.56349295, 0.5645169 , 0.563609  , 0.563065  ,\n",
       "        0.55888414, 0.5649696 , 0.55736476, 0.5595338 , 0.5635859 ,\n",
       "        0.5628516 , 0.56250495, 0.5679495 , 0.5553212 , 0.5553212 ,\n",
       "        0.5634887 , 0.56271595, 0.56326014, 0.5629523 , 0.5629168 ,\n",
       "        0.56297046, 0.56285775, 0.56301516, 0.5628134 , 0.56384647,\n",
       "        0.5636066 , 0.5649708 , 0.5582309 , 0.5567293 , 0.564117  ,\n",
       "        0.56273264, 0.5572325 , 0.5546745 , 0.55849   , 0.55525315,\n",
       "        0.5636418 , 0.56198615, 0.56198615, 0.56241614, 0.56319803],\n",
       "       [0.5312541 , 0.5310588 , 0.5296883 , 0.52983457, 0.5318092 ,\n",
       "        0.5309526 , 0.53169626, 0.5302975 , 0.53074497, 0.5302862 ,\n",
       "        0.5320437 , 0.5311008 , 0.532297  , 0.5304548 , 0.53183216,\n",
       "        0.53425133, 0.5301059 , 0.5308241 , 0.5301288 , 0.5324697 ,\n",
       "        0.5324632 , 0.5320116 , 0.5315122 , 0.53073925, 0.5309842 ,\n",
       "        0.5309842 , 0.5301114 , 0.53145844, 0.5308687 , 0.53002816,\n",
       "        0.53113914, 0.53198   , 0.5309802 , 0.5304841 , 0.5315736 ,\n",
       "        0.533386  , 0.53137547, 0.52841324, 0.5255766 , 0.53281176,\n",
       "        0.5236542 , 0.52234703, 0.5322946 , 0.53104466, 0.5320642 ,\n",
       "        0.5289258 , 0.53255147, 0.53147095, 0.5312531 , 0.5302248 ,\n",
       "        0.52991605, 0.53046274, 0.5306017 , 0.5309164 , 0.53154373,\n",
       "        0.53207684, 0.5272562 , 0.528284  , 0.5329701 , 0.5307498 ,\n",
       "        0.53062886, 0.53043   , 0.53012896, 0.5299953 , 0.53042966,\n",
       "        0.529663  , 0.5259767 , 0.5264451 , 0.5303429 , 0.52768236,\n",
       "        0.5314323 , 0.5312431 , 0.5264138 , 0.5269283 , 0.5269283 ,\n",
       "        0.5275481 , 0.5322977 , 0.53072554, 0.5304754 , 0.5301948 ,\n",
       "        0.5297196 , 0.531412  , 0.5302148 , 0.5305913 , 0.53088516,\n",
       "        0.5298541 , 0.53058255, 0.5319574 , 0.5297869 , 0.5301898 ,\n",
       "        0.5312621 , 0.53026056, 0.53075343, 0.5316121 , 0.5295827 ,\n",
       "        0.5302924 , 0.531386  , 0.5291329 , 0.5301355 , 0.5303892 ],\n",
       "       [0.532644  , 0.5307532 , 0.5320861 , 0.5321091 , 0.5318509 ,\n",
       "        0.5319953 , 0.5323918 , 0.5303317 , 0.53517   , 0.53196764,\n",
       "        0.5325667 , 0.53222895, 0.53199846, 0.53193545, 0.5324113 ,\n",
       "        0.53215176, 0.5328778 , 0.53229064, 0.5322005 , 0.53228354,\n",
       "        0.53256845, 0.53339857, 0.5338098 , 0.5322788 , 0.5327422 ,\n",
       "        0.53166425, 0.5339139 , 0.5336856 , 0.5321853 , 0.53298306,\n",
       "        0.5343705 , 0.533691  , 0.53366965, 0.53475505, 0.53271705,\n",
       "        0.5330865 , 0.5330865 , 0.53243893, 0.5325176 , 0.53269804,\n",
       "        0.53175616, 0.53216654, 0.5324754 , 0.5322118 , 0.53268033,\n",
       "        0.53197336, 0.53246844, 0.53340924, 0.53209484, 0.53335667,\n",
       "        0.5320087 , 0.5344156 , 0.5320366 , 0.5399178 , 0.53378475,\n",
       "        0.5312728 , 0.5326098 , 0.53423333, 0.53372467, 0.53458214,\n",
       "        0.531295  , 0.53272796, 0.5351075 , 0.53299224, 0.5329103 ,\n",
       "        0.532623  , 0.532562  , 0.53222924, 0.53290623, 0.5327041 ,\n",
       "        0.5323354 , 0.5329952 , 0.52964205, 0.53013647, 0.5342056 ,\n",
       "        0.53474295, 0.533867  , 0.53436613, 0.52986264, 0.5320737 ,\n",
       "        0.5320737 , 0.5333701 , 0.5321043 , 0.53002393, 0.53310204,\n",
       "        0.5330805 , 0.5326168 , 0.5327638 , 0.5323194 , 0.53307897,\n",
       "        0.5329196 , 0.53312653, 0.53259176, 0.5331588 , 0.53338015,\n",
       "        0.5327117 , 0.5323156 , 0.52704793, 0.5372896 , 0.53482467],\n",
       "       [0.5569858 , 0.5557984 , 0.5547596 , 0.5559613 , 0.5563718 ,\n",
       "        0.55721915, 0.557995  , 0.5555828 , 0.56074023, 0.556309  ,\n",
       "        0.5579543 , 0.5575816 , 0.5565924 , 0.5568124 , 0.5572254 ,\n",
       "        0.55671954, 0.55748546, 0.55703026, 0.5570823 , 0.5571655 ,\n",
       "        0.5568907 , 0.5564965 , 0.5562634 , 0.5561016 , 0.5567282 ,\n",
       "        0.5567282 , 0.5574485 , 0.5577075 , 0.5576025 , 0.5569002 ,\n",
       "        0.55713195, 0.55780774, 0.5573738 , 0.55839634, 0.5581314 ,\n",
       "        0.55707353, 0.5570083 , 0.55616665, 0.5577032 , 0.55682737,\n",
       "        0.557509  , 0.5538953 , 0.56337494, 0.5565357 , 0.55083793,\n",
       "        0.5563637 , 0.5578825 , 0.55751   , 0.5572202 , 0.5579592 ,\n",
       "        0.55984277, 0.55254114, 0.55196434, 0.5589536 , 0.5573657 ,\n",
       "        0.55735606, 0.55696267, 0.55680794, 0.55676496, 0.5566856 ,\n",
       "        0.5572306 , 0.5526761 , 0.5506723 , 0.5554684 , 0.5512426 ,\n",
       "        0.5588714 , 0.5585563 , 0.55328345, 0.5531109 , 0.5531109 ,\n",
       "        0.5556391 , 0.55893385, 0.5577054 , 0.5574046 , 0.55751276,\n",
       "        0.5576909 , 0.5580198 , 0.55726475, 0.55946463, 0.5565504 ,\n",
       "        0.5579188 , 0.5576359 , 0.55669427, 0.55746275, 0.55720305,\n",
       "        0.5575371 , 0.55804396, 0.55795544, 0.55805933, 0.55888194,\n",
       "        0.5573634 , 0.55816746, 0.55691373, 0.55930907, 0.55549085,\n",
       "        0.55144495, 0.5574866 , 0.55491483, 0.5512291 , 0.5537702 ],\n",
       "       [0.5420081 , 0.5418698 , 0.539021  , 0.53952855, 0.54167837,\n",
       "        0.54164493, 0.54203105, 0.5410034 , 0.54204786, 0.5435165 ,\n",
       "        0.54380536, 0.54214823, 0.54103786, 0.5417513 , 0.54278755,\n",
       "        0.542177  , 0.5428859 , 0.542686  , 0.54309887, 0.54309887,\n",
       "        0.5412639 , 0.5418499 , 0.541406  , 0.5409586 , 0.54411435,\n",
       "        0.54343885, 0.5420769 , 0.5410015 , 0.5424644 , 0.5453458 ,\n",
       "        0.5421685 , 0.53934354, 0.5345214 , 0.5524653 , 0.5349415 ,\n",
       "        0.53178465, 0.5430193 , 0.5436841 , 0.5422346 , 0.5415609 ,\n",
       "        0.54276603, 0.54348123, 0.5367295 , 0.5348791 , 0.5442717 ,\n",
       "        0.5419275 , 0.54194784, 0.5413717 , 0.54138565, 0.5411004 ,\n",
       "        0.5414114 , 0.5410996 , 0.5361361 , 0.53566194, 0.54170555,\n",
       "        0.5386494 , 0.54318386, 0.54303086, 0.53655726, 0.5384522 ,\n",
       "        0.5384522 , 0.54173523, 0.54327124, 0.54059786, 0.54295135,\n",
       "        0.5409438 , 0.5416364 , 0.54201674, 0.5423499 , 0.5418674 ,\n",
       "        0.5418449 , 0.5425082 , 0.54378265, 0.53999084, 0.54256094,\n",
       "        0.54200375, 0.5473255 , 0.54170144, 0.5376779 , 0.54162484,\n",
       "        0.541364  , 0.5368267 , 0.5379588 , 0.5419386 , 0.538886  ,\n",
       "        0.54257923, 0.54014075, 0.54014075, 0.5426019 , 0.54231006,\n",
       "        0.54050136, 0.54136574, 0.5419227 , 0.54152495, 0.542805  ,\n",
       "        0.54474705, 0.54218405, 0.5394538 , 0.53890353, 0.54081655],\n",
       "       [0.55223376, 0.55197185, 0.5518718 , 0.5510278 , 0.5520686 ,\n",
       "        0.5510857 , 0.55171454, 0.55118114, 0.5512178 , 0.5517975 ,\n",
       "        0.55267626, 0.55200833, 0.552315  , 0.55260986, 0.5530529 ,\n",
       "        0.5534795 , 0.5491113 , 0.55132586, 0.54998684, 0.55108863,\n",
       "        0.5512667 , 0.5524689 , 0.55127716, 0.55144864, 0.55144864,\n",
       "        0.5509728 , 0.55268884, 0.552063  , 0.5507158 , 0.55112594,\n",
       "        0.5515424 , 0.5514238 , 0.55093575, 0.5515258 , 0.5532509 ,\n",
       "        0.55219436, 0.5527072 , 0.54906994, 0.5527694 , 0.5489766 ,\n",
       "        0.54315776, 0.5539409 , 0.55264825, 0.5526427 , 0.551047  ,\n",
       "        0.55370337, 0.5530316 , 0.55281115, 0.55500525, 0.5499659 ,\n",
       "        0.54847556, 0.55509657, 0.55251545, 0.55236256, 0.5520103 ,\n",
       "        0.5520137 , 0.55202657, 0.55244297, 0.55249435, 0.5518869 ,\n",
       "        0.55159086, 0.5468896 , 0.54741246, 0.55348325, 0.5486172 ,\n",
       "        0.5530187 , 0.55345196, 0.54765695, 0.55038255, 0.55038255,\n",
       "        0.5524506 , 0.552809  , 0.55249816, 0.5521076 , 0.55271554,\n",
       "        0.5521103 , 0.5525826 , 0.55212116, 0.5527561 , 0.5527561 ,\n",
       "        0.5530451 , 0.5507964 , 0.5526882 , 0.5523012 , 0.55352944,\n",
       "        0.55180955, 0.54772264, 0.55251205, 0.55178237, 0.5490655 ,\n",
       "        0.54846084, 0.55217123, 0.5474041 , 0.5508414 , 0.5514325 ,\n",
       "        0.5515993 , 0.5515282 , 0.5500069 , 0.5506221 , 0.552303  ],\n",
       "       [0.5451821 , 0.545856  , 0.5423245 , 0.5453072 , 0.5445303 ,\n",
       "        0.54569733, 0.545517  , 0.54527897, 0.5456223 , 0.5461858 ,\n",
       "        0.5466897 , 0.5455812 , 0.54569554, 0.54676366, 0.5457272 ,\n",
       "        0.5444268 , 0.54532087, 0.546822  , 0.54672676, 0.54672676,\n",
       "        0.54575676, 0.54661226, 0.5462969 , 0.5449287 , 0.54567087,\n",
       "        0.54570735, 0.5453925 , 0.54517734, 0.5461205 , 0.54794836,\n",
       "        0.5456731 , 0.544783  , 0.54087335, 0.5504124 , 0.53971124,\n",
       "        0.53708375, 0.54575396, 0.5465205 , 0.5463012 , 0.5454315 ,\n",
       "        0.54642904, 0.5462775 , 0.54156494, 0.5422465 , 0.5473992 ,\n",
       "        0.54554576, 0.545776  , 0.5454984 , 0.5451749 , 0.5450679 ,\n",
       "        0.54515886, 0.5444083 , 0.5406948 , 0.5398273 , 0.5457905 ,\n",
       "        0.5435667 , 0.546387  , 0.5455036 , 0.54156816, 0.5423893 ,\n",
       "        0.5423893 , 0.5440866 , 0.5462556 , 0.5451503 , 0.5453502 ,\n",
       "        0.5449479 , 0.54515165, 0.5457894 , 0.5451443 , 0.5455587 ,\n",
       "        0.5463957 , 0.5450261 , 0.54605657, 0.54193556, 0.54569876,\n",
       "        0.5453021 , 0.5449719 , 0.54537034, 0.54530406, 0.54516405,\n",
       "        0.5444447 , 0.54563713, 0.54525054, 0.54624754, 0.5469557 ,\n",
       "        0.5453221 , 0.5451756 , 0.54481834, 0.5453552 , 0.54541904,\n",
       "        0.54523045, 0.54549396, 0.54553455, 0.5455569 , 0.5455205 ,\n",
       "        0.54358894, 0.5469353 , 0.5496076 , 0.54506886, 0.5416997 ],\n",
       "       [0.5993231 , 0.59688395, 0.5992513 , 0.60091513, 0.6010218 ,\n",
       "        0.6004767 , 0.59808105, 0.601875  , 0.5987578 , 0.60139805,\n",
       "        0.59913176, 0.5992997 , 0.60034907, 0.5992954 , 0.5993457 ,\n",
       "        0.5996902 , 0.59934527, 0.5996362 , 0.5991525 , 0.6000962 ,\n",
       "        0.6016015 , 0.5991481 , 0.59930074, 0.5991581 , 0.59853363,\n",
       "        0.6000005 , 0.59961534, 0.6007984 , 0.59899545, 0.5989907 ,\n",
       "        0.5989907 , 0.59941304, 0.5991948 , 0.59890944, 0.5986603 ,\n",
       "        0.59899026, 0.5995714 , 0.59912574, 0.5987227 , 0.6004926 ,\n",
       "        0.5988959 , 0.5960412 , 0.5988792 , 0.5978649 , 0.5987307 ,\n",
       "        0.5905703 , 0.5983738 , 0.5983738 , 0.5988772 , 0.59898496,\n",
       "        0.5987114 , 0.59968746, 0.5980249 , 0.59982014, 0.6000879 ,\n",
       "        0.5979293 , 0.5991014 , 0.6008797 , 0.5984468 , 0.59714335,\n",
       "        0.59595186, 0.6004303 , 0.5976719 , 0.5952802 , 0.6018446 ,\n",
       "        0.59591806, 0.59066916, 0.59882313, 0.59930706, 0.5915205 ,\n",
       "        0.5974625 , 0.6014984 , 0.60111475, 0.600505  , 0.59974045,\n",
       "        0.5994084 , 0.5995034 , 0.5996853 , 0.5982845 , 0.5981252 ,\n",
       "        0.59917164, 0.60013974, 0.5997848 , 0.5994414 , 0.5991323 ,\n",
       "        0.59935117, 0.5993333 , 0.5992539 , 0.59933543, 0.5973671 ,\n",
       "        0.5960924 , 0.5953839 , 0.5980215 , 0.59683305, 0.6010707 ,\n",
       "        0.59882975, 0.59789354, 0.59383094, 0.59891266, 0.59891266]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(stats[\"sample_token_similarity\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2b5ec87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.35613904])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sample_log_likelihoods = stats[\"sample_log_likelihoods\"]\n",
    "batch_sample_token_similarity = stats[\"sample_token_similarity\"]\n",
    "batch_sample_sentence_similarity = stats[\"sample_sentence_similarity\"]\n",
    "t = 0.001\n",
    "SAR = []\n",
    "for batch_data in zip(\n",
    "    batch_sample_log_likelihoods,\n",
    "    batch_sample_token_similarity,\n",
    "    batch_sample_sentence_similarity,\n",
    "):\n",
    "    sample_log_likelihoods = batch_data[0]\n",
    "    sample_token_similarity = batch_data[1]\n",
    "    sample_sentence_similarity = batch_data[2]\n",
    "\n",
    "    tokenSAR = []\n",
    "    for log_likelihoods, token_similarity in zip(\n",
    "        sample_log_likelihoods, sample_token_similarity\n",
    "    ):\n",
    "        log_likelihoods = np.array(log_likelihoods)\n",
    "        R_t = 1 - token_similarity\n",
    "        R_t_norm = R_t / R_t.sum()\n",
    "        E_t = -log_likelihoods * R_t_norm\n",
    "        tokenSAR.append(E_t.sum())\n",
    "\n",
    "    tokenSAR = np.array(tokenSAR)\n",
    "    probs_token_sar = np.exp(-tokenSAR)\n",
    "    R_s = (\n",
    "        probs_token_sar\n",
    "        * sample_sentence_similarity\n",
    "        * (1 - np.eye(sample_sentence_similarity.shape[0]))\n",
    "    )\n",
    "    sent_relevance = R_s.sum(-1) / t\n",
    "    E_s = -np.log(sent_relevance + probs_token_sar)\n",
    "    SAR.append(E_s.mean())\n",
    "np.array(SAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3dc0e6-804f-490e-9b77-4f5b3cb0ad64",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e7a7afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b8168930ec45de985b8ccc15dd05ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    '/data/home/wangys/model/Mistral-7B-Instruct-v0.2',\n",
    "    device_map='cuda:7',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained('/data/home/wangys/model/Mistral-7B-Instruct-v0.2', token_type_ids=None,\n",
    "                clean_up_tokenization_spaces=False)\n",
    "\n",
    "model = WhiteboxModel(base_model,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad18648a-b1c7-4089-832e-84e17be8b203",
   "metadata": {},
   "source": [
    "### Token level UE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "030b389b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/home/wangys/model/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /data/home/wangys/sentence_transformer_model/stsb-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(uncertainty=-8.356139042348568, input_text='Who is George Bush?', generation_text=\"George Bush may refer to one of two U.S. presidents named George Bush. Here's a brief overview of each:\\n\\n1. George H.W. Bush (b. 1924): He was the 41st President of the United States, serving from 1989 to 1993. Bush was a naval aviator in World War II and later served as a member of Congress from Texas. He also served as the\", generation_tokens=[5163, 13668, 993, 3295, 298, 624, 302, 989, 500, 28723, 28735, 28723, 1258, 6640, 5160, 5163, 13668, 28723, 4003, 28742, 28713, 264, 6817, 23094, 302, 1430, 28747, 13, 13, 28740, 28723, 5163, 382, 28723, 28780, 28723, 13668, 325, 28726, 28723, 28705, 28740, 28774, 28750, 28781, 1329, 650, 403, 272, 28705, 28781, 28740, 303, 5120, 302, 272, 2969, 3543, 28725, 10732, 477, 28705, 28740, 28774, 28783, 28774, 298, 28705, 28740, 28774, 28774, 28770, 28723, 13668, 403, 264, 23850, 1182, 28710, 1028, 297, 3304, 3273, 3717, 304, 2062, 6117, 390, 264, 4292, 302, 8463, 477, 7826, 28723, 650, 835, 6117, 390], model_path=None, estimator='SAR')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = SAR()\n",
    "estimate_uncertainty(model, estimator, input_text='Who is George Bush?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "247f5d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/home/wangys/model/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(uncertainty=13.410937309265137, input_text='Who is George Bush?', generation_text=\"George Bush may refer to one of two U.S. presidents named George Bush. Here's a brief overview of each:\\n\\n1. George H.W. Bush (b. 1924): He was the 41st President of the United States, serving from 1989 to 1993. Bush was a naval aviator in World War II and later served as a member of Congress from Texas. He also served as the\", generation_tokens=[5163, 13668, 993, 3295, 298, 624, 302, 989, 500, 28723, 28735, 28723, 1258, 6640, 5160, 5163, 13668, 28723, 4003, 28742, 28713, 264, 6817, 23094, 302, 1430, 28747, 13, 13, 28740, 28723, 5163, 382, 28723, 28780, 28723, 13668, 325, 28726, 28723, 28705, 28740, 28774, 28750, 28781, 1329, 650, 403, 272, 28705, 28781, 28740, 303, 5120, 302, 272, 2969, 3543, 28725, 10732, 477, 28705, 28740, 28774, 28783, 28774, 298, 28705, 28740, 28774, 28774, 28770, 28723, 13668, 403, 264, 23850, 1182, 28710, 1028, 297, 3304, 3273, 3717, 304, 2062, 6117, 390, 264, 4292, 302, 8463, 477, 7826, 28723, 650, 835, 6117, 390], model_path=None, estimator='MaximumSequenceProbability')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = MaximumSequenceProbability()\n",
    "estimate_uncertainty(model, estimator, input_text='Who is George Bush?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4043671a-939f-421b-b06b-24abf557fdc9",
   "metadata": {},
   "source": [
    "### Sequence level UE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c2e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = SAR()\n",
    "estimate_uncertainty(model, estimator, input_text='Who is George Bush?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dd6ed2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/home/wangys/model/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(uncertainty=-17.852901331445374, input_text='Who is George Bush?', generation_text=\"George Bush may refer to one of two U.S. presidents named George Bush. Here's a brief overview of each:\\n\\n1. George H.W. Bush (b. 1924): He was the 41st President of the United States, serving from 1989 to 1993. Bush was a naval aviator in World War II and later served as a member of Congress from Texas. He also served as the\", generation_tokens=[5163, 13668, 993, 3295, 298, 624, 302, 989, 500, 28723, 28735, 28723, 1258, 6640, 5160, 5163, 13668, 28723, 4003, 28742, 28713, 264, 6817, 23094, 302, 1430, 28747, 13, 13, 28740, 28723, 5163, 382, 28723, 28780, 28723, 13668, 325, 28726, 28723, 28705, 28740, 28774, 28750, 28781, 1329, 650, 403, 272, 28705, 28781, 28740, 303, 5120, 302, 272, 2969, 3543, 28725, 10732, 477, 28705, 28740, 28774, 28783, 28774, 298, 28705, 28740, 28774, 28774, 28770, 28723, 13668, 403, 264, 23850, 1182, 28710, 1028, 297, 3304, 3273, 3717, 304, 2062, 6117, 390, 264, 4292, 302, 8463, 477, 7826, 28723, 650, 835, 6117, 390], model_path=None, estimator='MeanPointwiseMutualInformation')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = MeanPointwiseMutualInformation()\n",
    "estimate_uncertainty(model, estimator, input_text='Who is George Bush?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9855deac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    }
   ],
   "source": [
    "stats = np.load('/data/home/wangys/lm-polygraph/examples/semantic_entropy.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8492d549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.7672994])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "def semantic_entropy(stats: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Estimates the semantic entropy for each sample in the input statistics.\n",
    "\n",
    "    Parameters:\n",
    "        stats (Dict[str, np.ndarray]): input statistics, which for multiple samples includes:\n",
    "            * generated samples in 'sample_texts',\n",
    "            * corresponding log probabilities in 'sample_log_probs',\n",
    "            * matrix with semantic similarities in 'semantic_matrix_entail'\n",
    "    Returns:\n",
    "        np.ndarray: float semantic entropy for each sample in input statistics.\n",
    "            Higher values indicate more uncertain samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    loglikelihoods_list = stats[\"sample_log_probs\"]\n",
    "    entailment_id = stats.get(\"entailment_id\", 1)\n",
    "    is_entailment = stats[\"semantic_matrix_entail\"] == entailment_id\n",
    "\n",
    "    # Concatenating hypotheses with input texts\n",
    "    hyps_list = [[' '.join([input_text, hyp]) for hyp in stats[\"sample_texts\"][i]] for i, input_text in enumerate(stats[\"input_texts\"])]\n",
    "    \n",
    "    return batched_call(hyps_list, loglikelihoods_list, is_entailment)\n",
    "\n",
    "def batched_call(hyps_list: List[List[str]], loglikelihoods_list: List[List[float]], is_entailment: np.ndarray, log_weights: Optional[List[List[float]]] = None) -> np.array:\n",
    "    if log_weights is None:\n",
    "        log_weights = [None] * len(hyps_list)\n",
    "    \n",
    "    semantic_logits = {}\n",
    "    sample_to_class = {}\n",
    "    class_to_sample = defaultdict(list)\n",
    "\n",
    "    # Determine classes for hypotheses\n",
    "    for idx, hyps in enumerate(hyps_list):\n",
    "        sample_to_class[idx], class_to_sample[idx] = determine_classes(hyps, is_entailment[idx])\n",
    "\n",
    "        # Collect likelihoods per class\n",
    "        class_likelihoods = [np.array(loglikelihoods_list[idx])[np.array(class_idx)] for class_idx in class_to_sample[idx]]\n",
    "        class_lp = [np.logaddexp.reduce(likelihoods) for likelihoods in class_likelihoods]\n",
    "        \n",
    "        # Apply weights if provided\n",
    "        if log_weights[idx] is None:\n",
    "            log_weights[idx] = [0] * len(hyps)\n",
    "        \n",
    "        semantic_logits[idx] = -np.mean([class_lp[sample_to_class[idx][j]] * np.exp(log_weights[idx][j]) for j in range(len(hyps))])\n",
    "    \n",
    "    return np.array([semantic_logits[i] for i in range(len(hyps_list))])\n",
    "\n",
    "def determine_classes(hyps: List[str], is_entailment: np.ndarray) -> (Dict[int, int], Dict[int, List[int]]):\n",
    "    sample_to_class = {}\n",
    "    class_to_sample = defaultdict(list)\n",
    "\n",
    "    for i in range(len(hyps)):\n",
    "        if i == 0:\n",
    "            class_to_sample[0] = [0]\n",
    "            sample_to_class[0] = 0\n",
    "            continue\n",
    "        \n",
    "        for class_id, class_indices in class_to_sample.items():\n",
    "            class_text_id = class_indices[0]\n",
    "            if is_entailment[class_text_id, i] and is_entailment[i, class_text_id]:\n",
    "                class_to_sample[class_id].append(i)\n",
    "                sample_to_class[i] = class_id\n",
    "                break\n",
    "        else:\n",
    "            new_class_id = len(class_to_sample)\n",
    "            class_to_sample[new_class_id] = [i]\n",
    "            sample_to_class[i] = new_class_id\n",
    "    \n",
    "    return sample_to_class, class_to_sample\n",
    "\n",
    "# Example usage:\n",
    "# stats = {\n",
    "#     'sample_log_probs': np.array([[0.1, 0.2], [0.4, 0.5]]),\n",
    "#     'sample_texts': [['text1', 'text2'], ['text3', 'text4']],\n",
    "#     'input_texts': ['input1', 'input2'],\n",
    "#     'semantic_matrix_entail': np.array([[[1, 0], [0, 1]], [[1, 1], [1, 1]]]),\n",
    "#     'entailment_id': 1\n",
    "# }\n",
    "output = semantic_entropy(stats)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8292b97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/home/wangys/model/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(uncertainty=-0.5354888443376555, input_text='Who is George Bush?', generation_text=\"George Bush may refer to one of two U.S. presidents named George Bush. Here's a brief overview of each:\\n\\n1. George H.W. Bush (b. 1924): He was the 41st President of the United States, serving from 1989 to 1993. Bush was a naval aviator in World War II and later served as a member of Congress from Texas. He also served as the\", generation_tokens=[5163, 13668, 993, 3295, 298, 624, 302, 989, 500, 28723, 28735, 28723, 1258, 6640, 5160, 5163, 13668, 28723, 4003, 28742, 28713, 264, 6817, 23094, 302, 1430, 28747, 13, 13, 28740, 28723, 5163, 382, 28723, 28780, 28723, 13668, 325, 28726, 28723, 28705, 28740, 28774, 28750, 28781, 1329, 650, 403, 272, 28705, 28781, 28740, 303, 5120, 302, 272, 2969, 3543, 28725, 10732, 477, 28705, 28740, 28774, 28783, 28774, 298, 28705, 28740, 28774, 28774, 28770, 28723, 13668, 403, 264, 23850, 1182, 28710, 1028, 297, 3304, 3273, 3717, 304, 2062, 6117, 390, 264, 4292, 302, 8463, 477, 7826, 28723, 650, 835, 6117, 390], model_path=None, estimator='LexicalSimilarity_rougeL')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = LexicalSimilarity('rougeL')\n",
    "estimate_uncertainty(model, estimator, input_text='Who is George Bush?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a906db0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/home/wangys/model/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(uncertainty=20.165683420266305, input_text='Who is George Bush?', generation_text=\"George Bush may refer to one of two U.S. presidents named George Bush. Here's a brief overview of each:\\n\\n1. George H.W. Bush (b. 1924): He was the 41st President of the United States, serving from 1989 to 1993. Bush was a naval aviator in World War II and later served as a member of Congress from Texas. He also served as the\", generation_tokens=[5163, 13668, 993, 3295, 298, 624, 302, 989, 500, 28723, 28735, 28723, 1258, 6640, 5160, 5163, 13668, 28723, 4003, 28742, 28713, 264, 6817, 23094, 302, 1430, 28747, 13, 13, 28740, 28723, 5163, 382, 28723, 28780, 28723, 13668, 325, 28726, 28723, 28705, 28740, 28774, 28750, 28781, 1329, 650, 403, 272, 28705, 28781, 28740, 303, 5120, 302, 272, 2969, 3543, 28725, 10732, 477, 28705, 28740, 28774, 28783, 28774, 298, 28705, 28740, 28774, 28774, 28770, 28723, 13668, 403, 264, 23850, 1182, 28710, 1028, 297, 3304, 3273, 3717, 304, 2062, 6117, 390, 264, 4292, 302, 8463, 477, 7826, 28723, 650, 835, 6117, 390], model_path=None, estimator='SemanticEntropy')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = SemanticEntropy()\n",
    "estimate_uncertainty(model, estimator, input_text='Who is George Bush?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f3c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PointwiseMutualInformation()\n",
    "estimate_uncertainty(model, estimator, input_text='Once upon a time there was a little girl who liked to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc03fd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/home/wangys/model/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(uncertainty=-0.5115664112993994, input_text='Who is George Bush?', generation_text=\"George Bush may refer to one of two U.S. presidents named George Bush. Here's a brief overview of each:\\n\\n1. George H.W. Bush (b. 1924): He was the 41st President of the United States, serving from 1989 to 1993. Bush was a naval aviator in World War II and later served as a member of Congress from Texas. He also served as the\", generation_tokens=[5163, 13668, 993, 3295, 298, 624, 302, 989, 500, 28723, 28735, 28723, 1258, 6640, 5160, 5163, 13668, 28723, 4003, 28742, 28713, 264, 6817, 23094, 302, 1430, 28747, 13, 13, 28740, 28723, 5163, 382, 28723, 28780, 28723, 13668, 325, 28726, 28723, 28705, 28740, 28774, 28750, 28781, 1329, 650, 403, 272, 28705, 28781, 28740, 303, 5120, 302, 272, 2969, 3543, 28725, 10732, 477, 28705, 28740, 28774, 28783, 28774, 298, 28705, 28740, 28774, 28774, 28770, 28723, 13668, 403, 264, 23850, 1182, 28710, 1028, 297, 3304, 3273, 3717, 304, 2062, 6117, 390, 264, 4292, 302, 8463, 477, 7826, 28723, 650, 835, 6117, 390], model_path=None, estimator='LexicalSimilarity_rougeL')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ue_method = LexicalSimilarity()\n",
    "input_text = \"Who is George Bush?\"\n",
    "estimate_uncertainty(model, ue_method, input_text=input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7940d-9f83-4872-a0e3-d2e9a83d8a9e",
   "metadata": {},
   "source": [
    "### BlackBox UE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb84386",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BlackboxModel(\n",
    "    'YOUR_OPENAI_TOKEN',\n",
    "    'gpt-3.5-turbo'\n",
    ")\n",
    "estimator = EigValLaplacian(verbose=True)\n",
    "estimate_uncertainty(model, estimator, input_text='When did Albert Einstein die?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b63635",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_TOKEN = 'YOUR_API_TOKEN'\n",
    "# for example let's take google/t5-small-ssm-nq model\n",
    "MODEL_ID = 'google/t5-large-ssm-nqo'\n",
    "\n",
    "model = BlackboxModel.from_huggingface(hf_api_token=API_TOKEN, hf_model_id=MODEL_ID, openai_api_key = None, openai_model_path = None)\n",
    "ue_method = LexicalSimilarity()\n",
    "input_text = \"Who is George Bush?\"\n",
    "estimate_uncertainty(model, ue_method, input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec1991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example let's take bigscience/bloomz-560m model\n",
    "MODEL_ID = 'bigscience/bloomz-560m'\n",
    "\n",
    "model = BlackboxModel.from_huggingface(hf_api_token=API_TOKEN, hf_model_id=MODEL_ID, openai_api_key = None, openai_model_path = None)\n",
    "ue_method = LexicalSimilarity()\n",
    "input_text = \"Who is George Bush?\"\n",
    "estimate_uncertainty(model, ue_method, input_text=input_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
