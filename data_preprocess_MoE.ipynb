{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "tqdm.pandas(desc='pandas bar')\n",
    "# from pandarallel import pandarallel\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "import logging\n",
    "# pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "from FlagEmbedding import FlagModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_google_train = pd.read_csv('/data/home/wangys/LLM_ER/process_data/amazon-google/ditto_CL_train.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_google_train_table = pd.read_csv('/data/home/wangys/LLM_ER/process_data/amazon-google/amazon-google/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amazon_google_train_table\n",
    "amazon_google_tableA = pd.read_csv('/data/home/wangys/LLM_ER/process_data/amazon-google/amazon-google/tableA.csv')\n",
    "amazon_google_tableB = pd.read_csv('/data/home/wangys/LLM_ER/process_data/amazon-google/amazon-google/tableB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_Title(row):\n",
    "    split_a = row[0].split('COL title VAL ')[1].split(' COL')[0]\n",
    "    split_b = row[1].split('COL title VAL ')[1].split(' COL')[0]\n",
    "    print(split_a)\n",
    "    id_a = amazon_google_tableA[amazon_google_tableA['title']==split_a].iloc[0,0]\n",
    "    id_b = amazon_google_tableB[amazon_google_tableB['title']==split_b].iloc[0,0]\n",
    "    return id_a,id_b\n",
    "amazon_google_train_pos = amazon_google_train[amazon_google_train['2']==1]\n",
    "# amazon_google_train_pos['0'].str.split('COL title VAL ')[0].split(' COL manufacturer')[0]\n",
    "amazon_google_train_pos_id = amazon_google_train_pos.progress_apply(Extract_Title,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/data/home/wangys/LLM_ER/process_data/amazon-google/pos_list.npy',np.array(amazon_google_train_pos_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ltable = np.load('/data/home/wangys/LLM_ER/process_data/amazon-google/dict_ltable.npy',allow_pickle=True).item()\n",
    "dict_rtable = np.load('/data/home/wangys/LLM_ER/process_data/amazon-google/dict_rtable.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableA = pd.read_csv('/data/home/wangys/LLM_ER/process_data/amazon-google/amazon-google/tableA.csv').fillna('')\n",
    "tableB = pd.read_csv('/data/home/wangys/LLM_ER/process_data/amazon-google/amazon-google/tableB.csv').fillna('')\n",
    "tableA_dict = tableA.set_index('id').to_dict(orient='records')\n",
    "tableB_dict = tableB.set_index('id').to_dict(orient='records')\n",
    "train = pd.read_csv('/data/home/wangys/LLM_ER/process_data/amazon-google/amazon-google/train.csv')\n",
    "valid = pd.read_csv('/data/home/wangys/LLM_ER/process_data/amazon-google/amazon-google/valid.csv')\n",
    "test = pd.read_csv('/data/home/wangys/LLM_ER/process_data/amazon-google/amazon-google/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableA_dict_update = {}\n",
    "tableB_dict_update = {}\n",
    "for i in range(len(tableA_dict)):\n",
    "    tableA_dict_update[i] = tableA_dict[i]\n",
    "for i in range(len(tableB_dict)):\n",
    "    tableB_dict_update[i] = tableB_dict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We try a different Negative Sampling Strategy here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f4beb6798c4c59b7436dfce12b1598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# amazon_google_train_pos_id\n",
    "query_list = []\n",
    "for pos_id_pair in tqdm(amazon_google_train_pos_id):\n",
    "    ltable_id = pos_id_pair[0]\n",
    "    rtable_id = pos_id_pair[1]\n",
    "    for l in dict_ltable[ltable_id]: ## All Dict in ltable\n",
    "        query_list.append([str(l),[str(r) for r in dict_rtable[rtable_id]],[]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df = pd.DataFrame(query_list)\n",
    "query_df.columns = ['query','pos','neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c61fa1b0424cd98d13d43f5502f283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/wangys/MoE-Example/SM/CMS/SBert/CMS-blocking-update.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "df = query_list_pd[query_list_pd.iloc[:,1].astype(str)!='[]']\n",
    "df.columns = ['query','pos','neg']\n",
    "# 打开一个文件以写入\n",
    "path = '/data/home/wangys/MoE-Example/SM/CMS/SBert/CMS-blocking-update.json'\n",
    "with open(path, 'w', encoding='utf-8') as file:\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        # 对于每行，创建一个字典\n",
    "        data = {\n",
    "            \"query\": row[\"query\"],\n",
    "            \"pos\": row[\"pos\"],\n",
    "            \"neg\": row[\"neg\"]\n",
    "        }\n",
    "\n",
    "        # 将字典转换为JSON字符串，并写入文件\n",
    "        file.write(json.dumps(data, ensure_ascii=False))\n",
    "        file.write('\\n')  # 每个JSON对象后添加换行符\n",
    "\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process the Mapping of Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltable_list = []\n",
    "rtable_list = []\n",
    "count = 0\n",
    "for key in dict_ltable.keys():\n",
    "    for l in dict_ltable[key]:\n",
    "        ltable_list.append([count,key,str(l)])\n",
    "        count += 1\n",
    "count = 0\n",
    "for key in dict_rtable.keys():\n",
    "    for r in dict_rtable[key]:\n",
    "        rtable_list.append([count,key,str(r)])\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltable_list = pd.DataFrame(ltable_list)\n",
    "rtable_list = pd.DataFrame(rtable_list)\n",
    "ltable_list.columns = ['index','id','query']\n",
    "rtable_list.columns = ['index','id','query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltable_index2id = dict(zip(ltable_list['index'],ltable_list['id']))\n",
    "rtable_index2id = dict(zip(rtable_list['index'],rtable_list['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------using 8*GPUs----------\n"
     ]
    }
   ],
   "source": [
    "model = FlagModel('/data/home/wangys/MoE-Example/ER/amazon-google/SBert/rerank-em', \n",
    "                  use_fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
      "Inference Embeddings: 100%|██████████| 7/7 [00:03<00:00,  2.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# embedding_a = model.encode(ltable_list['query'].to_list())\n",
    "embedding_a = model.encode([str(tableA_dict_update[i]) for i in range(len(tableA_dict_update))])\n",
    "embedding_b = model.encode(rtable_list['query'].to_list())\n",
    "# similarity = embedding_a @ embedding_b.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "embedding_a = torch.DoubleTensor(embedding_a) # 64 bit\n",
    "embedding_b = torch.DoubleTensor(embedding_b)\n",
    "if (torch.cuda.is_available()):\n",
    "    embedding_a = torch.DoubleTensor(embedding_a).cuda()\n",
    "    embedding_b = torch.DoubleTensor(embedding_b).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_array = torch.mm(embedding_a, embedding_b.T).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 7357/7357 [00:06<00:00, 1209.20it/s]\n"
     ]
    }
   ],
   "source": [
    "ltable_list\n",
    "# def L2R_search(row): ## augment-augment\n",
    "#     index = row['index']\n",
    "#     r_index = np.argsort(-out_array[index])[0] ## Most Similar\n",
    "#     r_id = rtable_index2id[r_index]\n",
    "#     return r_id\n",
    "k = 2\n",
    "def L2R_search(row):\n",
    "    index = row['id']\n",
    "    r_index = np.argsort(-out_array[index])[0:k] ## Most Similar\n",
    "    r_id = [rtable_index2id[r] for r in r_index]\n",
    "    return r_id\n",
    "ltable_list['r_id'] = ltable_list.progress_apply(L2R_search,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_all = pd.concat([train,valid,test])\n",
    "amazon_all_pos = amazon_all[amazon_all['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7960582690659811"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_num = 0\n",
    "recall_num = 0\n",
    "for index,row in amazon_all_pos.iterrows():\n",
    "    ltable_id = row['ltable_id'] ## ltable_id\n",
    "    rtable_id = row['rtable_id']\n",
    "    search_df = ltable_list[ltable_list['id']==ltable_id]\n",
    "    voting_num = search_df['r_id'].value_counts().idxmax()\n",
    "    if(rtable_id in voting_num):\n",
    "        recall_num += 1\n",
    "    base_num += 1\n",
    "recall_num/base_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1383909/3067993067.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mamazon_google\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/data/home/wangys/LLM_ER/process_data/amazon-google/amazon_google_train_filter.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "amazon_google = pd.read_csv('/data/home/wangys/LLM_ER/process_data/amazon-google/amazon_google_train_filter.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_google_test = pd.read_csv('/data/home/wangys/LLM_ER/process_data/amazon-google/amazon_google_test_filter.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'tlc dr. seuss reading learning system 2008',\n",
       " 'manufacturer': 'encore',\n",
       " 'price': 19.99,\n",
       " 'category': 'Education',\n",
       " 'subcategory': 'Language Learning',\n",
       " 'platform': 'Windows',\n",
       " 'edition': 'Standard',\n",
       " 'type': 'Software',\n",
       " 'modelno': 'RSLS-008'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.literal_eval(amazon_google.iloc[0,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"Output\": \"\"}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import ast\n",
    "temp_dict = {}\n",
    "temp_dict['Output'] = ''\n",
    "json.dumps(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in amazon_google_train_pos_id:\n",
    "    print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amazon_google_train_pos_id\n",
    "ind = []\n",
    "for a in amazon_google_train_pos_id:\n",
    "    i = amazon_google[(amazon_google['ltable_id']==a[0]) & (amazon_google['rtable_id']==a[1])].index\n",
    "    # print(i)\n",
    "    ind.extend(list(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_google_pos = amazon_google[amazon_google['label']==1]\n",
    "len(amazon_google_pos['rtable_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 6612/6612 [00:16<00:00, 391.09it/s]\n"
     ]
    }
   ],
   "source": [
    "def Prompt_Generalize(row): ## 在train筛选ltable_id and rtbale_id中所有类似的pos/neg放进去，但是不能完全相同，如果没有pos，那么从ground truth选4个，总数维持8个\n",
    "    temp_dict = {}\n",
    "    temp_dict['Output'] = ''\n",
    "    RAG_text = ''\n",
    "    ltable_id = row['ltable_id']\n",
    "    rtable_id = row['rtable_id']\n",
    "    text = 'You are an expert in detecting if two text descriptions of entities refer to the same or different entities. You are well-known to examine the fine details and can find subtle matches or mismatches to help you arrive at the final judgement. \\n\\nJudge whether Entity 1 and Entity 2 are match or dismatch, and choose within the given Options.\\n\\nReturn in JSON format.\\n\\nOptions: [match,dismatch]\\n\\nOutput format example:%s\\n\\nEntity 1:%s\\n\\nEntity 2:%s\\n\\nTake these examples as reference:' % (json.dumps(temp_dict),json.dumps(ast.literal_eval(row['l_entity'])),json.dumps(ast.literal_eval(row['r_entity'])))\n",
    "    select_df = amazon_google[(amazon_google['ltable_id']==ltable_id) | (amazon_google['rtable_id']==rtable_id)] ## 相同元素\n",
    "    select_df = select_df[~((select_df['ltable_id']==ltable_id) & (select_df['rtable_id']==rtable_id))] ## 排除自己\n",
    "    select_df_pos = select_df[select_df['label']==1]\n",
    "    select_df_neg = select_df[select_df['label']==0]\n",
    "    all_neg = amazon_google[amazon_google['label']==0]\n",
    "    # if(len(select_df_pos)>=3): ## Contain pos\n",
    "    #     for index,row_ in select_df_pos.sample(n=4,replace=False).iterrows():\n",
    "    #         print(index)\n",
    "    #         RAG_dict = {}\n",
    "    #         RAG_dict['Entity_1'] = str(row_['l_entity'])\n",
    "    #         RAG_dict['Entity_2'] = str(row_['r_entity'])\n",
    "    #         RAG_dict['Output'] = 'dismatch' if row_['label']==0 else 'match'\n",
    "    #         RAG_text += '%s\\n\\n' % json.dumps(RAG_dict)\n",
    "    # else:\n",
    "    if(len(select_df_neg)>=4):\n",
    "        df = pd.concat([amazon_google_gt.sample(n=4,replace=False),select_df_neg.sample(n=4,replace=False)]).sample(frac=1)\n",
    "    else:\n",
    "        df = pd.concat([amazon_google_gt.sample(n=4,replace=False),select_df_neg.sample(frac=1,replace=False),all_neg.sample(n=4-len(select_df_neg),replace=False)]).sample(frac=1)\n",
    "    for index,row_ in df.iterrows():\n",
    "        \n",
    "        RAG_dict = {}\n",
    "        RAG_dict['Entity_1'] = str(row_['l_entity'])\n",
    "        RAG_dict['Entity_2'] = str(row_['r_entity'])\n",
    "        RAG_dict['Output'] = 'dismatch' if row_['label']==0 else 'match'\n",
    "        RAG_text += '%s\\n\\n' % str(RAG_dict)\n",
    "    output_dict = {}\n",
    "    output_dict['Output'] = 'dismatch' if row['label']==0 else 'match'\n",
    "    return text + RAG_text, '' , str(output_dict)\n",
    "amazon_google_output = amazon_google.progress_apply(Prompt_Generalize,axis=1,result_type='expand')           \n",
    "# amazon_google_output = amazon_google_test.progress_apply(Prompt_Generalize,axis=1,result_type='expand')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_google_output.columns = ['instruction','input','output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/home/wangys/LLaMA-Factory-main/lora_weight/MoE/ER/llama-2-chat/amazon-google-train-MoE/adapter_config.json') as f:\n",
    "  data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(ant_buy_test_output.to_dict(orient='records'), open('/data/home/wangys/LLaMA-Factory-main/data/MoE/ER/ant_buy_test_output.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = CMS_test_output.iloc[:,:3]\n",
    "\n",
    "df.columns = ['instruction','input','output']\n",
    "json.dump(df.to_dict(orient='records'), open('/data/home/wangys/LLaMA-Factory-main/data/MoE/SM/synthea_test_output.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "md5: 6caf0ae3f121b4fb67b9a6287ced5973\n",
      "sha1: 3e88ea7a6940b48cc3694be60c1bdb8990be6663\n",
      "sha256: dd44e169c9f8ccfb211d49d68f767b6cafef1a34af8a90b2b4447a9dbad270e0\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def encrypt(fpath: str, algorithm: str) -> str:\n",
    "    with open(fpath, 'rb') as f:\n",
    "        return hashlib.new(algorithm, f.read()).hexdigest()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for algorithm in ('md5', 'sha1', 'sha256'):\n",
    "        hexdigest = encrypt('/data/home/wangys/LLaMA-Factory-main/data/MoE/DI/amazon_train_all.json', algorithm)\n",
    "        print(f'{algorithm}: {hexdigest}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, LlamaTokenizerFast\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained('/data/home/wangys/model/llama2-13b-chat')\n",
    "token_lengths = [len(tokenizer.encode(text, add_special_tokens=True)) for text in tqdm(prompt_opt['instruction'].to_list())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015381097793579102,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 7405,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91efb3e352d842ceaa77c613f0e5c6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7405 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Lengths: 4417\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "prompt_opt = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/ER/ant_buy_train_output.json')\n",
    "# prompt_opt = CMS_train_few_output\n",
    "prompt_opt.columns = ['instruction','input','output']\n",
    "# 加载 RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('/data/home/wangys/roberta-base')\n",
    "# tokenizer = LlamaTokenizerFast.from_pretrained('/data/home/wangys/model/llama2-13b-chat')\n",
    "\n",
    "# 给定的列表\n",
    "# text_list = [\n",
    "#     \"This is the first sentence.\",\n",
    "#     \"Here is another sentence.\",\n",
    "#     \"Yet another example sentence.\"\n",
    "# ]\n",
    "\n",
    "# 统计每个元素的 token 长度\n",
    "token_lengths = [len(tokenizer.encode(text, add_special_tokens=True)) for text in tqdm(prompt_opt['instruction'].to_list())]\n",
    "\n",
    "print(\"Token Lengths:\", max(token_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sum(np.array(token_lengths)>4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### semi-text-w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  11,  127,  824, 2427, 3342, 2321,  963,  319,   60,    9]),\n",
       " array([1606., 1766., 1926., 2086., 2246., 2406., 2566., 2726., 2886.,\n",
       "        3046., 3206.]))"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(token_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array(token_lengths)>4700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for semi_text_w,we add the labeled few-shot data, and the self-augmented and self-aligned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_text_w_train = pd.read_csv('/data/home/wangys/LLM_ER/process_data/semi-text-c/train_add.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semi_text_w_train\n",
    "dict_ltable = np.load('/data/home/wangys/LLM_ER/process_data/semi-text-c/dict_ltable.npy',allow_pickle=True).item()\n",
    "dict_rtable = np.load('/data/home/wangys/LLM_ER/process_data/semi-text-c/dict_ltable.npy',allow_pickle=True).item()\n",
    "semi_text_w = np.load('/data/home/wangys/LLM_ER/process_data/semi-text-c/semi-text-c.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_text_w_train.columns = ['ltable_id','rtable_id','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dict = {}\n",
    "for index,row in semi_text_w_train.iterrows():\n",
    "    ltable_id = row['ltable_id']\n",
    "    rtable_id = row['rtable_id']\n",
    "    left = str(dict_ltable[ltable_id])\n",
    "    if(not query_dict.__contains__(left)):\n",
    "        query_dict[left] = {}\n",
    "        query_dict[left]['pos'] = []\n",
    "        query_dict[left]['neg'] = []\n",
    "    if(row['label']==0):\n",
    "        tmp = query_dict[left]['neg']\n",
    "        tmp.append(str(dict_rtable[rtable_id]))\n",
    "        query_dict[left]['neg'] = tmp\n",
    "    else:\n",
    "        tmp = query_dict[left]['pos']\n",
    "        tmp.append(str(dict_rtable[rtable_id]))\n",
    "        query_dict[left]['pos'] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'/mpn': '[u16gx4m2a2666c16r,  cmu16gx4m2a266...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'/productID': '[csrlp2664]', '/mpn': '[cmk64g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'/productID': '[csr2616rl]', '/mpn': '[cmu16...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'/productID': '[csrlp2664]', '/mpn': '[cmk64g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>[{'/productID': '[843591072816]', 'category': ...</td>\n",
       "      <td>{'/productID': '[csrlp2664]', '/mpn': '[cmk64g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'/mpn': '[hpeh0146fawjb]', 'brand': '\"HP Ent...</td>\n",
       "      <td>[{'/sku': '[3ra3851aa5pack]', '/mpn': '[3ra385...</td>\n",
       "      <td>{'/sku': '[dh0072balwl2pack]', '/mpn': '[dh007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'/sku': '[dg0146famwl5pack]', '/mpn': '[dg01...</td>\n",
       "      <td>[{'/mpn': '[404712001,  404712001b]', '/gtin13...</td>\n",
       "      <td>{'/sku': '[dh0072balwl2pack]', '/mpn': '[dh007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3210</th>\n",
       "      <td>[]</td>\n",
       "      <td>[{'/mpn': '[hp456896001]', 'brand': 'HP', 'cat...</td>\n",
       "      <td>{'/sku': '[43194300310pack]', '/mpn': '[431943...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3211</th>\n",
       "      <td>[{'/mpn': '[483403b21]', 'brand': 'Genuine HPE...</td>\n",
       "      <td>[{'/sku': '[500670b212pack]', '/mpn': '[500670...</td>\n",
       "      <td>{'/mpn': '[432806b21]', 'brand': 'HP', 'catego...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3212</th>\n",
       "      <td>[{'/sku': '[470065235]', 'brand': 'Hewlett Pac...</td>\n",
       "      <td>[{'/mpn': '[356816001]', 'brand': 'Hewlett-Pac...</td>\n",
       "      <td>{'/sku': '[453475b21]', '/mpn': '[453475b21]',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3213</th>\n",
       "      <td>[]</td>\n",
       "      <td>[{'/sku': '[507825b21]', '/mpn': '[507825b21]'...</td>\n",
       "      <td>{'/sku': '[453475b21]', '/mpn': '[453475b21]',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3214</th>\n",
       "      <td>[{'/sku': '[408850b21]', '/mpn': '[408850b21]'...</td>\n",
       "      <td>[{'/sku': '[192106b21]', '/mpn': '[192106b21]'...</td>\n",
       "      <td>{'/sku': '[483403b21]', '/mpn': '[483403b21]',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3215 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    pos  \\\n",
       "0     [{'/mpn': '[u16gx4m2a2666c16r,  cmu16gx4m2a266...   \n",
       "1     [{'/productID': '[csr2616rl]', '/mpn': '[cmu16...   \n",
       "2                                                    []   \n",
       "3     [{'/mpn': '[hpeh0146fawjb]', 'brand': '\"HP Ent...   \n",
       "4     [{'/sku': '[dg0146famwl5pack]', '/mpn': '[dg01...   \n",
       "...                                                 ...   \n",
       "3210                                                 []   \n",
       "3211  [{'/mpn': '[483403b21]', 'brand': 'Genuine HPE...   \n",
       "3212  [{'/sku': '[470065235]', 'brand': 'Hewlett Pac...   \n",
       "3213                                                 []   \n",
       "3214  [{'/sku': '[408850b21]', '/mpn': '[408850b21]'...   \n",
       "\n",
       "                                                    neg  \\\n",
       "0                                                    []   \n",
       "1                                                    []   \n",
       "2     [{'/productID': '[843591072816]', 'category': ...   \n",
       "3     [{'/sku': '[3ra3851aa5pack]', '/mpn': '[3ra385...   \n",
       "4     [{'/mpn': '[404712001,  404712001b]', '/gtin13...   \n",
       "...                                                 ...   \n",
       "3210  [{'/mpn': '[hp456896001]', 'brand': 'HP', 'cat...   \n",
       "3211  [{'/sku': '[500670b212pack]', '/mpn': '[500670...   \n",
       "3212  [{'/mpn': '[356816001]', 'brand': 'Hewlett-Pac...   \n",
       "3213  [{'/sku': '[507825b21]', '/mpn': '[507825b21]'...   \n",
       "3214  [{'/sku': '[192106b21]', '/mpn': '[192106b21]'...   \n",
       "\n",
       "                                                  query  \n",
       "0     {'/productID': '[csrlp2664]', '/mpn': '[cmk64g...  \n",
       "1     {'/productID': '[csrlp2664]', '/mpn': '[cmk64g...  \n",
       "2     {'/productID': '[csrlp2664]', '/mpn': '[cmk64g...  \n",
       "3     {'/sku': '[dh0072balwl2pack]', '/mpn': '[dh007...  \n",
       "4     {'/sku': '[dh0072balwl2pack]', '/mpn': '[dh007...  \n",
       "...                                                 ...  \n",
       "3210  {'/sku': '[43194300310pack]', '/mpn': '[431943...  \n",
       "3211  {'/mpn': '[432806b21]', 'brand': 'HP', 'catego...  \n",
       "3212  {'/sku': '[453475b21]', '/mpn': '[453475b21]',...  \n",
       "3213  {'/sku': '[453475b21]', '/mpn': '[453475b21]',...  \n",
       "3214  {'/sku': '[483403b21]', '/mpn': '[483403b21]',...  \n",
       "\n",
       "[3215 rows x 3 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame(query_dict).T\n",
    "a['query'] = a.index\n",
    "a = a.reset_index(drop=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_output = a[(a['pos'].astype(str)!='[]') & (a['neg'].astype(str)!='[]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02934432029724121,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d477a52a38842b3a3fb1325047f828d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/wangys/LLM_ER/blocking/semi-text-c.jsonl\n"
     ]
    }
   ],
   "source": [
    "df = a_output\n",
    "# 打开一个文件以写入\n",
    "import json\n",
    "with open('/data/home/wangys/LLM_ER/blocking/semi-text-c.json', 'w', encoding='utf-8') as file:\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        # 对于每行，创建一个字典\n",
    "        data = {\n",
    "            \"query\": str(row[\"query\"]),\n",
    "            \"pos\": row[\"pos\"],\n",
    "            \"neg\": row[\"neg\"]\n",
    "        }\n",
    "\n",
    "        # 将字典转换为JSON字符串，并写入文件\n",
    "        file.write(json.dumps(data, ensure_ascii=False))\n",
    "        file.write('\\n')  # 每个JSON对象后添加换行符\n",
    "\n",
    "print(\"/data/home/wangys/LLM_ER/blocking/semi-text-c.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.024633407592773438,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "read text entity...",
       "rate": null,
       "total": 9234,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573a5b4d40054cac9ebbd7b0f6aec42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "read text entity...:   0%|          | 0/9234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "semi_text_w_left = pd.read_json('/data/home/wangys/LLM_ER/process_data/semi-text-w/semi-text-w/left.json')\n",
    "entities = []\n",
    "file_path = \"/data/home/wangys/LLM_ER/process_data/semi-text-w/semi-text-w/right.txt\"\n",
    "with open(file_path, \"r\",encoding='utf-8') as rd:\n",
    "    lines = rd.readlines()\n",
    "    for line in tqdm(lines, desc=\"read text entity...\"):\n",
    "        text = line.strip()\n",
    "        entities.append(text)\n",
    "semi_text_w_right = entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1984"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.Series(semi_text_w_right).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 4258/4258 [00:51<00:00, 83.42it/s]\n"
     ]
    }
   ],
   "source": [
    "max_length = 4096\n",
    "semi_text_w_train_gt = semi_text_w_train[semi_text_w_train['label']==1]\n",
    "def Prompt_Generalize_semi_text_w(row): ## 在train筛选ltable_id and rtbale_id中所有类似的pos/neg放进去，但是不能完全相同，如果没有pos，那么从ground truth选4个，总数维持8个\n",
    "    temp_dict = {}\n",
    "    temp_dict['Output'] = ''\n",
    "    RAG_text = ''\n",
    "    ltable_id = row['ltable_id']\n",
    "    rtable_id = row['rtable_id']\n",
    "    text = 'You are an expert in detecting if two text descriptions of entities refer to the same or different entities. You are well-known to examine the fine details and can find subtle matches or mismatches to help you arrive at the final judgement. \\n\\nJudge whether Entity 1 and Entity 2 are match or dismatch, and choose within the given Options.\\n\\nReturn in JSON format.\\n\\nOptions: [match,dismatch]\\n\\nOutput format example:%s\\n\\nEntity 1:%s\\n\\nEntity 2:%s\\n\\nTake these examples as reference:\\n\\n' % (json.dumps(temp_dict),json.dumps(str(row['l_entity'])),json.dumps(str(row['r_entity'])))\n",
    "    select_df = semi_text_w_train[(semi_text_w_train['ltable_id']==ltable_id) | (semi_text_w_train['rtable_id']==rtable_id)] ## 相同元素\n",
    "    select_df = select_df[~((select_df['ltable_id']==ltable_id) & (select_df['rtable_id']==rtable_id))] ## 排除自己\n",
    "    select_df_pos = select_df[select_df['label']==1]\n",
    "    select_df_neg = select_df[select_df['label']==0]\n",
    "    all_neg = semi_text_w_train[semi_text_w_train['label']==0]\n",
    "    # if(len(select_df_pos)>=3): ## Contain pos\n",
    "    #     for index,row_ in select_df_pos.sample(n=4,replace=False).iterrows():\n",
    "    #         print(index)\n",
    "    #         RAG_dict = {}\n",
    "    #         RAG_dict['Entity_1'] = str(row_['l_entity'])\n",
    "    #         RAG_dict['Entity_2'] = str(row_['r_entity'])\n",
    "    #         RAG_dict['Output'] = 'dismatch' if row_['label']==0 else 'match'\n",
    "    #         RAG_text += '%s\\n\\n' % json.dumps(RAG_dict)\n",
    "    # else:\n",
    "    if(len(select_df_neg)>=3):\n",
    "        df = pd.concat([semi_text_w_train_gt.sample(n=3,replace=False),select_df_neg.sample(n=3,replace=False)]).sample(frac=1)\n",
    "    else:\n",
    "        df = pd.concat([semi_text_w_train_gt.sample(n=3,replace=False),select_df_neg.sample(frac=1,replace=False),all_neg.sample(n=3-len(select_df_neg),replace=False)]).sample(frac=1)\n",
    "    RAG_text_bck = RAG_text\n",
    "    for index,row_ in df.iterrows():\n",
    "        \n",
    "        RAG_dict = {}\n",
    "        RAG_dict['Entity_1'] = str(row_['l_entity'])\n",
    "        RAG_dict['Entity_2'] = str(row_['r_entity'])\n",
    "        RAG_dict['Output'] = 'dismatch' if row_['label']==0 else 'match'\n",
    "        RAG_text_bck += '%s\\n\\n' % str(RAG_dict)\n",
    "        if(len(tokenizer.encode(RAG_text_bck, add_special_tokens=True))<max_length):\n",
    "            RAG_text += '%s\\n\\n' % str(RAG_dict)\n",
    "        else:\n",
    "            break\n",
    "    output_dict = {}\n",
    "    output_dict['Output'] = 'dismatch' if row['label']==0 else 'match'\n",
    "    return text + RAG_text, '' , str(output_dict)\n",
    "semi_text_w_output = semi_text_w_train.progress_apply(Prompt_Generalize_semi_text_w,axis=1,result_type='expand')    \n",
    "# semi_text_w_output_test = semi_text_w_test.progress_apply(Prompt_Generalize_semi_text_w,axis=1,result_type='expand')           \n",
    "       \n",
    "# amazon_google_output = amazon_google_test.progress_apply(Prompt_Generalize,axis=1,result_type='expand')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(semi_text_w_output.iloc[0,0])\n",
    "# semi_text_w_output.columns = ['instruction','input','output']\n",
    "semi_text_w_output_test.columns = ['instruction','input','output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We try RAG on semi-text-w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_text_c_train = pd.read_csv('/data/home/wangys/LLM_ER/process_data/semi-text-c/train_add.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_text_c_test = pd.read_csv('/data/home/wangys/LLM_ER/process_data/semi-text-c/semi-text-c/test.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_text_c_ltable_dict = np.load('/data/home/wangys/LLM_ER/process_data/semi-text-c/dict_ltable.npy',allow_pickle=True).item()\n",
    "semi_text_c_rtable_dict = np.load('/data/home/wangys/LLM_ER/process_data/semi-text-c/dict_rtable.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semi_text_c_train.columns = semi_text_w_train.columns[:3]\n",
    "semi_text_c_train['l_entity'] = semi_text_c_train['ltable_id'].map(semi_text_c_ltable_dict)\n",
    "semi_text_c_train['r_entity'] = semi_text_c_train['rtable_id'].map(semi_text_c_rtable_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_text_c_test.columns = semi_text_w_train.columns[:3]\n",
    "semi_text_c_test['l_entity'] = semi_text_c_test['ltable_id'].map(semi_text_c_ltable_dict)\n",
    "semi_text_c_test['r_entity'] = semi_text_c_test['rtable_id'].map(semi_text_c_rtable_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_text_c_train_ind = semi_text_c_train.iloc[:,-2:].astype(str).drop_duplicates().index\n",
    "semi_text_c_train = semi_text_c_train.loc[semi_text_c_train_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_text_c_train['index'] = semi_text_c_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 4179/4179 [02:19<00:00, 30.04it/s]\n"
     ]
    }
   ],
   "source": [
    "semi_text_c_train_gt = semi_text_c_train[semi_text_c_train['label']==1]\n",
    "def Prompt_Generalize_semi_text_c(row): ## 在train筛选ltable_id and rtbale_id中所有类似的pos/neg放进去，但是不能完全相同，如果没有pos，那么从ground truth选4个，总数维持8个\n",
    "    temp_dict = {}\n",
    "    temp_dict['Output'] = ''\n",
    "    RAG_text = ''\n",
    "    ltable_id = row['ltable_id']\n",
    "    rtable_id = row['rtable_id']\n",
    "    text = 'You are an expert in detecting if two text descriptions of entities refer to the same or different entities. You are well-known to examine the fine details and can find subtle matches or mismatches to help you arrive at the final judgement. \\n\\nJudge whether Entity 1 and Entity 2 are match or dismatch, and choose within the given Options.\\n\\nReturn in JSON format.\\n\\nOptions: [match,dismatch]\\n\\nOutput format example:%s\\n\\nEntity 1:%s\\n\\nEntity 2:%s\\n\\nTake these examples as reference:\\n\\n' % (json.dumps(temp_dict),json.dumps(str(row['l_entity'])),json.dumps(str(row['r_entity'])))\n",
    "    select_df = semi_text_c_train[(semi_text_c_train['ltable_id']==ltable_id) | (semi_text_c_train['rtable_id']==rtable_id)] ## 相同元素\n",
    "    select_df = select_df[~((select_df['ltable_id']==ltable_id) & (select_df['rtable_id']==rtable_id))] ## 排除自己\n",
    "    select_df_pos = select_df[select_df['label']==1]\n",
    "    select_df_neg = select_df[select_df['label']==0]\n",
    "    all_neg = semi_text_c_train[semi_text_c_train['label']==0]\n",
    "    # if(len(select_df_pos)>=3): ## Contain pos\n",
    "    #     for index,row_ in select_df_pos.sample(n=4,replace=False).iterrows():\n",
    "    #         print(index)\n",
    "    #         RAG_dict = {}\n",
    "    #         RAG_dict['Entity_1'] = str(row_['l_entity'])\n",
    "    #         RAG_dict['Entity_2'] = str(row_['r_entity'])\n",
    "    #         RAG_dict['Output'] = 'dismatch' if row_['label']==0 else 'match'\n",
    "    #         RAG_text += '%s\\n\\n' % json.dumps(RAG_dict)\n",
    "    # else:\n",
    "    if(len(select_df_neg)>=3):\n",
    "        df = pd.concat([semi_text_c_train_gt.sample(n=3,replace=False),select_df_neg.sample(n=3,replace=False)]).sample(frac=1)\n",
    "    else:\n",
    "        df = pd.concat([semi_text_c_train_gt.sample(n=3,replace=False),select_df_neg.sample(frac=1,replace=False),all_neg.sample(n=3-len(select_df_neg),replace=False)]).sample(frac=1)\n",
    "    RAG_text_bck = RAG_text\n",
    "    for index,row_ in df.iterrows():\n",
    "        \n",
    "        RAG_dict = {}\n",
    "        RAG_dict['Entity_1'] = str(row_['l_entity'])\n",
    "        RAG_dict['Entity_2'] = str(row_['r_entity'])\n",
    "        RAG_dict['Output'] = 'dismatch' if row_['label']==0 else 'match'\n",
    "        RAG_text_bck += '%s\\n\\n' % str(RAG_dict)\n",
    "        RAG_length = len(tokenizer.encode(RAG_text_bck, add_special_tokens=True))\n",
    "        if(<max_length):\n",
    "            RAG_text += '%s\\n\\n' % str(RAG_dict)\n",
    "        else:\n",
    "            # print(row['index'])\n",
    "            break\n",
    "    output_dict = {}\n",
    "    output_dict['Output'] = 'dismatch' if row['label']==0 else 'match'\n",
    "    return text + RAG_text, '' , str(output_dict)\n",
    "# semi_text_c_output = semi_text_c_train.progress_apply(Prompt_Generalize_semi_text_c,axis=1,result_type='expand')    \n",
    "semi_text_c_output_test = semi_text_c_test.progress_apply(Prompt_Generalize_semi_text_c,axis=1,result_type='expand')           \n",
    "       \n",
    "# amazon_google_output = amazon_google_test.progress_apply(Prompt_Generalize,axis=1,result_type='expand')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_text_c_output.columns = ['instruction','input','output']\n",
    "semi_text_c_output_test.columns = ['instruction','input','output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Walmart-Amazon from here\n",
    "walmart_amazon_train = pd.read_csv('/data/home/wangys/LLM_ER/process_data/walmart-amazon/train.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_amazon_test = pd.read_csv('/data/home/wangys/LLM_ER/process_data/walmart-amazon/test_align.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 2049/2049 [00:05<00:00, 348.23it/s]\n"
     ]
    }
   ],
   "source": [
    "walmart_amazon_train_gt = walmart_amazon_train[walmart_amazon_train['label']==1]\n",
    "def Prompt_Generalize_walmart_amazon(row): ## 在train筛选ltable_id and rtbale_id中所有类似的pos/neg放进去，但是不能完全相同，如果没有pos，那么从ground truth选4个，总数维持8个\n",
    "    temp_dict = {}\n",
    "    temp_dict['Output'] = ''\n",
    "    RAG_text = ''\n",
    "    ltable_id = row['ltable_id']\n",
    "    rtable_id = row['rtable_id']\n",
    "    text = 'You are an expert in detecting if two text descriptions of entities refer to the same or different entities. You are well-known to examine the fine details and can find subtle matches or mismatches to help you arrive at the final judgement. \\n\\nJudge whether Entity 1 and Entity 2 are match or dismatch, and choose within the given Options.\\n\\nReturn in JSON format.\\n\\nOptions: [match,dismatch]\\n\\nOutput format example:%s\\n\\nEntity 1:%s\\n\\nEntity 2:%s\\n\\nTake these examples as reference:\\n\\n' % (json.dumps(temp_dict),json.dumps(str(row['l_entity'])),json.dumps(str(row['r_entity'])))\n",
    "    select_df = walmart_amazon_train[(walmart_amazon_train['ltable_id']==ltable_id) | (walmart_amazon_train['rtable_id']==rtable_id)] ## 相同元素\n",
    "    select_df = select_df[~((select_df['ltable_id']==ltable_id) & (select_df['rtable_id']==rtable_id))] ## 排除自己\n",
    "    select_df_pos = select_df[select_df['label']==1]\n",
    "    select_df_neg = select_df[select_df['label']==0]\n",
    "    all_neg = walmart_amazon_train[walmart_amazon_train['label']==0]\n",
    "    # if(len(select_df_pos)>=3): ## Contain pos\n",
    "    #     for index,row_ in select_df_pos.sample(n=4,replace=False).iterrows():\n",
    "    #         print(index)\n",
    "    #         RAG_dict = {}\n",
    "    #         RAG_dict['Entity_1'] = str(row_['l_entity'])\n",
    "    #         RAG_dict['Entity_2'] = str(row_['r_entity'])\n",
    "    #         RAG_dict['Output'] = 'dismatch' if row_['label']==0 else 'match'\n",
    "    #         RAG_text += '%s\\n\\n' % json.dumps(RAG_dict)\n",
    "    # else:\n",
    "    if(len(select_df_neg)>=4):\n",
    "        df = pd.concat([walmart_amazon_train_gt.sample(n=3,replace=False),select_df_neg.sample(n=4,replace=False)]).sample(frac=1)\n",
    "    else:\n",
    "        df = pd.concat([walmart_amazon_train_gt.sample(n=4,replace=False),select_df_neg.sample(frac=1,replace=False),all_neg.sample(n=4-len(select_df_neg),replace=False)]).sample(frac=1)\n",
    "    for index,row_ in df.iterrows():\n",
    "        \n",
    "        RAG_dict = {}\n",
    "        RAG_dict['Entity_1'] = str(row_['l_entity'])\n",
    "        RAG_dict['Entity_2'] = str(row_['r_entity'])\n",
    "        RAG_dict['Output'] = 'dismatch' if row_['label']==0 else 'match'\n",
    "        # RAG_text_bck += '%s\\n\\n' % str(RAG_dict)\n",
    "        # RAG_length = len(tokenizer.encode(RAG_text_bck, add_special_tokens=True))\n",
    "        RAG_text += '%s\\n\\n' % str(RAG_dict)\n",
    "    output_dict = {}\n",
    "    output_dict['Output'] = 'dismatch' if row['label']==0 else 'match'\n",
    "    return text + RAG_text, '' , str(output_dict)\n",
    "# walmart_amazon_train_output = walmart_amazon_train.progress_apply(Prompt_Generalize_walmart_amazon,axis=1,result_type='expand') \n",
    "walmart_amazon_test_output = walmart_amazon_test.progress_apply(Prompt_Generalize_walmart_amazon,axis=1,result_type='expand')    \n",
    "# semi_text_c_output = semi_text_c_train.progress_apply(Prompt_Generalize_semi_text_c,axis=1,result_type='expand')    \n",
    "# semi_text_c_output_test = semi_text_c_test.progress_apply(Prompt_Generalize_semi_text_c,axis=1,result_type='expand')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "## wdc-all We try to concat multiple train files\n",
    "wdc_all_train = pd.read_csv('/data/home/wangys/LLM_ER/process_data/wdc-all/wdc_all/train.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdc_all_train_sample = pd.read_csv('/data/home/wangys/LLM_ER/process_data/wdc-all/train.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l_entity</th>\n",
       "      <th>r_entity</th>\n",
       "      <th>label</th>\n",
       "      <th>ltable_id</th>\n",
       "      <th>rtable_id</th>\n",
       "      <th>l_entity_diff</th>\n",
       "      <th>r_entity_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9038</th>\n",
       "      <td>{'title': '\"GoPro HDMI Cable\"@en Cable AHDMC-3...</td>\n",
       "      <td>{\\n  \"title\": \" \"GoPro Headstrap Plus Quickcli...</td>\n",
       "      <td>0</td>\n",
       "      <td>9038</td>\n",
       "      <td>9038</td>\n",
       "      <td>{\\n  \"title\": \" \"GoPro HDMI Cable\"@en Cable AH...</td>\n",
       "      <td>{\\n  \"title\": \" \"GoPro Headstrap Plus Quickcli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9039</th>\n",
       "      <td>{\\n  \"title\": \"Benq ZOWIE RL2455 24\\\" Full HD ...</td>\n",
       "      <td>{\\n  \"title\": \"Zowie RL2455 E-Sports 24\\\" Full...</td>\n",
       "      <td>1</td>\n",
       "      <td>9039</td>\n",
       "      <td>9039</td>\n",
       "      <td>{'title': '\"Benq ZOWIE RL2455 24\" Full HD TN G...</td>\n",
       "      <td>{\\n  \"title\": \"Zowie RL2455 E-Sports 24\\\" Full...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9040</th>\n",
       "      <td>{'title': '\" Mens Nixon The Crew Watch A1186-0...</td>\n",
       "      <td>{\\n  \"title\": \"\\\" Mens Nixon The Brigade Watch...</td>\n",
       "      <td>0</td>\n",
       "      <td>9040</td>\n",
       "      <td>9040</td>\n",
       "      <td>{\\n  \"title\": \" Mens Nixon The Crew Watch A118...</td>\n",
       "      <td>{\\n  \"title\": \" Mens Nixon The Brigade Watch A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9041</th>\n",
       "      <td>{'title': 'Nike Metcon 2 - Black/White/Wolf Gr...</td>\n",
       "      <td>{'title': '\"Nike Metcon DSX Flyknit - Wolf Gre...</td>\n",
       "      <td>0</td>\n",
       "      <td>9041</td>\n",
       "      <td>9041</td>\n",
       "      <td>{'title': '\"Nike Metcon 2 - Black/White/Wolf G...</td>\n",
       "      <td>{'title': '\"Nike Metcon DSX Flyknit - Wolf Gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9042</th>\n",
       "      <td>{'title': '\"PowerLine HD Day Night Cloud Camer...</td>\n",
       "      <td>{'title': 'HD DVR Surveillance System, 16 Infr...</td>\n",
       "      <td>0</td>\n",
       "      <td>9042</td>\n",
       "      <td>9042</td>\n",
       "      <td>{'title': 'PowerLine HD Day Night Cloud Camera...</td>\n",
       "      <td>{'title': 'HD DVR Surveillance System, 16 Infr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13431</th>\n",
       "      <td>{\\n  \"title\": \"ASUS GeForce GTX 1070 TURBO 8GB...</td>\n",
       "      <td>{\\n  \"title\": \" \"ASUS NVIDIA GeForce GTX 1070 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>13431</td>\n",
       "      <td>13431</td>\n",
       "      <td>{\\n  \"title\": \"ASUS GeForce GTX 1070 TURBO 8GB...</td>\n",
       "      <td>{'title': '\"ASUS NVIDIA GeForce GTX 1070 Turbo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13432</th>\n",
       "      <td>{'title': '\" Seiko Men\\'s Solar Powered Watch\"...</td>\n",
       "      <td>{'title': \"Seiko Astron Men's GPS Titanium Chr...</td>\n",
       "      <td>0</td>\n",
       "      <td>13432</td>\n",
       "      <td>13432</td>\n",
       "      <td>{'title': '\" Seiko Men\\'s Solar Powered Watch\"...</td>\n",
       "      <td>{'title': '\" Seiko Astron Men\\'s GPS Titanium ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13433</th>\n",
       "      <td>{'title': 'Fujifilm Instax Mini Film Twin Pack...</td>\n",
       "      <td>{'title': '\"Fujifilm Instax Mini 8 Instant Fil...</td>\n",
       "      <td>0</td>\n",
       "      <td>13433</td>\n",
       "      <td>13433</td>\n",
       "      <td>{'title': 'Fujifilm Instax Mini Film Twin Pack...</td>\n",
       "      <td>{'title': 'Fujifilm Instax Mini 8 Instant Film...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13434</th>\n",
       "      <td>{'title': '\"Skagen SKW6237 Holst mens watch\"'}</td>\n",
       "      <td>{\\n  \"title\": \" \" Mens Skagen Signatur Regulat...</td>\n",
       "      <td>0</td>\n",
       "      <td>13434</td>\n",
       "      <td>13434</td>\n",
       "      <td>{\\n  \"title\": \" \"Skagen SKW6237 Holst mens wat...</td>\n",
       "      <td>{\\n  \"title\": \" \" Mens Skagen Signatur Regulat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13435</th>\n",
       "      <td>{'title': 'Kingston Technology DataTraveler 10...</td>\n",
       "      <td>{'title': '\"Kingston Technology DataTraveler S...</td>\n",
       "      <td>0</td>\n",
       "      <td>13435</td>\n",
       "      <td>13435</td>\n",
       "      <td>{'title': '\"Kingston Technology DataTraveler 1...</td>\n",
       "      <td>{'title': '\"Kingston Technology DataTraveler S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4398 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                l_entity  \\\n",
       "9038   {'title': '\"GoPro HDMI Cable\"@en Cable AHDMC-3...   \n",
       "9039   {\\n  \"title\": \"Benq ZOWIE RL2455 24\\\" Full HD ...   \n",
       "9040   {'title': '\" Mens Nixon The Crew Watch A1186-0...   \n",
       "9041   {'title': 'Nike Metcon 2 - Black/White/Wolf Gr...   \n",
       "9042   {'title': '\"PowerLine HD Day Night Cloud Camer...   \n",
       "...                                                  ...   \n",
       "13431  {\\n  \"title\": \"ASUS GeForce GTX 1070 TURBO 8GB...   \n",
       "13432  {'title': '\" Seiko Men\\'s Solar Powered Watch\"...   \n",
       "13433  {'title': 'Fujifilm Instax Mini Film Twin Pack...   \n",
       "13434     {'title': '\"Skagen SKW6237 Holst mens watch\"'}   \n",
       "13435  {'title': 'Kingston Technology DataTraveler 10...   \n",
       "\n",
       "                                                r_entity  label  ltable_id  \\\n",
       "9038   {\\n  \"title\": \" \"GoPro Headstrap Plus Quickcli...      0       9038   \n",
       "9039   {\\n  \"title\": \"Zowie RL2455 E-Sports 24\\\" Full...      1       9039   \n",
       "9040   {\\n  \"title\": \"\\\" Mens Nixon The Brigade Watch...      0       9040   \n",
       "9041   {'title': '\"Nike Metcon DSX Flyknit - Wolf Gre...      0       9041   \n",
       "9042   {'title': 'HD DVR Surveillance System, 16 Infr...      0       9042   \n",
       "...                                                  ...    ...        ...   \n",
       "13431  {\\n  \"title\": \" \"ASUS NVIDIA GeForce GTX 1070 ...      1      13431   \n",
       "13432  {'title': \"Seiko Astron Men's GPS Titanium Chr...      0      13432   \n",
       "13433  {'title': '\"Fujifilm Instax Mini 8 Instant Fil...      0      13433   \n",
       "13434  {\\n  \"title\": \" \" Mens Skagen Signatur Regulat...      0      13434   \n",
       "13435  {'title': '\"Kingston Technology DataTraveler S...      0      13435   \n",
       "\n",
       "       rtable_id                                      l_entity_diff  \\\n",
       "9038        9038  {\\n  \"title\": \" \"GoPro HDMI Cable\"@en Cable AH...   \n",
       "9039        9039  {'title': '\"Benq ZOWIE RL2455 24\" Full HD TN G...   \n",
       "9040        9040  {\\n  \"title\": \" Mens Nixon The Crew Watch A118...   \n",
       "9041        9041  {'title': '\"Nike Metcon 2 - Black/White/Wolf G...   \n",
       "9042        9042  {'title': 'PowerLine HD Day Night Cloud Camera...   \n",
       "...          ...                                                ...   \n",
       "13431      13431  {\\n  \"title\": \"ASUS GeForce GTX 1070 TURBO 8GB...   \n",
       "13432      13432  {'title': '\" Seiko Men\\'s Solar Powered Watch\"...   \n",
       "13433      13433  {'title': 'Fujifilm Instax Mini Film Twin Pack...   \n",
       "13434      13434  {\\n  \"title\": \" \"Skagen SKW6237 Holst mens wat...   \n",
       "13435      13435  {'title': '\"Kingston Technology DataTraveler 1...   \n",
       "\n",
       "                                           r_entity_diff  \n",
       "9038   {\\n  \"title\": \" \"GoPro Headstrap Plus Quickcli...  \n",
       "9039   {\\n  \"title\": \"Zowie RL2455 E-Sports 24\\\" Full...  \n",
       "9040   {\\n  \"title\": \" Mens Nixon The Brigade Watch A...  \n",
       "9041   {'title': '\"Nike Metcon DSX Flyknit - Wolf Gre...  \n",
       "9042   {'title': 'HD DVR Surveillance System, 16 Infr...  \n",
       "...                                                  ...  \n",
       "13431  {'title': '\"ASUS NVIDIA GeForce GTX 1070 Turbo...  \n",
       "13432  {'title': '\" Seiko Astron Men\\'s GPS Titanium ...  \n",
       "13433  {'title': 'Fujifilm Instax Mini 8 Instant Film...  \n",
       "13434  {\\n  \"title\": \" \" Mens Skagen Signatur Regulat...  \n",
       "13435  {'title': '\"Kingston Technology DataTraveler S...  \n",
       "\n",
       "[4398 rows x 7 columns]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdc_all_test = pd.read_csv('/data/home/wangys/LLM_ER/process_data/wdc-all/wdc_all/dpo_test.csv',index_col=0)\n",
    "wdc_all_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdc_all_train = pd.concat([wdc_all_train,wdc_all_train_sample]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdc_all_train.to_csv('/data/home/wangys/LLM_ER/process_data/wdc-all/train_unique.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar:   0%|          | 0/4398 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 4398/4398 [00:14<00:00, 310.88it/s]\n"
     ]
    }
   ],
   "source": [
    "wdc_all_train_gt = wdc_all_train[wdc_all_train['label']==1]\n",
    "def Prompt_Generalize_wdc_all(row): ## 在train筛选ltable_id and rtbale_id中所有类似的pos/neg放进去，但是不能完全相同，如果没有pos，那么从ground truth选4个，总数维持8个\n",
    "    temp_dict = {}\n",
    "    temp_dict['Output'] = ''\n",
    "    RAG_text = ''\n",
    "    ltable_id = row['ltable_id']\n",
    "    rtable_id = row['rtable_id']\n",
    "    text = 'You are an expert in detecting if two text descriptions of entities refer to the same or different entities. You are well-known to examine the fine details and can find subtle matches or mismatches to help you arrive at the final judgement. \\n\\nJudge whether Entity 1 and Entity 2 are match or dismatch, and choose within the given Options.\\n\\nReturn in JSON format.\\n\\nOptions: [match,dismatch]\\n\\nOutput format example:%s\\n\\nEntity 1:%s\\n\\nEntity 2:%s\\n\\nTake these examples as reference:\\n\\n' % (json.dumps(temp_dict),json.dumps(str(row['l_entity'])),json.dumps(str(row['r_entity'])))\n",
    "    select_df = wdc_all_train[(wdc_all_train['ltable_id']==ltable_id) | (wdc_all_train['rtable_id']==rtable_id)] ## 相同元素\n",
    "    select_df = select_df[~((select_df['ltable_id']==ltable_id) & (select_df['rtable_id']==rtable_id))] ## 排除自己\n",
    "    select_df_pos = select_df[select_df['label']==1]\n",
    "    select_df_neg = select_df[select_df['label']==0]\n",
    "    all_neg = wdc_all_train[wdc_all_train['label']==0]\n",
    "    # if(len(select_df_pos)>=3): ## Contain pos\n",
    "    #     for index,row_ in select_df_pos.sample(n=4,replace=False).iterrows():\n",
    "    #         print(index)\n",
    "    #         RAG_dict = {}\n",
    "    #         RAG_dict['Entity_1'] = str(row_['l_entity'])\n",
    "    #         RAG_dict['Entity_2'] = str(row_['r_entity'])\n",
    "    #         RAG_dict['Output'] = 'dismatch' if row_['label']==0 else 'match'\n",
    "    #         RAG_text += '%s\\n\\n' % json.dumps(RAG_dict)\n",
    "    # else:\n",
    "    if(len(select_df_neg)>=4):\n",
    "        df = pd.concat([wdc_all_train_gt.sample(n=4,replace=False),select_df_neg.sample(n=4,replace=False)]).sample(frac=1)\n",
    "    else:\n",
    "        df = pd.concat([wdc_all_train_gt.sample(n=4,replace=False),select_df_neg.sample(frac=1,replace=False),all_neg.sample(n=4-len(select_df_neg),replace=False)]).sample(frac=1)\n",
    "    for index,row_ in df.iterrows():\n",
    "        \n",
    "        RAG_dict = {}\n",
    "        RAG_dict['Entity_1'] = str(row_['l_entity'])\n",
    "        RAG_dict['Entity_2'] = str(row_['r_entity'])\n",
    "        RAG_dict['Output'] = 'dismatch' if row_['label']==0 else 'match'\n",
    "        # RAG_text_bck += '%s\\n\\n' % str(RAG_dict)\n",
    "        # RAG_length = len(tokenizer.encode(RAG_text_bck, add_special_tokens=True))\n",
    "        RAG_text += '%s\\n\\n' % str(RAG_dict)\n",
    "    output_dict = {}\n",
    "    output_dict['Output'] = 'dismatch' if row['label']==0 else 'match'\n",
    "    return text + RAG_text, '' , str(output_dict)\n",
    "# wdc_all_train_output = wdc_all_train.progress_apply(Prompt_Generalize_wdc_all,axis=1,result_type='expand')\n",
    "wdc_all_test_output = wdc_all_test.progress_apply(Prompt_Generalize_wdc_all,axis=1,result_type='expand')\n",
    "\n",
    "# walmart_amazon_train_output = walmart_amazon_train.progress_apply(Prompt_Generalize_walmart_amazon,axis=1,result_type='expand') \n",
    "# walmart_amazon_test_output = walmart_amazon_test.progress_apply(Prompt_Generalize_walmart_amazon,axis=1,result_type='expand')    \n",
    "# semi_text_c_output = semi_text_c_train.progress_apply(Prompt_Generalize_semi_text_c,axis=1,result_type='expand')    \n",
    "# semi_text_c_output_test = semi_text_c_test.progress_apply(Prompt_Generalize_semi_text_c,axis=1,result_type='expand')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdc_all_train_output.columns = ['instruction','input','output']\n",
    "wdc_all_test_output.columns = ['instruction','input','output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ant_Buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "ant_buy_train = pd.read_csv('/data/home/wangys/LLM_ER/process_data/ant-buy/train.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "ant_buy_test = pd.read_csv('/data/home/wangys/LLM_ER/process_data/ant-buy/exp_data/dpo_test.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar:   0%|          | 0/1916 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 1916/1916 [00:05<00:00, 341.97it/s]\n"
     ]
    }
   ],
   "source": [
    "ant_buy_train_gt = ant_buy_train[ant_buy_train['label']==1]\n",
    "def Prompt_Generalize_ant_buy(row): ## 在train筛选ltable_id and rtbale_id中所有类似的pos/neg放进去，但是不能完全相同，如果没有pos，那么从ground truth选4个，总数维持8个\n",
    "    temp_dict = {}\n",
    "    temp_dict['Output'] = ''\n",
    "    RAG_text = ''\n",
    "    ltable_id = row['ltable_id']\n",
    "    rtable_id = row['rtable_id']\n",
    "    text = 'You are an expert in detecting if two text descriptions of entities refer to the same or different entities. You are well-known to examine the fine details and can find subtle matches or mismatches to help you arrive at the final judgement. \\n\\nJudge whether Entity 1 and Entity 2 are match or dismatch, and choose within the given Options.\\n\\nReturn in JSON format.\\n\\nOptions: [match,dismatch]\\n\\nOutput format example:%s\\n\\nEntity 1:%s\\n\\nEntity 2:%s\\n\\nTake these examples as reference:\\n\\n' % (json.dumps(temp_dict),json.dumps(str(row['l_entity'])),json.dumps(str(row['r_entity'])))\n",
    "    select_df = ant_buy_train[(ant_buy_train['ltable_id']==ltable_id) | (ant_buy_train['rtable_id']==rtable_id)] ## 相同元素\n",
    "    select_df = select_df[~((select_df['ltable_id']==ltable_id) & (select_df['rtable_id']==rtable_id))] ## 排除自己\n",
    "    select_df_pos = select_df[select_df['label']==1]\n",
    "    select_df_neg = select_df[select_df['label']==0]\n",
    "    all_neg = ant_buy_train[ant_buy_train['label']==0]\n",
    "    # if(len(select_df_pos)>=3): ## Contain pos\n",
    "    #     for index,row_ in select_df_pos.sample(n=4,replace=False).iterrows():\n",
    "    #         print(index)\n",
    "    #         RAG_dict = {}\n",
    "    #         RAG_dict['Entity_1'] = str(row_['l_entity'])\n",
    "    #         RAG_dict['Entity_2'] = str(row_['r_entity'])\n",
    "    #         RAG_dict['Output'] = 'dismatch' if row_['label']==0 else 'match'\n",
    "    #         RAG_text += '%s\\n\\n' % json.dumps(RAG_dict)\n",
    "    # else:\n",
    "    if(len(select_df_neg)>=4):\n",
    "        df = pd.concat([ant_buy_train_gt.sample(n=4,replace=False),select_df_neg.sample(n=4,replace=False)]).sample(frac=1)\n",
    "    else:\n",
    "        df = pd.concat([ant_buy_train_gt.sample(n=4,replace=False),select_df_neg.sample(frac=1,replace=False),all_neg.sample(n=4-len(select_df_neg),replace=False)]).sample(frac=1)\n",
    "    for index,row_ in df.iterrows():\n",
    "        \n",
    "        RAG_dict = {}\n",
    "        RAG_dict['Entity_1'] = str(row_['l_entity'])\n",
    "        RAG_dict['Entity_2'] = str(row_['r_entity'])\n",
    "        RAG_dict['Output'] = 'dismatch' if row_['label']==0 else 'match'\n",
    "        # RAG_text_bck += '%s\\n\\n' % str(RAG_dict)\n",
    "        # RAG_length = len(tokenizer.encode(RAG_text_bck, add_special_tokens=True))\n",
    "        RAG_text += '%s\\n\\n' % str(RAG_dict)\n",
    "    output_dict = {}\n",
    "    output_dict['Output'] = 'dismatch' if row['label']==0 else 'match'\n",
    "    return text + RAG_text, '' , str(output_dict)\n",
    "# ant_buy_train_output = ant_buy_train.progress_apply(Prompt_Generalize_ant_buy,axis=1,result_type='expand')\n",
    "ant_buy_test_output = ant_buy_test.progress_apply(Prompt_Generalize_ant_buy,axis=1,result_type='expand')\n",
    "# wdc_all_test_output = wdc_all_test.progress_apply(Prompt_Generalize_wdc_all,axis=1,result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "ant_buy_train_output.columns = ['instruction','input','output']\n",
    "ant_buy_test_output.columns = ['instruction','input','output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema_Matching Dataset Process\n",
    "### Use the same template as Entity Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMS = pd.read_excel('/data/home/wangys/LLM_ER/process_data/SM/omop_cms_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic = pd.read_excel('/data/home/wangys/LLM_ER/process_data/SM/omop_mimic_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthea = pd.read_excel('/data/home/wangys/LLM_ER/process_data/SM/omop_synthea_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMS = synthea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMS['tableA'] = CMS['omop'] + '|' + CMS['des1'] \n",
    "CMS['tableB'] = CMS['table'] + '|' + CMS['des2'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc37ec5fe8564b09a1be41780b13cfad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87a7add3baa46bb9deeb647b160cc67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CMS_tableA = list(CMS['tableA'].unique())\n",
    "CMS_tableB = list(CMS['tableB'].unique())\n",
    "CMS_tableA_dict = {}\n",
    "CMS_tableB_dict = {}\n",
    "for i in tqdm(range(len(CMS_tableA))):\n",
    "    CMS_tableA_dict[CMS_tableA[i]] = i\n",
    "for i in tqdm(range(len(CMS_tableB))):\n",
    "    CMS_tableB_dict[CMS_tableB[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMS_tableA_dict_rev = {v: k for k, v in CMS_tableA_dict.items()}\n",
    "CMS_tableB_dict_rev = {v: k for k, v in CMS_tableB_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(CMS['table'].unique())\n",
    "CMS['tableA_id'] = CMS['tableA'].map(CMS_tableA_dict)\n",
    "CMS['tableB_id'] = CMS['tableB'].map(CMS_tableB_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMS_shuffle = CMS.sample(frac=1)\n",
    "CMS_train = CMS_shuffle.iloc[:int(len(CMS_shuffle)*0.8)]\n",
    "CMS_test = CMS_shuffle.iloc[int(len(CMS_shuffle)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMS_train_few = CMS_train.sample(frac=0.125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMS_train_few.to_csv('/data/home/wangys/LLM_ER/process_data/SM/synthea_train_few.csv')\n",
    "CMS_train.to_csv('/data/home/wangys/LLM_ER/process_data/SM/synthea_train.csv')\n",
    "CMS_test.to_csv('/data/home/wangys/LLM_ER/process_data/SM/synthea_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMS_train_few = pd.read_csv('/data/home/wangys/LLM_ER/process_data/SM/synthea_train_few.csv',index_col=0)\n",
    "CMS_train = pd.read_csv('/data/home/wangys/LLM_ER/process_data/SM/synthea_train.csv',index_col=0)\n",
    "CMS_test = pd.read_csv('/data/home/wangys/LLM_ER/process_data/SM/synthea_test.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ecaca5921f44d33a22ca1bef5dcc6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2007539/2076948446.py:15: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  pos = list(CMS_train[(CMS_train['tableA']==query) & (CMS['label']==1)]['tableB'].unique()) ## List All Positive with same query\n"
     ]
    }
   ],
   "source": [
    "# CMS_train_few\n",
    "## Train Sentence_BERT\n",
    "### For Few-shot \n",
    "query_list = []\n",
    "# for index,row in CMS_train_few.iterrows():\n",
    "#     query = row['tableA']\n",
    "#     pos = list(CMS[(CMS['tableA']==query) & (CMS['label']==1)]['tableB'].unique())\n",
    "#     neg = list(CMS_train_few[(CMS_train_few['tableA']==query)& (CMS_train_few['label']==0)]['tableB'].unique())\n",
    "#     if(pos!=[]):\n",
    "#         query_list.append([query,pos,neg])\n",
    "### For All\n",
    "# for index,row in tqdm(CMS_train.iterrows()):\n",
    "for query in tqdm(CMS_train['tableA'].unique()):\n",
    "    # query = row['tableA']\n",
    "    pos = list(CMS_train[(CMS_train['tableA']==query) & (CMS['label']==1)]['tableB'].unique()) ## List All Positive with same query\n",
    "    neg = list(CMS_train[(CMS_train['tableA']==query)& (CMS_train['label']==0)]['tableB'].unique())\n",
    "    # if(pos!=[]):\n",
    "    query_list.append([query,pos,neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMS_pos = CMS[CMS['label']==1]\n",
    "# CMS_pos = CMS_train[CMS_train['label']==1]\n",
    "for i in range(len(CMS_tableB_dict)):\n",
    "# for i in CMS_test[CMS_test['label']==1]['tableA_id'].unique():\n",
    "    length = len(CMS_pos[(CMS_pos['tableB_id']==i)])\n",
    "    # if(length>1):\n",
    "    print(length,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>death-person_id|the death domain contains the ...</td>\n",
       "      <td>['outpatientclaims-desynpuf_id|outpatientclaim...</td>\n",
       "      <td>['outpatientclaims-clm_id|outpatientclaims per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>procedure_occurrence-procedure_occurrence_id|t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['outpatientclaims-admtng_icd9_dgns_cd|outpati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>note-note_date|the note table captures unstruc...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['beneficiarysummary-bene_esrd_ind|beneficiary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>person-birth_datetime|the person domain contai...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['carrierclaims-prf_physn_npi|carrierclaims pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>device_exposure-device_source_concept_id|the d...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['outpatientclaims-at_physn_npi|outpatientclai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>measurement-measurement_time|the measurement t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['outpatientclaims-icd9_prcdr_cd|outpatientcla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>measurement-measurement_source_concept_id|the ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['beneficiarysummary-sp_chf|beneficiarysummary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>payer_plan_period-plan_source_value|the payer_...</td>\n",
       "      <td>['beneficiarysummary-bene_hmo_cvrage_tot_mons|...</td>\n",
       "      <td>['outpatientclaims-desynpuf_id|outpatientclaim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>measurement-operator_concept_id|the measuremen...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['inpatientclaims-hcpcs_cd|inpatientclaims per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>person-race_concept_id|the person domain conta...</td>\n",
       "      <td>['beneficiarysummary-bene_race_cd|beneficiarys...</td>\n",
       "      <td>['outpatientclaims-nch_bene_ptb_coinsrnc_amt|o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>267 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  \\\n",
       "0    death-person_id|the death domain contains the ...   \n",
       "1    procedure_occurrence-procedure_occurrence_id|t...   \n",
       "2    note-note_date|the note table captures unstruc...   \n",
       "3    person-birth_datetime|the person domain contai...   \n",
       "4    device_exposure-device_source_concept_id|the d...   \n",
       "..                                                 ...   \n",
       "262  measurement-measurement_time|the measurement t...   \n",
       "263  measurement-measurement_source_concept_id|the ...   \n",
       "264  payer_plan_period-plan_source_value|the payer_...   \n",
       "265  measurement-operator_concept_id|the measuremen...   \n",
       "266  person-race_concept_id|the person domain conta...   \n",
       "\n",
       "                                                     1  \\\n",
       "0    ['outpatientclaims-desynpuf_id|outpatientclaim...   \n",
       "1                                                   []   \n",
       "2                                                   []   \n",
       "3                                                   []   \n",
       "4                                                   []   \n",
       "..                                                 ...   \n",
       "262                                                 []   \n",
       "263                                                 []   \n",
       "264  ['beneficiarysummary-bene_hmo_cvrage_tot_mons|...   \n",
       "265                                                 []   \n",
       "266  ['beneficiarysummary-bene_race_cd|beneficiarys...   \n",
       "\n",
       "                                                     2  \n",
       "0    ['outpatientclaims-clm_id|outpatientclaims per...  \n",
       "1    ['outpatientclaims-admtng_icd9_dgns_cd|outpati...  \n",
       "2    ['beneficiarysummary-bene_esrd_ind|beneficiary...  \n",
       "3    ['carrierclaims-prf_physn_npi|carrierclaims pe...  \n",
       "4    ['outpatientclaims-at_physn_npi|outpatientclai...  \n",
       "..                                                 ...  \n",
       "262  ['outpatientclaims-icd9_prcdr_cd|outpatientcla...  \n",
       "263  ['beneficiarysummary-sp_chf|beneficiarysummary...  \n",
       "264  ['outpatientclaims-desynpuf_id|outpatientclaim...  \n",
       "265  ['inpatientclaims-hcpcs_cd|inpatientclaims per...  \n",
       "266  ['outpatientclaims-nch_bene_ptb_coinsrnc_amt|o...  \n",
       "\n",
       "[267 rows x 3 columns]"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_list_pd = pd.DataFrame(query_list)\n",
    "query_list_pd.astype(str).drop_duplicates()\n",
    "# query_list_pd[query_list_pd[1].astype(str)!='[]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMS_train_few['ltable_id'] = CMS_train_few['tableA_id']\n",
    "CMS_train_few['rtable_id'] = CMS_train_few['tableB_id']\n",
    "CMS_train['ltable_id'] = CMS_train['tableA_id']\n",
    "CMS_train['rtable_id'] = CMS_train['tableB_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMS_test['ltable_id'] = CMS_test['tableA_id']\n",
    "CMS_test['rtable_id'] = CMS_test['tableB_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 64 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CMS_train_few_gt = CMS_train_few[CMS_train_few['label']==1]\n",
    "CMS_train_few_gt = CMS_train[CMS_train['label']==1]\n",
    "CMS_train_few = CMS_train\n",
    "def Prompt_Generalize_CMS(row): ## 在train筛选ltable_id and rtbale_id中所有类似的pos/neg放进去，但是不能完全相同，如果没有pos，那么从ground truth选4个，总数维持8个\n",
    "    temp_dict = {}\n",
    "    temp_dict['Output'] = ''\n",
    "    RAG_text = ''\n",
    "    col1_dict = {}\n",
    "    col1_dict['column_name'] = row['tableA'].split('|')[0]\n",
    "    col1_dict['description'] = row['tableA'].split('|')[1]\n",
    "    col2_dict = {}\n",
    "    col2_dict['column_name'] = row['tableB'].split('|')[0]\n",
    "    col2_dict['description'] = row['tableB'].split('|')[1]\n",
    "    ltable_id = row['ltable_id']\n",
    "    rtable_id = row['rtable_id']\n",
    "    text = 'You are an expert in detecting if two columns in tables refer to the same or different schema. You are well-known to examine the fine details and can find subtle matches or mismatches to help you arrive at the final judgement. \\n\\nJudge whether Column 1 and Column 2 are match or dismatch, and choose within the given Options.\\n\\nReturn in JSON format.\\n\\nOptions: [match,dismatch]\\n\\nOutput format example:%s\\n\\nColumn 1:%s\\n\\nColumn 2:%s\\n\\nTake these examples as reference:\\n\\n' % (json.dumps(temp_dict),json.dumps(str(col1_dict)),json.dumps(str(col2_dict)))\n",
    "    select_df = CMS_train_few_gt[(CMS_train_few_gt['ltable_id']==ltable_id) | (CMS_train_few_gt['rtable_id']==rtable_id)] ## 相同元素\n",
    "    select_df = select_df[~((select_df['ltable_id']==ltable_id) & (select_df['rtable_id']==rtable_id))] ## 排除自己\n",
    "    select_df_pos = select_df[select_df['label']==1]\n",
    "    select_df_neg = select_df[select_df['label']==0]\n",
    "    all_neg = CMS_train_few[CMS_train_few['label']==0]\n",
    "    # if(len(select_df_pos)>=3): ## Contain pos\n",
    "    #     for index,row_ in select_df_pos.sample(n=4,replace=False).iterrows():\n",
    "    #         print(index)\n",
    "    #         RAG_dict = {}\n",
    "    #         RAG_dict['Entity_1'] = str(row_['l_entity'])\n",
    "    #         RAG_dict['Entity_2'] = str(row_['r_entity'])\n",
    "    #         RAG_dict['Output'] = 'dismatch' if row_['label']==0 else 'match'\n",
    "    #         RAG_text += '%s\\n\\n' % json.dumps(RAG_dict)\n",
    "    # else:\n",
    "    if(len(select_df_neg)>=4):\n",
    "        df = pd.concat([CMS_train_few_gt.sample(n=2,replace=False),select_df_neg.sample(n=4,replace=False)]).sample(frac=1)\n",
    "    else:\n",
    "        df = pd.concat([CMS_train_few_gt.sample(n=2,replace=False),select_df_neg.sample(frac=1,replace=False),all_neg.sample(n=4-len(select_df_neg),replace=False)]).sample(frac=1)\n",
    "    for index,row_ in df.iterrows():\n",
    "        col1_dict = {}\n",
    "        col1_dict['column_name'] = row_['tableA'].split('|')[0]\n",
    "        col1_dict['description'] = row_['tableA'].split('|')[1]\n",
    "        col2_dict = {}\n",
    "        col2_dict['column_name'] = row_['tableB'].split('|')[0]\n",
    "        col2_dict['description'] = row_['tableB'].split('|')[1]\n",
    "        ltable_id = row['ltable_id']\n",
    "        rtable_id = row['rtable_id']\n",
    "        RAG_dict = {}\n",
    "        RAG_dict['Column 1'] = str(col1_dict)\n",
    "        RAG_dict['Column 2'] = str(col2_dict)\n",
    "        RAG_dict['Output'] = 'dismatch' if row_['label']==0 else 'match'\n",
    "        # RAG_text_bck += '%s\\n\\n' % str(RAG_dict)\n",
    "        # RAG_length = len(tokenizer.encode(RAG_text_bck, add_special_tokens=True))\n",
    "        RAG_text += '%s\\n\\n' % str(RAG_dict)\n",
    "    output_dict = {}\n",
    "    output_dict['Output'] = 'dismatch' if row['label']==0 else 'match'\n",
    "    # return text + RAG_text, '' , str(output_dict)\n",
    "    return text + RAG_text, '' , str(output_dict), row['ltable_id'], row['rtable_id']\n",
    "# CMS_train_few_output = CMS_train_few.progress_apply(Prompt_Generalize_CMS,axis=1,result_type='expand')\n",
    "# CMS_test_few_output = CMS_test.progress_apply(Prompt_Generalize_CMS,axis=1,result_type='expand')\n",
    "CMS_train_output = CMS_train.parallel_apply(Prompt_Generalize_CMS,axis=1,result_type='expand')\n",
    "CMS_test_output = CMS_test.progress_apply(Prompt_Generalize_CMS,axis=1,result_type='expand')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMS_train_output_pos = CMS_train_output[CMS_train_output[2]==\"{'Output': 'match'}\"]\n",
    "CMS_test_output_pos = CMS_test_output[CMS_test_output[2]==\"{'Output': 'match'}\"]\n",
    "CMS_train_output_select = CMS_train_output[(CMS_train_output[3].isin(CMS_test_output_pos[3].unique())) | (CMS_train_output[4].isin(CMS_test_output_pos[4].unique()))]\n",
    "CMS_train_output_select = pd.concat([CMS_train_output_select,CMS_train_output_pos]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27879</th>\n",
       "      <td>You are an expert in detecting if two columns ...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'dismatch'}</td>\n",
       "      <td>251</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25394</th>\n",
       "      <td>You are an expert in detecting if two columns ...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'dismatch'}</td>\n",
       "      <td>228</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>You are an expert in detecting if two columns ...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'dismatch'}</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19854</th>\n",
       "      <td>You are an expert in detecting if two columns ...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'dismatch'}</td>\n",
       "      <td>178</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7674</th>\n",
       "      <td>You are an expert in detecting if two columns ...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'dismatch'}</td>\n",
       "      <td>69</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8418</th>\n",
       "      <td>You are an expert in detecting if two columns ...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'match'}</td>\n",
       "      <td>75</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18664</th>\n",
       "      <td>You are an expert in detecting if two columns ...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'match'}</td>\n",
       "      <td>168</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>You are an expert in detecting if two columns ...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'match'}</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19252</th>\n",
       "      <td>You are an expert in detecting if two columns ...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'match'}</td>\n",
       "      <td>173</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7939</th>\n",
       "      <td>You are an expert in detecting if two columns ...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'match'}</td>\n",
       "      <td>71</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5115 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0 1   \\\n",
       "27879  You are an expert in detecting if two columns ...      \n",
       "25394  You are an expert in detecting if two columns ...      \n",
       "370    You are an expert in detecting if two columns ...      \n",
       "19854  You are an expert in detecting if two columns ...      \n",
       "7674   You are an expert in detecting if two columns ...      \n",
       "...                                                  ... ..   \n",
       "8418   You are an expert in detecting if two columns ...      \n",
       "18664  You are an expert in detecting if two columns ...      \n",
       "2687   You are an expert in detecting if two columns ...      \n",
       "19252  You are an expert in detecting if two columns ...      \n",
       "7939   You are an expert in detecting if two columns ...      \n",
       "\n",
       "                            2    3   4  \n",
       "27879  {'Output': 'dismatch'}  251  18  \n",
       "25394  {'Output': 'dismatch'}  228  86  \n",
       "370    {'Output': 'dismatch'}    3  37  \n",
       "19854  {'Output': 'dismatch'}  178  96  \n",
       "7674   {'Output': 'dismatch'}   69  15  \n",
       "...                       ...  ...  ..  \n",
       "8418      {'Output': 'match'}   75  93  \n",
       "18664     {'Output': 'match'}  168  16  \n",
       "2687      {'Output': 'match'}   24  23  \n",
       "19252     {'Output': 'match'}  173  49  \n",
       "7939      {'Output': 'match'}   71  58  \n",
       "\n",
       "[5115 rows x 5 columns]"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CMS_train_output_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMS_train_few_output.columns = ['instruction','input','output']\n",
    "CMS_test_few_output.columns = ['instruction','input','output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMS_train_output_select.columns = ['instruction','input','output','ltable_id','rtable_id']\n",
    "CMS_test_output.columns = ['instruction','input','output','ltable_id','rtable_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = CMS_test_output.iloc[:,:3]\n",
    "json.dump(df.to_dict(orient='records'), open('/data/home/wangys/LLaMA-Factory-main/data/MoE/SM/synthea_test_output.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "md5: 39d1f0f652a90a322cc5de4a62b29eb3\n",
      "sha1: e7a9f54bb67d426530703c5937ecb7dfbd591378\n",
      "sha256: ee55372343cd7c3e8d92fd07ab45a74bc62a0a34dc4715f39e4c108d970ff6e8\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def encrypt(fpath: str, algorithm: str) -> str:\n",
    "    with open(fpath, 'rb') as f:\n",
    "        return hashlib.new(algorithm, f.read()).hexdigest()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for algorithm in ('md5', 'sha1', 'sha256'):\n",
    "        hexdigest = encrypt('/data/home/wangys/LLaMA-Factory-main/data/MoE/SM/synthea_train_output.json', algorithm)\n",
    "        print(f'{algorithm}: {hexdigest}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge and Shuffle\n",
    "amazon_google = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/ER/amazon-google-train.json')\n",
    "ant_buy = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/ER/ant_buy_train_output.json')\n",
    "semi_text_c = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/ER/semi-text-c-train-MoE.json')\n",
    "semi_text_w = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/ER/semi-text-w-train-MoE.json')\n",
    "walmart_amazon = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/ER/walmart_amazon_train_output.json')\n",
    "wdc_all = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/ER/wdc_all_train_output.json')\n",
    "hospital_train = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/hospital-train-MoE.json')\n",
    "beer_train = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/beer-train-MoE.json')\n",
    "rayyan_train = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/rayyan-train-MoE.json')\n",
    "RE = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/RE/RE-train_t=4.json')\n",
    "CMS_train = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/SM/CMS_train_output.json')\n",
    "mimic_train = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/SM/mimic_train_output.json')\n",
    "synthea_train = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/SM/synthea_train_output.json')\n",
    "walmart_train = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/DI/walmart_train_output_wide.json')\n",
    "amazon_train = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/DI/amazon_train_output_wide.json')\n",
    "restaurant_train = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/DI/restaurant_train_output_wide.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_df_input = [amazon_google,ant_buy,semi_text_c,semi_text_w,walmart_amazon,wdc_all,CMS_train,mimic_train,synthea_train]\n",
    "merge_df_input = [RE,walmart_train,amazon_train,restaurant_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def merge_dataframes(dataframes):\n",
    "    merged_dataframes = []\n",
    "\n",
    "    for i, df in enumerate(dataframes):\n",
    "        # 创建一个空的DataFrame用于累积合并的结果\n",
    "        cumulative_merge = pd.DataFrame()\n",
    "\n",
    "        for j, other_df in enumerate(dataframes):\n",
    "            if i != j:\n",
    "                # 根据output列中的元素种类执行不同的合并策略\n",
    "                if other_df['output'].nunique() == 2:\n",
    "                    # 执行原始策略\n",
    "                    dismatch_rows = other_df[other_df['output'].str.contains('dismatch', na=False)]\n",
    "                    non_dismatch_rows = other_df[~other_df['output'].str.contains('dismatch', na=False)]\n",
    "\n",
    "                    selected_dismatch = dismatch_rows.sample(n=min(70, len(dismatch_rows)), replace=True)\n",
    "                    selected_non_dismatch = non_dismatch_rows.sample(n=min(210, len(non_dismatch_rows)), replace=True)\n",
    "\n",
    "                    merge_result = pd.concat([selected_dismatch, selected_non_dismatch])\n",
    "                else:\n",
    "                    # 随机选择150行数据\n",
    "                    selected_rows = other_df.sample(n=min(150, len(other_df)), replace=True)\n",
    "                    merge_result = selected_rows\n",
    "\n",
    "                # 累积合并结果\n",
    "                cumulative_merge = pd.concat([cumulative_merge, merge_result])\n",
    "\n",
    "        # 将累积的合并结果与原始DataFrame合并\n",
    "        final_merged_df = pd.concat([df, cumulative_merge])\n",
    "\n",
    "        # 将最终合并的DataFrame添加到结果列表中\n",
    "        merged_dataframes.append(final_merged_df)\n",
    "\n",
    "    return merged_dataframes\n",
    "\n",
    "merge_df_output = merge_dataframes(merge_df_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(dataframes):\n",
    "    merged_dataframes = []\n",
    "\n",
    "    for df in dataframes:\n",
    "        df.columns =['instruction','input','output']\n",
    "        # 检查output列中元素的种类数量\n",
    "        if df['output'].nunique() == 2:\n",
    "            # 如果只有两种元素，执行原始策略\n",
    "            dismatch_rows = df[df['output'].str.contains('dismatch', na=False)]\n",
    "            non_dismatch_rows = df[~df['output'].str.contains('dismatch', na=False)]\n",
    "\n",
    "            selected_dismatch = dismatch_rows.sample(n=min(80, len(dismatch_rows)), replace=True)\n",
    "            selected_non_dismatch = non_dismatch_rows.sample(n=min(240, len(non_dismatch_rows)), replace=True)\n",
    "\n",
    "            merged_df = pd.concat([df, selected_dismatch, selected_non_dismatch])\n",
    "        else:\n",
    "            # 如果有多于两种元素，随机选择150行数据\n",
    "            selected_rows = df.sample(n=min(100, len(df)), replace=True)\n",
    "            merged_df = pd.concat([df, selected_rows])\n",
    "\n",
    "        # 将合并后的数据添加到新列表中\n",
    "        merged_dataframes.append(merged_df)\n",
    "\n",
    "    return merged_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_var_name(obj):\n",
    "    for var_name in globals():\n",
    "        if globals()[var_name] is obj:\n",
    "            return var_name\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SHA1_cal(path):\n",
    "    hexdigest = encrypt(path, 'sha1')\n",
    "    return hexdigest\n",
    "# SHA1_cal('/data/home/wangys/LLaMA-Factory-main/data/MoE/ER/ant_buy_train_output.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RE\n",
      "walmart_train\n",
      "amazon_train\n",
      "restaurant_train\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "dataset_info = {}\n",
    "for i in range(len(merge_df_output)):\n",
    "# for i in [6,7,8]:\n",
    "    df = merge_df_output[i]\n",
    "    name = find_var_name(merge_df_input[i])\n",
    "    print(name)\n",
    "    path_save = '/data/home/wangys/LLaMA-Factory-main/data/MoE/add/%s.json' % name\n",
    "    json.dump(df.to_dict(orient='records'), open(path_save, 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "    name += '-MoE-Add'\n",
    "    dataset_info[name] = {}\n",
    "    dataset_info[name][\"file_name\"] = path_save.replace('/data/home/wangys/LLaMA-Factory-main/data/','')\n",
    "    dataset_info[name][\"file_sha1\"] = SHA1_cal(path_save)\n",
    "    dataset_info[name][\"columns\"] = {\n",
    "        \"prompt\": \"instruction\",\n",
    "        \"query\": \"input\",\n",
    "        \"response\": \"output\"\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in attribute extraction from...</td>\n",
       "      <td></td>\n",
       "      <td>{\"city\": \"` los angeles '\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in attribute extraction from...</td>\n",
       "      <td></td>\n",
       "      <td>{\"city\": \"` studio city '\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in attribute extraction from...</td>\n",
       "      <td></td>\n",
       "      <td>{\"city\": \"` bel air '\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in attribute extraction from...</td>\n",
       "      <td></td>\n",
       "      <td>{\"city\": \"` sherman oaks '\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in attribute extraction from...</td>\n",
       "      <td></td>\n",
       "      <td>{\"city\": \"` los angeles '\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4218</th>\n",
       "      <td>You are an expert in attribute extraction from...</td>\n",
       "      <td></td>\n",
       "      <td>{'category': 'projection screens'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109</th>\n",
       "      <td>You are an expert in attribute extraction from...</td>\n",
       "      <td></td>\n",
       "      <td>{'category': 'mp3 players'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4278</th>\n",
       "      <td>You are an expert in attribute extraction from...</td>\n",
       "      <td></td>\n",
       "      <td>{'brand': 'griffin technology'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6509</th>\n",
       "      <td>You are an expert in attribute extraction from...</td>\n",
       "      <td></td>\n",
       "      <td>{'category': 'memory cards'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7645</th>\n",
       "      <td>You are an expert in attribute extraction from...</td>\n",
       "      <td></td>\n",
       "      <td>{'brand': 'metra'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1016 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            instruction input  \\\n",
       "0     You are an expert in attribute extraction from...         \n",
       "1     You are an expert in attribute extraction from...         \n",
       "2     You are an expert in attribute extraction from...         \n",
       "3     You are an expert in attribute extraction from...         \n",
       "4     You are an expert in attribute extraction from...         \n",
       "...                                                 ...   ...   \n",
       "4218  You are an expert in attribute extraction from...         \n",
       "2109  You are an expert in attribute extraction from...         \n",
       "4278  You are an expert in attribute extraction from...         \n",
       "6509  You are an expert in attribute extraction from...         \n",
       "7645  You are an expert in attribute extraction from...         \n",
       "\n",
       "                                  output  \n",
       "0            {\"city\": \"` los angeles '\"}  \n",
       "1            {\"city\": \"` studio city '\"}  \n",
       "2                {\"city\": \"` bel air '\"}  \n",
       "3           {\"city\": \"` sherman oaks '\"}  \n",
       "4            {\"city\": \"` los angeles '\"}  \n",
       "...                                  ...  \n",
       "4218  {'category': 'projection screens'}  \n",
       "2109         {'category': 'mp3 players'}  \n",
       "4278     {'brand': 'griffin technology'}  \n",
       "6509        {'category': 'memory cards'}  \n",
       "7645                  {'brand': 'metra'}  \n",
       "\n",
       "[1016 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df_output[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"RE-MoE-Add\": {\"file_name\": \"MoE/add/RE.json\", \"file_sha1\": \"b759b90cb04b77c53a72034fcf9c4960df549667\", \"columns\": {\"prompt\": \"instruction\", \"query\": \"input\", \"response\": \"output\"}}, \"walmart_train-MoE-Add\": {\"file_name\": \"MoE/add/walmart_train.json\", \"file_sha1\": \"771a5cadb52ce4f85b111db204627195c9c1fe4f\", \"columns\": {\"prompt\": \"instruction\", \"query\": \"input\", \"response\": \"output\"}}, \"amazon_train-MoE-Add\": {\"file_name\": \"MoE/add/amazon_train.json\", \"file_sha1\": \"db097ac843ef02e63f951cd4575e13eb3bd35582\", \"columns\": {\"prompt\": \"instruction\", \"query\": \"input\", \"response\": \"output\"}}, \"restaurant_train-MoE-Add\": {\"file_name\": \"MoE/add/restaurant_train.json\", \"file_sha1\": \"e0db121115abc361504e3f9ebf3651556702cc29\", \"columns\": {\"prompt\": \"instruction\", \"query\": \"input\", \"response\": \"output\"}}'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(dataset_info).replace(\"'\",'\"')[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1383909/3464378397.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# str(dataset_info).replace(\"'\",'\"')[1:-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtask_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtask_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_info' is not defined"
     ]
    }
   ],
   "source": [
    "# str(dataset_info).replace(\"'\",'\"')[1:-1]\n",
    "task_list = list(dataset_info.keys())\n",
    "task_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WANDB_MODE=disabled accelerate launch src/train_bash.py     --stage sft     --model_name_or_path Mistral-7B-Instruct-v0.2    --do_train     --finetuning_type lora     --dataset walmart_train-MoE-Add     --output_dir lora_weight/MoE/add/Mistral/walmart_train-MoE-Add --overwrite_output_dir     --lr_scheduler_type cosine     --num_train_epochs 15.0     --gradient_accumulation_steps 8     --per_device_eval_batch_size 8     --fp16     --template mistral     --lora_r 16 --logging_steps 5 --plot_loss  --lora_target all --save_steps 50 --use_unsloth --cutoff_len 2048',\n",
       " 'WANDB_MODE=disabled accelerate launch src/train_bash.py     --stage sft     --model_name_or_path Mistral-7B-Instruct-v0.2    --do_train     --finetuning_type lora     --dataset amazon_train-MoE-Add     --output_dir lora_weight/MoE/add/Mistral/amazon_train-MoE-Add --overwrite_output_dir     --lr_scheduler_type cosine     --num_train_epochs 15.0     --gradient_accumulation_steps 8     --per_device_eval_batch_size 8     --fp16     --template mistral     --lora_r 16 --logging_steps 5 --plot_loss  --lora_target all --save_steps 50 --use_unsloth --cutoff_len 2048',\n",
       " 'WANDB_MODE=disabled accelerate launch src/train_bash.py     --stage sft     --model_name_or_path Mistral-7B-Instruct-v0.2    --do_train     --finetuning_type lora     --dataset restaurant_train-MoE-Add     --output_dir lora_weight/MoE/add/Mistral/restaurant_train-MoE-Add --overwrite_output_dir     --lr_scheduler_type cosine     --num_train_epochs 15.0     --gradient_accumulation_steps 8     --per_device_eval_batch_size 8     --fp16     --template mistral     --lora_r 16 --logging_steps 5 --plot_loss  --lora_target all --save_steps 50 --use_unsloth --cutoff_len 2048']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"WANDB_MODE=disabled accelerate launch src/train_bash.py     --stage sft     --model_name_or_path Mistral-7B-Instruct-v0.2    --do_train     --finetuning_type lora     --dataset %s     --output_dir lora_weight/MoE/add/Mistral/%s --overwrite_output_dir     --lr_scheduler_type cosine     --num_train_epochs 15.0     --gradient_accumulation_steps 8     --per_device_eval_batch_size 8     --fp16     --template mistral     --lora_r 16 --logging_steps 5 --plot_loss  --lora_target all --save_steps 50 --use_unsloth --cutoff_len 2048\" % (t,t) for t in task_list[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_MODE=disabled accelerate launch src/train_bash.py     --stage sft     --model_name_or_path Mistral-7B-Instruct-v0.2    --do_train     --finetuning_type lora     --dataset hospital_train-MoE-Add     --output_dir lora_weight/MoE/add/Mistral/hospital_train-MoE-Add --overwrite_output_dir     --lr_scheduler_type cosine     --num_train_epochs 15.0     --gradient_accumulation_steps 8     --per_device_eval_batch_size 8     --fp16     --template mistral     --lora_r 16 --logging_steps 5 --plot_loss  --lora_target all --save_steps 50 --use_unsloth --cutoff_len 4096 --quantization_bit 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Here we Start Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_all = pd.read_csv('/data/home/wangys/LLM_ER/process_data/walmart-amazon-dirty/exp_data/tableA.csv').fillna('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
