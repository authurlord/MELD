{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from typing import Optional, List, Tuple\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from vllm import EngineArgs, LLMEngine, SamplingParams, RequestOutput\n",
    "from tqdm.notebook import tqdm\n",
    "from vllm.lora.request import LoRARequest\n",
    "tqdm.pandas(desc='pandas bar')\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-04 00:02:27 config.py:813] Defaulting to use mp for distributed inference\n",
      "INFO 09-04 00:02:28 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='/data/home/wangys/LLAMA-backup/LLaMA-Factory-main/Mistral-7B-Instruct-v0.2', speculative_config=None, tokenizer='/data/home/wangys/LLAMA-backup/LLaMA-Factory-main/Mistral-7B-Instruct-v0.2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/data/home/wangys/LLAMA-backup/LLaMA-Factory-main/Mistral-7B-Instruct-v0.2, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "WARNING 09-04 00:02:28 multiproc_gpu_executor.py:59] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-04 00:02:28 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2457552)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2457553)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2457551)\u001b[0;0m INFO 09-04 00:02:28 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 09-04 00:02:28 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 09-04 00:02:28 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W904 00:02:28.025871930 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:57513 (errno: 97 - Address family not supported by protocol).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=2457552)\u001b[0;0m INFO 09-04 00:02:28 utils.py:975] Found nccl from library libnccl.so.2\n",
      "INFO 09-04 00:02:28 utils.py:975] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2457551)\u001b[0;0m INFO 09-04 00:02:28 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2457553)\u001b[0;0m INFO 09-04 00:02:28 utils.py:975] Found nccl from library libnccl.so.2\n",
      "INFO 09-04 00:02:28 utils.py:975] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2457552)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2457553)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2457551)\u001b[0;0m INFO 09-04 00:02:28 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-04 00:02:28 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-04 00:02:28 pynccl.py:63] vLLM is using nccl==2.20.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W904 00:02:28.269972751 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:57513 (errno: 97 - Address family not supported by protocol).\n",
      "[W904 00:02:28.279263311 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:57513 (errno: 97 - Address family not supported by protocol).\n",
      "[W904 00:02:28.288051120 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:57513 (errno: 97 - Address family not supported by protocol).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-04 00:02:29 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /home/wangys/.cache/vllm/gpu_p2p_access_cache_for_4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2457551)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2457553)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2457552)\u001b[0;0m INFO 09-04 00:02:29 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /home/wangys/.cache/vllm/gpu_p2p_access_cache_for_4,5,6,7.json\n",
      "INFO 09-04 00:02:29 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /home/wangys/.cache/vllm/gpu_p2p_access_cache_for_4,5,6,7.json\n",
      "INFO 09-04 00:02:29 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /home/wangys/.cache/vllm/gpu_p2p_access_cache_for_4,5,6,7.json\n",
      "INFO 09-04 00:02:29 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f4771e8ecc0>, local_subscribe_port=37241, remote_subscribe_port=None)\n",
      "INFO 09-04 00:02:29 model_runner.py:879] Starting to load model /data/home/wangys/LLAMA-backup/LLaMA-Factory-main/Mistral-7B-Instruct-v0.2...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2457553)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2457551)\u001b[0;0m INFO 09-04 00:02:29 model_runner.py:879] Starting to load model /data/home/wangys/LLAMA-backup/LLaMA-Factory-main/Mistral-7B-Instruct-v0.2...\n",
      "INFO 09-04 00:02:29 model_runner.py:879] Starting to load model /data/home/wangys/LLAMA-backup/LLaMA-Factory-main/Mistral-7B-Instruct-v0.2...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2457552)\u001b[0;0m INFO 09-04 00:02:29 model_runner.py:879] Starting to load model /data/home/wangys/LLAMA-backup/LLaMA-Factory-main/Mistral-7B-Instruct-v0.2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=2457551)\u001b[0;0m "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b55a96ea4b4b43b22716afeda5042e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pt checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=2457553)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2457552)\u001b[0;0m /home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:416: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:416: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:416: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2457551)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2457553)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2457552)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:416: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-04 00:02:37 model_runner.py:890] Loading model weights took 3.3975 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2457553)\u001b[0;0m INFO 09-04 00:02:37 model_runner.py:890] Loading model weights took 3.3975 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2457552)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2457551)\u001b[0;0m INFO 09-04 00:02:37 model_runner.py:890] Loading model weights took 3.3975 GB\n",
      "INFO 09-04 00:02:37 model_runner.py:890] Loading model weights took 3.3975 GB\n",
      "INFO 09-04 00:02:43 distributed_gpu_executor.py:56] # GPU blocks: 121115, # CPU blocks: 8192\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_id = \"/data/home/wangys/LLAMA-backup/LLaMA-Factory-main/Mistral-7B-Instruct-v0.2\"\n",
    "llm = LLM(model=\"/data/home/wangys/LLAMA-backup/LLaMA-Factory-main/Mistral-7B-Instruct-v0.2\",\n",
    "                             enable_lora=True,\n",
    "                             max_loras=32,\n",
    "                             max_lora_rank=64,\n",
    "                             max_cpu_loras=32,\n",
    "                             max_num_seqs=256,enforce_eager=True,tensor_parallel_size=4,\n",
    "                             disable_log_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling_params_oasst = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=512)\n",
    "# oasst_lora_id = \"kaitchup/Meta-Llama-3-8B-oasst-Adapter\"\n",
    "# oasst_lora_path = snapshot_download(repo_id=oasst_lora_id)\n",
    "# oasstLR = LoRARequest(\"oasst\", 1, oasst_lora_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EM_amazon_google = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/ER/amazon-google-test.json')\n",
    "EM_ant_buy = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/ER/ant_buy_test_output.json')\n",
    "SM_CMS = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/SM/CMS_test_few_output.json')\n",
    "EM_semi_text_c = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/ER/semi-text-c-test-MoE.json')\n",
    "EM_semi_text_w = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/ER/semi-text-w-test-MoE.json')\n",
    "EM_walmart_amazon = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/ER/walmart_amazon_test_output.json')\n",
    "EM_wdc_all = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/ER/wdc_all_test_output.json')\n",
    "\n",
    "SM_synthea = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/SM/synthea_test_few_output.json')\n",
    "DC_hospital = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/hospital/hospital-test.json')\n",
    "DC_beer = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/beer/beer-test-20.json')\n",
    "DC_rayyan = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/rayyan/rayyan-test-20.json')\n",
    "DI_walmart = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/DI/walmart_test_output_wide.json')\n",
    "DI_amazon = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/DI/amazon_test_output_wide.json')\n",
    "DI_restaurant = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/DI/restaurant_test_output_wide.json')\n",
    "AVE_oa_mine = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/oa_mine/oa_mine_test_small.json')\n",
    "CTA_SimTab = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/CTA/SimTab_test_few.json')\n",
    "CTA_webtable = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/CTA/WebTable_Test_few.json')\n",
    "RE_wikigs = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/RE/RE-test_t=4.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [EM_amazon_google,EM_ant_buy,SM_CMS,EM_semi_text_c,EM_semi_text_w,EM_walmart_amazon,EM_wdc_all,SM_synthea,DC_hospital,DC_beer,DC_rayyan,DI_walmart,DI_amazon,DI_restaurant,AVE_oa_mine,CTA_SimTab,CTA_webtable,RE_wikigs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file = pd.DataFrame()\n",
    "def find_var_name(obj):\n",
    "    for var_name in globals():\n",
    "        if globals()[var_name] is obj:\n",
    "            return var_name\n",
    "    return None\n",
    "for df in file_list:\n",
    "    file_name = find_var_name(df)\n",
    "    # if(len(df)>200):\n",
    "        # df = df.sample(n=200).reset_index(drop=True)\n",
    "    df_select = df.iloc[:,:3]\n",
    "    df_select.columns = ['instruction','input','output']\n",
    "    df_select['task'] = file_name.split('_')[0]\n",
    "    df_select['dataset'] = file_name\n",
    "    all_file = pd.concat([all_file,df_select])\n",
    "    print(len(df),file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = dict(zip(all_file['instruction'],all_file['output']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed201912c58145fe92a9ab9892976fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pandas bar:   0%|          | 0/29587 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_length = 2000\n",
    "MoE_list_update_top_2 = pd.read_csv('/data/home/wangys/MoE-Example/Router/MoE_list_update_top_2.csv',index_col=0)\n",
    "def AST(row):\n",
    "    CD = row['cross-dataset']\n",
    "    row['cross-dataset'] = eval(CD)\n",
    "    CT = row['cross-task']\n",
    "    row['cross-task'] = eval(CT)\n",
    "    expert = row['expert_predict']\n",
    "    row['expert_predict'] = eval(expert)[:2]\n",
    "    return row\n",
    "MoE_list_update_top_2 = MoE_list_update_top_2.progress_apply(AST,axis=1)\n",
    "expert_list = []\n",
    "for index,row in MoE_list_update_top_2.iterrows():\n",
    "    # expert_list.append(set(row['cross-dataset'])) if set(row['cross-dataset']) not in expert_list else None\n",
    "    # expert_list.append(set(row['cross-task'])) if set(row['cross-task']) not in expert_list else None\n",
    "    expert_list.append(set(row['expert_predict'])) if set(row['expert_predict']) not in expert_list else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_path_dict = {}\n",
    "evaluate_task = 'expert_predict'\n",
    "for e in expert_list:\n",
    "    expert_0,expert_1 = list(e)\n",
    "    folder_path = 'lora_weight/merge/%s#%s' % (expert_0,expert_1)\n",
    "    folder_path_rev = 'lora_weight/merge/%s#%s' % (expert_1,expert_0)\n",
    "\n",
    "    if(os.path.exists(folder_path)):\n",
    "        lora_path_dict['%s#%s' % (expert_0,expert_1)] = folder_path\n",
    "        lora_path_dict['%s#%s' % (expert_1,expert_0)] = folder_path\n",
    "    elif(os.path.exists(folder_path_rev)):\n",
    "        lora_path_dict['%s#%s' % (expert_0,expert_1)] = folder_path_rev\n",
    "        lora_path_dict['%s#%s' % (expert_1,expert_0)] = folder_path_rev\n",
    "    else:\n",
    "        print(folder_path)\n",
    "lora_id_list=list(lora_path_dict.keys())\n",
    "def assign_lora_id(row):\n",
    "    expert_0,expert_1 = row[evaluate_task]\n",
    "    row['lora_id'] = lora_id_list.index('%s#%s' % (expert_0,expert_1))\n",
    "    return row\n",
    "MoE_list_update_top_2 = MoE_list_update_top_2.apply(assign_lora_id,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistral|webtable-MoE-CT#Mistral|SimTab-MoE-CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2479155/2538932839.py:3: DeprecationWarning: The 'lora_local_path' attribute is deprecated and will be removed in a future version. Please use 'lora_path' instead.\n",
      "  quest = LoRARequest(lora_id_list[i], i, lora_path_dict[lora_id_list[i]])\n"
     ]
    }
   ],
   "source": [
    "lora_quest_list  = []\n",
    "for i in range(len(lora_id_list)):\n",
    "    quest = LoRARequest(lora_id_list[i], i, lora_path_dict[lora_id_list[i]])\n",
    "    lora_quest_list.append(quest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MoE_list_update_top_2['output'] = MoE_list_update_top_2['query'].map(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MoE_list_update_top_2.to_csv('Router/MoE_update.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['{\"Output\": \"dismatch\"}', '{\"Output\": \"match\"}', \"{'Output': 'dismatch'}\", \"{'Output': 'match'}\", \"{'brand': 'aluratek'}\", \"{'category': 'printers'}\", \"{'brand': 'hp'}\", \"{'category': 'gps'}\", \"{'brand': 'panasonic'}\", \"{'brand': 'link depot'}\", \"{'category': 'mp3 accessories'}\", \"{'brand': 'acer'}\", \"{'brand': 'evga'}\", \"{'category': 'electronics - general'}\", \"{'category': 'mice'}\", \"{'category': 'memory'}\", \"{'brand': 'edge tech'}\", \"{'brand': 'accell'}\", \"{'brand': 'aoc'}\", \"{'brand': 'xantech'}\", \"{'category': 'stationery & office machinery'}\", \"{'brand': 'maxell'}\", \"{'brand': 's j paper'}\", \"{'category': 'digital cameras'}\", \"{'category': 'photography - general'}\", \"{'brand': 'kroo'}\", \"{'category': 'networking'}\", \"{'brand': 'mace security'}\", \"{'brand': 'canon'}\", \"{'brand': 'wausau paper'}\", \"{'brand': 'draper'}\", \"{'brand': 'creative labs'}\", \"{'category': 'car stereos'}\", \"{'brand': 'kenwood'}\", \"{'brand': 'epson'}\", \"{'brand': 'avery'}\", \"{'brand': 'booq'}\", \"{'brand': 'ofm'}\", \"{'brand': 'rca'}\", \"{'brand': 'cables to go'}\", \"{'brand': 'sumas'}\"]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(selection_list)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(prompt_list)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m---> 14\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(prompt_list, \n\u001b[1;32m     15\u001b[0m                         SamplingParams(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m), lora_request\u001b[38;5;241m=\u001b[39mlora_quest_list[lora_id],\n\u001b[1;32m     16\u001b[0m                         guided_options_request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(guided_choice\u001b[38;5;241m=\u001b[39mselection_list))\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[1;32m     18\u001b[0m         prompt \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mprompt\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "MoE_list_update_top_2['prediction'] = ''\n",
    "### Group by same lora_id\n",
    "for lora_id in range(len(lora_id_list)):\n",
    "# for lora_id in [0]:\n",
    "    generation_list = []\n",
    "    selection_index = MoE_list_update_top_2[MoE_list_update_top_2['lora_id']==lora_id].index\n",
    "    result = MoE_list_update_top_2.iloc[selection_index]\n",
    "    \n",
    "    prompt_list = [\"[INST] %s [/INST]\" % str(a) for a in result['query'].to_list()] \n",
    "    selection_list = list(result['output'].unique())\n",
    "    selection_list = [s for s in selection_list if s!='']\n",
    "    print(selection_list)\n",
    "    if(len(prompt_list)>0):\n",
    "        outputs = llm.generate(prompt_list, \n",
    "                            SamplingParams(temperature=0.0, top_p=1.0, max_tokens=512), lora_request=lora_quest_list[lora_id],\n",
    "                            guided_options_request=dict(guided_choice=selection_list))\n",
    "        for output in outputs:\n",
    "            prompt = output.prompt\n",
    "            generated_text = output.outputs[0].text\n",
    "            # print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "            generation_list.append(generated_text)\n",
    "        # MoE_list_update_top_2.iloc[selection_index]['prediction'] = generation_list\n",
    "        for i in range(len(generation_list)):\n",
    "            MoE_list_update_top_2.iloc[selection_index[i],-1] = generation_list[i]\n",
    "MoE_list_update_top_2.to_csv('Router/MoE_update_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 844\n"
     ]
    }
   ],
   "source": [
    "# for lora_id in range(len(lora_id_list)):\n",
    "for lora_id in [0]:\n",
    "    generation_list = []\n",
    "    selection_index = MoE_list_update_top_2[MoE_list_update_top_2['lora_id']==lora_id].index\n",
    "    result = MoE_list_update_top_2.iloc[selection_index]\n",
    "    print(lora_id,len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.7939698492462312\n",
      "\n",
      "Recall:0.7669902912621359\n",
      "\n",
      "F1:0.7802469135802469\n"
     ]
    }
   ],
   "source": [
    "def Transfer(row):\n",
    "    if(row['output'].__contains__('mismatch')) or (row['output'].__contains__('dismatch')):\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    if(row['prediction'].__contains__('mismatch')) or (row['prediction'].__contains__('dismatch')):\n",
    "        predict = 0\n",
    "    else:\n",
    "        predict = 1\n",
    "    return label,predict\n",
    "result_output = MoE_list_update_top_2.apply(Transfer,axis=1,result_type='expand')\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "# print(file_path)\n",
    "print('Precision:{}\\n\\nRecall:{}\\n\\nF1:{}'.format(\n",
    "    precision_score(y_true=result_output[0],y_pred=result_output[1]),\n",
    "    recall_score(y_true=result_output[0],y_pred=result_output[1]),\n",
    "    f1_score(y_true=result_output[0],y_pred=result_output[1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
