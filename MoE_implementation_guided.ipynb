{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from typing import Optional, List, Tuple\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from vllm import EngineArgs, LLMEngine, SamplingParams, RequestOutput\n",
    "from tqdm.notebook import tqdm\n",
    "from vllm.lora.request import LoRARequest\n",
    "tqdm.pandas(desc='pandas bar')\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4,5,6,7'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-06 21:06:14 config.py:890] Defaulting to use mp for distributed inference\n",
      "WARNING 09-06 21:06:14 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "WARNING 09-06 21:06:14 config.py:378] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-06 21:06:14 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='/data/home/wangys/model/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='/data/home/wangys/model/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/data/home/wangys/model/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=False)\n",
      "WARNING 09-06 21:06:14 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-06 21:06:14 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3248917)\u001b[0;0m INFO 09-06 21:06:15 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3248918)\u001b[0;0m INFO 09-06 21:06:15 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3248916)\u001b[0;0m INFO 09-06 21:06:15 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W906 21:06:15.739767245 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:59639 (errno: 97 - Address family not supported by protocol).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-06 21:06:15 utils.py:977] Found nccl from library libnccl.so.2\n",
      "INFO 09-06 21:06:15 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3248916)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3248918)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3248917)\u001b[0;0m INFO 09-06 21:06:15 utils.py:977] Found nccl from library libnccl.so.2\n",
      "INFO 09-06 21:06:15 utils.py:977] Found nccl from library libnccl.so.2\n",
      "INFO 09-06 21:06:15 utils.py:977] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3248918)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3248916)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3248917)\u001b[0;0m INFO 09-06 21:06:15 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-06 21:06:15 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-06 21:06:15 pynccl.py:63] vLLM is using nccl==2.20.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W906 21:06:15.001307028 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:59639 (errno: 97 - Address family not supported by protocol).\n",
      "[W906 21:06:15.008862079 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:59639 (errno: 97 - Address family not supported by protocol).\n",
      "[W906 21:06:15.010286053 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:59639 (errno: 97 - Address family not supported by protocol).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-06 21:06:16 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/wangys/.cache/vllm/gpu_p2p_access_cache_for_4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3248916)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3248917)\u001b[0;0m INFO 09-06 21:06:16 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/wangys/.cache/vllm/gpu_p2p_access_cache_for_4,5,6,7.json\n",
      "INFO 09-06 21:06:16 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/wangys/.cache/vllm/gpu_p2p_access_cache_for_4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3248918)\u001b[0;0m INFO 09-06 21:06:16 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/wangys/.cache/vllm/gpu_p2p_access_cache_for_4,5,6,7.json\n",
      "INFO 09-06 21:06:16 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f9920e1d100>, local_subscribe_port=43855, remote_subscribe_port=None)\n",
      "INFO 09-06 21:06:16 model_runner.py:915] Starting to load model /data/home/wangys/model/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3248916)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3248918)\u001b[0;0m INFO 09-06 21:06:16 model_runner.py:915] Starting to load model /data/home/wangys/model/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3248917)\u001b[0;0m INFO 09-06 21:06:16 model_runner.py:915] Starting to load model /data/home/wangys/model/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 09-06 21:06:16 model_runner.py:915] Starting to load model /data/home/wangys/model/Meta-Llama-3.1-8B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acd6a891d7246ff972ae7c91a4f9bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-06 21:06:17 model_runner.py:926] Loading model weights took 3.7881 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3248918)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3248916)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3248917)\u001b[0;0m INFO 09-06 21:06:17 model_runner.py:926] Loading model weights took 3.7881 GB\n",
      "INFO 09-06 21:06:17 model_runner.py:926] Loading model weights took 3.7881 GB\n",
      "INFO 09-06 21:06:17 model_runner.py:926] Loading model weights took 3.7881 GB\n",
      "INFO 09-06 21:06:26 distributed_gpu_executor.py:57] # GPU blocks: 90774, # CPU blocks: 8192\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# model_id = '/data/home/wangys/LLAMA-backup/LLaMA-Factory-main/Mistral-7B-Instruct-v0.2'\n",
    "model_id = '/data/home/wangys/model/Meta-Llama-3.1-8B-Instruct'\n",
    "\n",
    "llm = LLM(model=model_id,\n",
    "                             enable_lora=True,\n",
    "                             max_loras=32,\n",
    "                             max_lora_rank=64,\n",
    "                             max_cpu_loras=32,\n",
    "                             max_num_seqs=256,enforce_eager=True,tensor_parallel_size=4,\n",
    "                             disable_log_stats=True,gpu_memory_utilization=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling_params_oasst = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=512)\n",
    "# oasst_lora_id = 'kaitchup/Meta-Llama-3-8B-oasst-Adapter'\n",
    "# oasst_lora_path = snapshot_download(repo_id=oasst_lora_id)\n",
    "# oasstLR = LoRARequest('oasst', 1, oasst_lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=6,7 python vllm_enrich.py \\\n",
    "--llm_path /data/home/wangys/model/Meta-Llama-3.1-8B-Instruct \\\n",
    "--data_name AB \\\n",
    "--gpu_num 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=6,7 python vllm_enrich.py \\\n",
    "--llm_path /data/home/wangys/model/Phi-3.5-mini-instruct \\\n",
    "--data_name AB \\\n",
    "--gpu_num 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EM_amazon_google = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/ER/amazon-google-test.json')\n",
    "EM_ant_buy = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/ER/ant_buy_test_output.json')\n",
    "SM_CMS = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/SM/CMS_test_few_output.json')\n",
    "EM_semi_text_c = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/ER/semi-text-c-test-MoE.json')\n",
    "EM_semi_text_w = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/ER/semi-text-w-test-MoE.json')\n",
    "EM_walmart_amazon = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/ER/walmart_amazon_test_output.json')\n",
    "EM_wdc_all = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/ER/wdc_all_test_output.json')\n",
    "\n",
    "SM_synthea = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/SM/synthea_test_few_output.json')\n",
    "DC_hospital = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/hospital/hospital-test.json')\n",
    "DC_beer = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/beer/beer-test-20.json')\n",
    "DC_rayyan = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/rayyan/rayyan-test-20.json')\n",
    "DI_walmart = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/DI/walmart_test_output_wide.json')\n",
    "DI_amazon = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/DI/amazon_test_output_wide.json')\n",
    "DI_restaurant = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/MoE/DI/restaurant_test_output_wide.json')\n",
    "AVE_oa_mine = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/oa_mine/oa_mine_test_small.json')\n",
    "CTA_SimTab = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/CTA/SimTab_test_few.json')\n",
    "CTA_webtable = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/CTA/WebTable_Test_few.json')\n",
    "RE_wikigs = pd.read_json('/data/home/wangys/LLaMA-Factory-main/data/RE/RE-test_t=4.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [EM_amazon_google,EM_ant_buy,SM_CMS,EM_semi_text_c,EM_semi_text_w,EM_walmart_amazon,EM_wdc_all,SM_synthea,DC_hospital,DC_beer,DC_rayyan,DI_walmart,DI_amazon,DI_restaurant,AVE_oa_mine,CTA_SimTab,CTA_webtable,RE_wikigs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file = pd.DataFrame()\n",
    "def find_var_name(obj):\n",
    "    for var_name in globals():\n",
    "        if globals()[var_name] is obj:\n",
    "            return var_name\n",
    "    return None\n",
    "for df in file_list:\n",
    "    file_name = find_var_name(df)\n",
    "    # if(len(df)>200):\n",
    "        # df = df.sample(n=200).reset_index(drop=True)\n",
    "    df_select = df.iloc[:,:3]\n",
    "    df_select.columns = ['instruction','input','output']\n",
    "    df_select['task'] = file_name.split('_')[0]\n",
    "    df_select['dataset'] = file_name\n",
    "    all_file = pd.concat([all_file,df_select])\n",
    "    print(len(df),file_name)\n",
    "output_dict = dict(zip(all_file['instruction'],all_file['output']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102f377d2575490880d0eeac2828133d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pandas bar:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_length = 2000\n",
    "# MoE_list_update_top_2 = pd.read_csv('/data/home/wangys/MoE-Example/Router/MoE_list_update_top_2.csv',index_col=0).iloc[:sample_length]\n",
    "MoE_list_update_top_2 = pd.read_csv('Router/MoE_update.csv',index_col=0).iloc[:sample_length]\n",
    "def AST(row):\n",
    "    CD = row['cross-dataset']\n",
    "    row['cross-dataset'] = eval(CD)\n",
    "    CT = row['cross-task']\n",
    "    row['cross-task'] = eval(CT)\n",
    "    expert = row['expert_predict']\n",
    "    row['expert_predict'] = eval(expert)[:2]\n",
    "    return row\n",
    "MoE_list_update_top_2 = MoE_list_update_top_2.progress_apply(AST,axis=1)\n",
    "expert_list = []\n",
    "for index,row in MoE_list_update_top_2.iterrows():\n",
    "    # expert_list.append(set(row['cross-dataset'])) if set(row['cross-dataset']) not in expert_list else None\n",
    "    # expert_list.append(set(row['cross-task'])) if set(row['cross-task']) not in expert_list else None\n",
    "    expert_list.append(set(row['expert_predict'])) if set(row['expert_predict']) not in expert_list else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_path_dict = {}\n",
    "evaluate_task = 'expert_predict'\n",
    "for e in expert_list:\n",
    "    expert_0,expert_1 = list(e)\n",
    "    folder_path = 'lora_weight/merge/%s#%s' % (expert_0,expert_1)\n",
    "    folder_path_rev = 'lora_weight/merge/%s#%s' % (expert_1,expert_0)\n",
    "\n",
    "    if(os.path.exists(folder_path)):\n",
    "        lora_path_dict['%s#%s' % (expert_0,expert_1)] = folder_path\n",
    "        lora_path_dict['%s#%s' % (expert_1,expert_0)] = folder_path\n",
    "    elif(os.path.exists(folder_path_rev)):\n",
    "        lora_path_dict['%s#%s' % (expert_0,expert_1)] = folder_path_rev\n",
    "        lora_path_dict['%s#%s' % (expert_1,expert_0)] = folder_path_rev\n",
    "    else:\n",
    "        print(folder_path)\n",
    "lora_id_list=list(lora_path_dict.keys())\n",
    "def assign_lora_id(row):\n",
    "    expert_0,expert_1 = row[evaluate_task]\n",
    "    row['lora_id'] = lora_id_list.index('%s#%s' % (expert_0,expert_1))\n",
    "    return row\n",
    "MoE_list_update_top_2 = MoE_list_update_top_2.apply(assign_lora_id,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistral|webtable-MoE-CT#Mistral|SimTab-MoE-CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2553060/2538932839.py:3: DeprecationWarning: The 'lora_local_path' attribute is deprecated and will be removed in a future version. Please use 'lora_path' instead.\n",
      "  quest = LoRARequest(lora_id_list[i], i, lora_path_dict[lora_id_list[i]])\n"
     ]
    }
   ],
   "source": [
    "lora_quest_list  = []\n",
    "for i in range(len(lora_id_list)):\n",
    "    quest = LoRARequest(lora_id_list[i], i, lora_path_dict[lora_id_list[i]])\n",
    "    lora_quest_list.append(quest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m MoE_list_update_top_2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m MoE_list_update_top_2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[43moutput_dict\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_dict' is not defined"
     ]
    }
   ],
   "source": [
    "MoE_list_update_top_2['output'] = MoE_list_update_top_2['query'].map(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MoE_list_update_top_2.to_csv('Router/MoE_update.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"Output\": \"dismatch\"}', '{\"Output\": \"match\"}']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling FSM index for all state transitions: 100%|██████████| 23/23 [00:00<00:00, 123.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-06 13:04:20 tokenizer.py:174] No tokenizer found in lora_weight/merge/Mistral|semi_text_w-MoE-CT#Mistral|amazon_google-MoE-CT, using base model tokenizer instead. (Exception: lora_weight/merge/Mistral|semi_text_w-MoE-CT#Mistral|amazon_google-MoE-CT does not appear to have a file named config.json. Checkout 'https://huggingface.co/lora_weight/merge/Mistral|semi_text_w-MoE-CT#Mistral|amazon_google-MoE-CT/tree/None' for available files.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 844/844 [00:41<00:00, 20.24it/s, est. speed input: 37786.00 toks/s, output: 178.41 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "['{\"Output\": \"dismatch\"}', '{\"Output\": \"match\"}']\n",
      "WARNING 09-06 13:05:04 tokenizer.py:174] No tokenizer found in lora_weight/merge/Mistral|amazon_google-MoE-CT#Mistral|SimTab-MoE-CT, using base model tokenizer instead. (Exception: lora_weight/merge/Mistral|amazon_google-MoE-CT#Mistral|SimTab-MoE-CT does not appear to have a file named config.json. Checkout 'https://huggingface.co/lora_weight/merge/Mistral|amazon_google-MoE-CT#Mistral|SimTab-MoE-CT/tree/None' for available files.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 985/985 [01:03<00:00, 15.61it/s, est. speed input: 29087.88 toks/s, output: 137.21 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['{\"Output\": \"dismatch\"}', '{\"Output\": \"match\"}']\n",
      "WARNING 09-06 13:06:10 tokenizer.py:174] No tokenizer found in lora_weight/merge/Mistral|amazon_google-MoE-CT#Mistral|walmart-MoE-CT, using base model tokenizer instead. (Exception: lora_weight/merge/Mistral|amazon_google-MoE-CT#Mistral|walmart-MoE-CT does not appear to have a file named config.json. Checkout 'https://huggingface.co/lora_weight/merge/Mistral|amazon_google-MoE-CT#Mistral|walmart-MoE-CT/tree/None' for available files.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts: 100%|██████████| 171/171 [00:11<00:00, 15.45it/s, est. speed input: 28387.93 toks/s, output: 135.96 toks/s]\n"
     ]
    }
   ],
   "source": [
    "MoE_list_update_top_2['prediction'] = ''\n",
    "### Group by same lora_id\n",
    "for lora_id in range(len(lora_id_list)):\n",
    "# for lora_id in [0]:\n",
    "    generation_list = []\n",
    "    selection_index = MoE_list_update_top_2[MoE_list_update_top_2['lora_id']==lora_id].index\n",
    "    result = MoE_list_update_top_2.iloc[selection_index]\n",
    "    \n",
    "    prompt_list = ['[INST] %s [/INST]' % str(a) for a in result['query'].to_list()] \n",
    "    selection_list = list(result['output'].unique())\n",
    "    selection_list = [s for s in selection_list if s!='']\n",
    "    print(selection_list)\n",
    "    if(len(prompt_list)>0):\n",
    "        outputs = llm.generate(prompt_list, \n",
    "                            SamplingParams(temperature=0.0, \n",
    "                                           top_p=1.0, \n",
    "                                           max_tokens=512,\n",
    "                                           logprobs=1), \n",
    "                            lora_request=lora_quest_list[lora_id],\n",
    "                            guided_options_request=dict(\n",
    "                                guided_choice=selection_list\n",
    "                                ))\n",
    "        for output in outputs:\n",
    "            prompt = output.prompt\n",
    "            generated_text = output.outputs[0].text\n",
    "            # print(f'Prompt: {prompt!r}, Generated text: {generated_text!r}')\n",
    "            generation_list.append(generated_text)\n",
    "        # MoE_list_update_top_2.iloc[selection_index]['prediction'] = generation_list\n",
    "        for i in range(len(generation_list)):\n",
    "            MoE_list_update_top_2.iloc[selection_index[i],-1] = generation_list[i]\n",
    "# MoE_list_update_top_2.to_csv('Router/MoE_update_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_1 = {'name': '', 'description': '', 'price': '', 'category': '', 'sku': '', 'brand': '', 'modelno': '', 'key_features': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_dict = {}\n",
    "format_dict['Entity 1'] = dict_1\n",
    "format_dict['Entity 2'] = dict_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Entity 1': {'name': '',\n",
       "  'description': '',\n",
       "  'price': '',\n",
       "  'category': '',\n",
       "  'sku': '',\n",
       "  'brand': '',\n",
       "  'modelno': '',\n",
       "  'key_features': ''},\n",
       " 'Entity 2': {'name': '',\n",
       "  'description': '',\n",
       "  'price': '',\n",
       "  'category': '',\n",
       "  'sku': '',\n",
       "  'brand': '',\n",
       "  'modelno': '',\n",
       "  'key_features': ''}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 844\n"
     ]
    }
   ],
   "source": [
    "# for lora_id in range(len(lora_id_list)):\n",
    "for lora_id in [0]:\n",
    "    generation_list = []\n",
    "    selection_index = MoE_list_update_top_2[MoE_list_update_top_2['lora_id']==lora_id].index\n",
    "    result = MoE_list_update_top_2.iloc[selection_index]\n",
    "    print(lora_id,len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.7939698492462312\n",
      "\n",
      "Recall:0.7669902912621359\n",
      "\n",
      "F1:0.7802469135802469\n"
     ]
    }
   ],
   "source": [
    "def Transfer(row):\n",
    "    if(row['output'].__contains__('mismatch')) or (row['output'].__contains__('dismatch')):\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    if(row['prediction'].__contains__('mismatch')) or (row['prediction'].__contains__('dismatch')):\n",
    "        predict = 0\n",
    "    else:\n",
    "        predict = 1\n",
    "    return label,predict\n",
    "result_output = MoE_list_update_top_2.apply(Transfer,axis=1,result_type='expand')\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "# print(file_path)\n",
    "print('Precision:{}\\n\\nRecall:{}\\n\\nF1:{}'.format(\n",
    "    precision_score(y_true=result_output[0],y_pred=result_output[1]),\n",
    "    recall_score(y_true=result_output[0],y_pred=result_output[1]),\n",
    "    f1_score(y_true=result_output[0],y_pred=result_output[1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
